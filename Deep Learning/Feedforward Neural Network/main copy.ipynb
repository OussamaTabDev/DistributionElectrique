{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bffdb6a",
   "metadata": {},
   "source": [
    "# Deep learing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86043c9",
   "metadata": {},
   "source": [
    "## FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "779a60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout , GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f247e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3867222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>B</th>\n",
       "      <th>A</th>\n",
       "      <th>Ia</th>\n",
       "      <th>Ib</th>\n",
       "      <th>Ic</th>\n",
       "      <th>Va</th>\n",
       "      <th>Vb</th>\n",
       "      <th>Vc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-151.291812</td>\n",
       "      <td>-9.677452</td>\n",
       "      <td>85.800162</td>\n",
       "      <td>0.400750</td>\n",
       "      <td>-0.132935</td>\n",
       "      <td>-0.267815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-336.186183</td>\n",
       "      <td>-76.283262</td>\n",
       "      <td>18.328897</td>\n",
       "      <td>0.312732</td>\n",
       "      <td>-0.123633</td>\n",
       "      <td>-0.189099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-502.891583</td>\n",
       "      <td>-174.648023</td>\n",
       "      <td>-80.924663</td>\n",
       "      <td>0.265728</td>\n",
       "      <td>-0.114301</td>\n",
       "      <td>-0.151428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-593.941905</td>\n",
       "      <td>-217.703359</td>\n",
       "      <td>-124.891924</td>\n",
       "      <td>0.235511</td>\n",
       "      <td>-0.104940</td>\n",
       "      <td>-0.130570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-643.663617</td>\n",
       "      <td>-224.159427</td>\n",
       "      <td>-132.282815</td>\n",
       "      <td>0.209537</td>\n",
       "      <td>-0.095554</td>\n",
       "      <td>-0.113983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7856</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-66.237921</td>\n",
       "      <td>38.457041</td>\n",
       "      <td>24.912239</td>\n",
       "      <td>0.094421</td>\n",
       "      <td>-0.552019</td>\n",
       "      <td>0.457598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7857</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.849493</td>\n",
       "      <td>37.465454</td>\n",
       "      <td>25.515675</td>\n",
       "      <td>0.103778</td>\n",
       "      <td>-0.555186</td>\n",
       "      <td>0.451407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7858</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.446698</td>\n",
       "      <td>36.472055</td>\n",
       "      <td>26.106554</td>\n",
       "      <td>0.113107</td>\n",
       "      <td>-0.558211</td>\n",
       "      <td>0.445104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7859</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.029633</td>\n",
       "      <td>35.477088</td>\n",
       "      <td>26.684731</td>\n",
       "      <td>0.122404</td>\n",
       "      <td>-0.561094</td>\n",
       "      <td>0.438690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7860</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-64.598401</td>\n",
       "      <td>34.480799</td>\n",
       "      <td>27.250065</td>\n",
       "      <td>0.131669</td>\n",
       "      <td>-0.563835</td>\n",
       "      <td>0.432166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7861 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      G  C  B  A          Ia          Ib          Ic        Va        Vb  \\\n",
       "0     1  0  0  1 -151.291812   -9.677452   85.800162  0.400750 -0.132935   \n",
       "1     1  0  0  1 -336.186183  -76.283262   18.328897  0.312732 -0.123633   \n",
       "2     1  0  0  1 -502.891583 -174.648023  -80.924663  0.265728 -0.114301   \n",
       "3     1  0  0  1 -593.941905 -217.703359 -124.891924  0.235511 -0.104940   \n",
       "4     1  0  0  1 -643.663617 -224.159427 -132.282815  0.209537 -0.095554   \n",
       "...  .. .. .. ..         ...         ...         ...       ...       ...   \n",
       "7856  0  0  0  0  -66.237921   38.457041   24.912239  0.094421 -0.552019   \n",
       "7857  0  0  0  0  -65.849493   37.465454   25.515675  0.103778 -0.555186   \n",
       "7858  0  0  0  0  -65.446698   36.472055   26.106554  0.113107 -0.558211   \n",
       "7859  0  0  0  0  -65.029633   35.477088   26.684731  0.122404 -0.561094   \n",
       "7860  0  0  0  0  -64.598401   34.480799   27.250065  0.131669 -0.563835   \n",
       "\n",
       "            Vc  \n",
       "0    -0.267815  \n",
       "1    -0.189099  \n",
       "2    -0.151428  \n",
       "3    -0.130570  \n",
       "4    -0.113983  \n",
       "...        ...  \n",
       "7856  0.457598  \n",
       "7857  0.451407  \n",
       "7858  0.445104  \n",
       "7859  0.438690  \n",
       "7860  0.432166  \n",
       "\n",
       "[7861 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"G\" , \"C\" , \"B\" , \"A\" , \"Ia\" , \"Ib\" , \"Ic\" , \"Va\" , \"Vb\" , \"Vc\"] \n",
    "# Ia\" , \"Ib\" , \"Ic\" , \"Va\" , \"Vb\" , \"Vc is Input : features\n",
    "# G , C , B , A is OutPut : labels\n",
    "df = pd.read_csv(\"../../classData.csv\")\n",
    "cols[4:]\n",
    "# Step 1: Encode fault combinations into single-class labels\n",
    "fault_map = {\n",
    "    '0000': 0,  # No Fault\n",
    "    '1001': 1,  # LG\n",
    "    '0011': 2,  # LL\n",
    "    '0110': 3 , # LL\n",
    "    '1011': 4,  # LLG\n",
    "    '0111': 5,  # LLL\n",
    "    '1111': 6   # LLLG\n",
    "}\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5b5cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fault_type'] = df[['G', 'C', 'B', 'A']].astype(str).agg(''.join, axis=1)\n",
    "# Map to single class label\n",
    "df['fault_class'] = df['fault_type'].map(fault_map)\n",
    "# Drop rows with unknown fault combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3a91397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep dropped rows (unknown fault types) in a separate DataFrame\n",
    "df_unknown = df[~df['fault_type'].isin(fault_map.keys())].copy()\n",
    "\n",
    "# Filter valid rows for training\n",
    "df = df[df['fault_type'].isin(fault_map.keys())].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dafbd75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>B</th>\n",
       "      <th>A</th>\n",
       "      <th>Ia</th>\n",
       "      <th>Ib</th>\n",
       "      <th>Ic</th>\n",
       "      <th>Va</th>\n",
       "      <th>Vb</th>\n",
       "      <th>Vc</th>\n",
       "      <th>fault_type</th>\n",
       "      <th>fault_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-151.291812</td>\n",
       "      <td>-9.677452</td>\n",
       "      <td>85.800162</td>\n",
       "      <td>0.400750</td>\n",
       "      <td>-0.132935</td>\n",
       "      <td>-0.267815</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-336.186183</td>\n",
       "      <td>-76.283262</td>\n",
       "      <td>18.328897</td>\n",
       "      <td>0.312732</td>\n",
       "      <td>-0.123633</td>\n",
       "      <td>-0.189099</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-502.891583</td>\n",
       "      <td>-174.648023</td>\n",
       "      <td>-80.924663</td>\n",
       "      <td>0.265728</td>\n",
       "      <td>-0.114301</td>\n",
       "      <td>-0.151428</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-593.941905</td>\n",
       "      <td>-217.703359</td>\n",
       "      <td>-124.891924</td>\n",
       "      <td>0.235511</td>\n",
       "      <td>-0.104940</td>\n",
       "      <td>-0.130570</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-643.663617</td>\n",
       "      <td>-224.159427</td>\n",
       "      <td>-132.282815</td>\n",
       "      <td>0.209537</td>\n",
       "      <td>-0.095554</td>\n",
       "      <td>-0.113983</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7856</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-66.237921</td>\n",
       "      <td>38.457041</td>\n",
       "      <td>24.912239</td>\n",
       "      <td>0.094421</td>\n",
       "      <td>-0.552019</td>\n",
       "      <td>0.457598</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7857</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.849493</td>\n",
       "      <td>37.465454</td>\n",
       "      <td>25.515675</td>\n",
       "      <td>0.103778</td>\n",
       "      <td>-0.555186</td>\n",
       "      <td>0.451407</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7858</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.446698</td>\n",
       "      <td>36.472055</td>\n",
       "      <td>26.106554</td>\n",
       "      <td>0.113107</td>\n",
       "      <td>-0.558211</td>\n",
       "      <td>0.445104</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7859</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.029633</td>\n",
       "      <td>35.477088</td>\n",
       "      <td>26.684731</td>\n",
       "      <td>0.122404</td>\n",
       "      <td>-0.561094</td>\n",
       "      <td>0.438690</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7860</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-64.598401</td>\n",
       "      <td>34.480799</td>\n",
       "      <td>27.250065</td>\n",
       "      <td>0.131669</td>\n",
       "      <td>-0.563835</td>\n",
       "      <td>0.432166</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7861 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      G  C  B  A          Ia          Ib          Ic        Va        Vb  \\\n",
       "0     1  0  0  1 -151.291812   -9.677452   85.800162  0.400750 -0.132935   \n",
       "1     1  0  0  1 -336.186183  -76.283262   18.328897  0.312732 -0.123633   \n",
       "2     1  0  0  1 -502.891583 -174.648023  -80.924663  0.265728 -0.114301   \n",
       "3     1  0  0  1 -593.941905 -217.703359 -124.891924  0.235511 -0.104940   \n",
       "4     1  0  0  1 -643.663617 -224.159427 -132.282815  0.209537 -0.095554   \n",
       "...  .. .. .. ..         ...         ...         ...       ...       ...   \n",
       "7856  0  0  0  0  -66.237921   38.457041   24.912239  0.094421 -0.552019   \n",
       "7857  0  0  0  0  -65.849493   37.465454   25.515675  0.103778 -0.555186   \n",
       "7858  0  0  0  0  -65.446698   36.472055   26.106554  0.113107 -0.558211   \n",
       "7859  0  0  0  0  -65.029633   35.477088   26.684731  0.122404 -0.561094   \n",
       "7860  0  0  0  0  -64.598401   34.480799   27.250065  0.131669 -0.563835   \n",
       "\n",
       "            Vc fault_type  fault_class  \n",
       "0    -0.267815       1001            1  \n",
       "1    -0.189099       1001            1  \n",
       "2    -0.151428       1001            1  \n",
       "3    -0.130570       1001            1  \n",
       "4    -0.113983       1001            1  \n",
       "...        ...        ...          ...  \n",
       "7856  0.457598       0000            0  \n",
       "7857  0.451407       0000            0  \n",
       "7858  0.445104       0000            0  \n",
       "7859  0.438690       0000            0  \n",
       "7860  0.432166       0000            0  \n",
       "\n",
       "[7861 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f85dc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>B</th>\n",
       "      <th>A</th>\n",
       "      <th>Ia</th>\n",
       "      <th>Ib</th>\n",
       "      <th>Ic</th>\n",
       "      <th>Va</th>\n",
       "      <th>Vb</th>\n",
       "      <th>Vc</th>\n",
       "      <th>fault_type</th>\n",
       "      <th>fault_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [G, C, B, A, Ia, Ib, Ic, Va, Vb, Vc, fault_type, fault_class]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa171c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OussamaTab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>B</th>\n",
       "      <th>A</th>\n",
       "      <th>Ia</th>\n",
       "      <th>Ib</th>\n",
       "      <th>Ic</th>\n",
       "      <th>Va</th>\n",
       "      <th>Vb</th>\n",
       "      <th>Vc</th>\n",
       "      <th>fault_type</th>\n",
       "      <th>fault_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6183</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-55.143378</td>\n",
       "      <td>96.111945</td>\n",
       "      <td>-44.337574</td>\n",
       "      <td>-0.514016</td>\n",
       "      <td>-0.025847</td>\n",
       "      <td>0.539862</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5205</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-880.573612</td>\n",
       "      <td>377.335059</td>\n",
       "      <td>503.236289</td>\n",
       "      <td>-0.022689</td>\n",
       "      <td>0.042364</td>\n",
       "      <td>-0.019674</td>\n",
       "      <td>1111</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6199</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-64.486428</td>\n",
       "      <td>89.603386</td>\n",
       "      <td>-28.480788</td>\n",
       "      <td>-0.413102</td>\n",
       "      <td>-0.174646</td>\n",
       "      <td>0.587748</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3126</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54.209175</td>\n",
       "      <td>-568.911763</td>\n",
       "      <td>517.219031</td>\n",
       "      <td>0.205024</td>\n",
       "      <td>0.031069</td>\n",
       "      <td>-0.236093</td>\n",
       "      <td>0110</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5456</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>562.816481</td>\n",
       "      <td>63.021761</td>\n",
       "      <td>6.863071</td>\n",
       "      <td>0.192229</td>\n",
       "      <td>-0.492558</td>\n",
       "      <td>0.300329</td>\n",
       "      <td>1111</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3691</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-865.095908</td>\n",
       "      <td>585.962104</td>\n",
       "      <td>281.237504</td>\n",
       "      <td>-0.011798</td>\n",
       "      <td>0.041151</td>\n",
       "      <td>-0.029353</td>\n",
       "      <td>0111</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-40.498894</td>\n",
       "      <td>784.478380</td>\n",
       "      <td>-741.470860</td>\n",
       "      <td>0.136708</td>\n",
       "      <td>-0.004891</td>\n",
       "      <td>-0.131817</td>\n",
       "      <td>0110</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>794.880620</td>\n",
       "      <td>-731.873006</td>\n",
       "      <td>-63.009877</td>\n",
       "      <td>0.001265</td>\n",
       "      <td>-0.037265</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>1111</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7842</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-70.120237</td>\n",
       "      <td>52.013561</td>\n",
       "      <td>15.234174</td>\n",
       "      <td>-0.038279</td>\n",
       "      <td>-0.493339</td>\n",
       "      <td>0.531617</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>404.616187</td>\n",
       "      <td>-882.023518</td>\n",
       "      <td>479.404069</td>\n",
       "      <td>-0.024406</td>\n",
       "      <td>-0.017768</td>\n",
       "      <td>0.042174</td>\n",
       "      <td>0111</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1573 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      G  C  B  A          Ia          Ib          Ic        Va        Vb  \\\n",
       "6183  0  0  0  0  -55.143378   96.111945  -44.337574 -0.514016 -0.025847   \n",
       "5205  1  1  1  1 -880.573612  377.335059  503.236289 -0.022689  0.042364   \n",
       "6199  0  0  0  0  -64.486428   89.603386  -28.480788 -0.413102 -0.174646   \n",
       "3126  0  1  1  0   54.209175 -568.911763  517.219031  0.205024  0.031069   \n",
       "5456  1  1  1  1  562.816481   63.021761    6.863071  0.192229 -0.492558   \n",
       "...  .. .. .. ..         ...         ...         ...       ...       ...   \n",
       "3691  0  1  1  1 -865.095908  585.962104  281.237504 -0.011798  0.041151   \n",
       "2971  0  1  1  0  -40.498894  784.478380 -741.470860  0.136708 -0.004891   \n",
       "4971  1  1  1  1  794.880620 -731.873006  -63.009877  0.001265 -0.037265   \n",
       "7842  0  0  0  0  -70.120237   52.013561   15.234174 -0.038279 -0.493339   \n",
       "4234  0  1  1  1  404.616187 -882.023518  479.404069 -0.024406 -0.017768   \n",
       "\n",
       "            Vc fault_type  fault_class  \n",
       "6183  0.539862       0000            0  \n",
       "5205 -0.019674       1111            6  \n",
       "6199  0.587748       0000            0  \n",
       "3126 -0.236093       0110            3  \n",
       "5456  0.300329       1111            6  \n",
       "...        ...        ...          ...  \n",
       "3691 -0.029353       0111            5  \n",
       "2971 -0.131817       0110            3  \n",
       "4971  0.036000       1111            6  \n",
       "7842  0.531617       0000            0  \n",
       "4234  0.042174       0111            5  \n",
       "\n",
       "[1573 rows x 12 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train , valid , test = np.split(df.sample(frac = 1) , [int(0.6 * len(df)) , int(0.8 * len(df))])\n",
    "# train  60% |||||||||||||\n",
    "# valid  20%  1234567890\n",
    "# test 20%\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b33ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataFrame):\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    x = dataFrame[['Ia', 'Ib', 'Ic', 'Va', 'Vb', 'Vc']].values\n",
    "    # Get the fault class values from the dataFrame parameter (not the global df)\n",
    "    y = dataFrame['fault_class'].values\n",
    "    \n",
    "    # Scale the input features\n",
    "    scaler = StandardScaler() \n",
    "    x = scaler.fit_transform(x)\n",
    "\n",
    "    # One-hot encode the target for categorical classification\n",
    "    y_cat = to_categorical(y, num_classes=7)\n",
    "    \n",
    "    return x, y_cat  # Return the one-hot encoded y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d92a9709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.25779106, -1.8601511 ,  0.62284151, -0.02038476, -0.11019885,\n",
       "         0.12993103],\n",
       "       [-1.89208126,  0.77016339,  1.472434  , -0.06191187,  0.11217192,\n",
       "        -0.05462961],\n",
       "       [-0.17148759,  0.31337061, -0.16041547, -1.24038587, -0.75202465,\n",
       "         1.92089321],\n",
       "       ...,\n",
       "       [ 1.63635684,  0.16688166, -0.02821615,  0.4673606 , -1.48181883,\n",
       "         1.05083626],\n",
       "       [-0.13220813,  0.21495135, -0.23144604, -1.25872136,  1.83966289,\n",
       "        -0.66742273],\n",
       "       [ 0.04802021,  0.05111479, -0.12577211,  1.77157642,  0.07151247,\n",
       "        -1.73558789]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-run these steps with the fixed function\n",
    "X_train, Y_train = scale_dataset(train)\n",
    "X_valid, Y_valid = scale_dataset(valid)\n",
    "X_test, Y_test = scale_dataset(test)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9443924e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.25779106, -1.8601511 ,  0.62284151, -0.02038476, -0.11019885,\n",
       "         0.12993103],\n",
       "       [-1.89208126,  0.77016339,  1.472434  , -0.06191187,  0.11217192,\n",
       "        -0.05462961],\n",
       "       [-0.17148759,  0.31337061, -0.16041547, -1.24038587, -0.75202465,\n",
       "         1.92089321],\n",
       "       ...,\n",
       "       [ 1.63635684,  0.16688166, -0.02821615,  0.4673606 , -1.48181883,\n",
       "         1.05083626],\n",
       "       [-0.13220813,  0.21495135, -0.23144604, -1.25872136,  1.83966289,\n",
       "        -0.66742273],\n",
       "       [ 0.04802021,  0.05111479, -0.12577211,  1.77157642,  0.07151247,\n",
       "        -1.73558789]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46110a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d1ef3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OussamaTab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Build the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(6,)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(7, activation='softmax')  # 7 classes for multiclass classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a00fcfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.2843 - loss: 1.7840 - val_accuracy: 0.6291 - val_loss: 1.0858\n",
      "Epoch 2/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6560 - loss: 0.9747 - val_accuracy: 0.7513 - val_loss: 0.6321\n",
      "Epoch 3/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7372 - loss: 0.6412 - val_accuracy: 0.8098 - val_loss: 0.4912\n",
      "Epoch 4/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7865 - loss: 0.5142 - val_accuracy: 0.8174 - val_loss: 0.4278\n",
      "Epoch 5/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7958 - loss: 0.4548 - val_accuracy: 0.8104 - val_loss: 0.3916\n",
      "Epoch 6/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7993 - loss: 0.4177 - val_accuracy: 0.8314 - val_loss: 0.3673\n",
      "Epoch 7/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8021 - loss: 0.4099 - val_accuracy: 0.8416 - val_loss: 0.3551\n",
      "Epoch 8/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8004 - loss: 0.3934 - val_accuracy: 0.8295 - val_loss: 0.3397\n",
      "Epoch 9/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8118 - loss: 0.3712 - val_accuracy: 0.8473 - val_loss: 0.3282\n",
      "Epoch 10/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8207 - loss: 0.3595 - val_accuracy: 0.8219 - val_loss: 0.3235\n",
      "Epoch 11/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8246 - loss: 0.3464 - val_accuracy: 0.8365 - val_loss: 0.3163\n",
      "Epoch 12/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8137 - loss: 0.3588 - val_accuracy: 0.8289 - val_loss: 0.3044\n",
      "Epoch 13/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8371 - loss: 0.3191 - val_accuracy: 0.8308 - val_loss: 0.3000\n",
      "Epoch 14/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8282 - loss: 0.3273 - val_accuracy: 0.8378 - val_loss: 0.2997\n",
      "Epoch 15/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8331 - loss: 0.3241 - val_accuracy: 0.8403 - val_loss: 0.2891\n",
      "Epoch 16/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8339 - loss: 0.3127 - val_accuracy: 0.8448 - val_loss: 0.2858\n",
      "Epoch 17/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8352 - loss: 0.3054 - val_accuracy: 0.8282 - val_loss: 0.2804\n",
      "Epoch 18/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8285 - loss: 0.3102 - val_accuracy: 0.8295 - val_loss: 0.2780\n",
      "Epoch 19/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8253 - loss: 0.3148 - val_accuracy: 0.8352 - val_loss: 0.2777\n",
      "Epoch 20/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8506 - loss: 0.2823 - val_accuracy: 0.8378 - val_loss: 0.2709\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Train the model\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, Y_train, \n",
    "    epochs=20, \n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid)  # Use separate validation data instead of validation_split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be6318db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       445\n",
      "           1       0.91      1.00      0.95       243\n",
      "           3       0.96      0.94      0.95       215\n",
      "           4       0.96      0.96      0.96       224\n",
      "           5       0.51      0.78      0.62       222\n",
      "           6       0.47      0.17      0.25       224\n",
      "\n",
      "    accuracy                           0.83      1573\n",
      "   macro avg       0.80      0.81      0.79      1573\n",
      "weighted avg       0.82      0.83      0.81      1573\n",
      "\n",
      "[[443   2   0   0   0   0]\n",
      " [  0 242   0   1   0   0]\n",
      " [ 12   0 203   0   0   0]\n",
      " [  0  10   0 214   0   0]\n",
      " [  0   0   6   1 173  42]\n",
      " [  0  12   2   8 164  38]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Make predictions on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert softmax probabilities to class predictions\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Convert one-hot encoded test labels back to class indices for comparison\n",
    "y_test_classes = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n",
    "print(confusion_matrix(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "750fed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "model_path = 'Knn_model.h5'\n",
    "save_model(model, model_path)\n",
    "# print(f\"Saved {model_type} model to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92504da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(self, model_type):\n",
    "        \"\"\"\n",
    "        Save a trained model to disk.\n",
    "        \n",
    "        Args:\n",
    "            model_type (str): Type of model to save\n",
    "        \"\"\"\n",
    "        if model_type not in self.models:\n",
    "            raise ValueError(f\"Model {model_type} not built yet. Call build_model first.\")\n",
    "        \n",
    "        model_path = os.path.join(self.output_dir, f'{model_type}_model.h5')\n",
    "        save_model(self.models[model_type], model_path)\n",
    "        print(f\"Saved {model_type} model to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58405359",
   "metadata": {},
   "source": [
    "## better Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d6be4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OussamaTab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#  1. Create a deeper, more complex model\n",
    "model_improved = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(6,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(7, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ff845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: np.float64(0.5578424414478353), 1: np.float64(1.1627218934911243), 2: np.float64(1.3187919463087248), 3: np.float64(1.1358381502890174), 4: np.float64(1.1575846833578793), 5: np.float64(1.1837349397590362)}\n"
     ]
    }
   ],
   "source": [
    "# 2. Use a different optimizer with a lower learning rate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# 3. Add class weights to handle class imbalance\n",
    "# Calculate class weights based on class frequencies\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Get class indices from the one-hot encoded training labels\n",
    "y_train_classes = np.argmax(Y_train, axis=1)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_classes),\n",
    "    y=y_train_classes\n",
    ")\n",
    "\n",
    "# Convert to dictionary for Keras\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(\"Class weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b48987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compile with additional metrics\n",
    "model_improved.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'AUC', 'Precision', 'Recall']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b0fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Use callbacks for early stopping and learning rate reduction\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d36c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - AUC: 0.7436 - Precision: 0.5140 - Recall: 0.0354 - accuracy: 0.3419 - loss: 1.6283 - val_AUC: 0.9638 - val_Precision: 0.7877 - val_Recall: 0.5642 - val_accuracy: 0.6991 - val_loss: 0.6887 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - AUC: 0.9545 - Precision: 0.7523 - Recall: 0.5313 - accuracy: 0.6773 - loss: 0.7363 - val_AUC: 0.9786 - val_Precision: 0.8041 - val_Recall: 0.7156 - val_accuracy: 0.7697 - val_loss: 0.4871 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - AUC: 0.9740 - Precision: 0.8098 - Recall: 0.6913 - accuracy: 0.7573 - loss: 0.5468 - val_AUC: 0.9843 - val_Precision: 0.8469 - val_Recall: 0.7602 - val_accuracy: 0.8066 - val_loss: 0.4103 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - AUC: 0.9782 - Precision: 0.8106 - Recall: 0.7171 - accuracy: 0.7642 - loss: 0.4953 - val_AUC: 0.9868 - val_Precision: 0.8349 - val_Recall: 0.7850 - val_accuracy: 0.8136 - val_loss: 0.3655 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - AUC: 0.9817 - Precision: 0.8210 - Recall: 0.7427 - accuracy: 0.7902 - loss: 0.4499 - val_AUC: 0.9886 - val_Precision: 0.8452 - val_Recall: 0.7920 - val_accuracy: 0.8193 - val_loss: 0.3390 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - AUC: 0.9820 - Precision: 0.8193 - Recall: 0.7498 - accuracy: 0.7904 - loss: 0.4376 - val_AUC: 0.9886 - val_Precision: 0.8365 - val_Recall: 0.8104 - val_accuracy: 0.8244 - val_loss: 0.3338 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - AUC: 0.9837 - Precision: 0.8122 - Recall: 0.7594 - accuracy: 0.7893 - loss: 0.4135 - val_AUC: 0.9897 - val_Precision: 0.8465 - val_Recall: 0.8206 - val_accuracy: 0.8365 - val_loss: 0.3165 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - AUC: 0.9844 - Precision: 0.8160 - Recall: 0.7663 - accuracy: 0.7914 - loss: 0.4019 - val_AUC: 0.9896 - val_Precision: 0.8418 - val_Recall: 0.8193 - val_accuracy: 0.8302 - val_loss: 0.3099 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - AUC: 0.9862 - Precision: 0.8311 - Recall: 0.7879 - accuracy: 0.8130 - loss: 0.3824 - val_AUC: 0.9906 - val_Precision: 0.8515 - val_Recall: 0.8244 - val_accuracy: 0.8429 - val_loss: 0.2969 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - AUC: 0.9874 - Precision: 0.8468 - Recall: 0.8019 - accuracy: 0.8221 - loss: 0.3685 - val_AUC: 0.9897 - val_Precision: 0.8316 - val_Recall: 0.8136 - val_accuracy: 0.8219 - val_loss: 0.2996 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - AUC: 0.9870 - Precision: 0.8311 - Recall: 0.7890 - accuracy: 0.8150 - loss: 0.3669 - val_AUC: 0.9906 - val_Precision: 0.8408 - val_Recall: 0.8263 - val_accuracy: 0.8340 - val_loss: 0.2875 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - AUC: 0.9886 - Precision: 0.8435 - Recall: 0.8063 - accuracy: 0.8242 - loss: 0.3426 - val_AUC: 0.9905 - val_Precision: 0.8464 - val_Recall: 0.8270 - val_accuracy: 0.8352 - val_loss: 0.2869 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - AUC: 0.9888 - Precision: 0.8419 - Recall: 0.8114 - accuracy: 0.8270 - loss: 0.3375 - val_AUC: 0.9905 - val_Precision: 0.8493 - val_Recall: 0.8314 - val_accuracy: 0.8391 - val_loss: 0.2852 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.9887 - Precision: 0.8337 - Recall: 0.8063 - accuracy: 0.8235 - loss: 0.3317 - val_AUC: 0.9909 - val_Precision: 0.8501 - val_Recall: 0.8333 - val_accuracy: 0.8422 - val_loss: 0.2751 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - AUC: 0.9887 - Precision: 0.8406 - Recall: 0.8145 - accuracy: 0.8258 - loss: 0.3375 - val_AUC: 0.9907 - val_Precision: 0.8443 - val_Recall: 0.8346 - val_accuracy: 0.8397 - val_loss: 0.2868 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - AUC: 0.9894 - Precision: 0.8415 - Recall: 0.8177 - accuracy: 0.8320 - loss: 0.3239 - val_AUC: 0.9897 - val_Precision: 0.8333 - val_Recall: 0.8142 - val_accuracy: 0.8232 - val_loss: 0.2874 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - AUC: 0.9889 - Precision: 0.8385 - Recall: 0.8106 - accuracy: 0.8275 - loss: 0.3259 - val_AUC: 0.9912 - val_Precision: 0.8462 - val_Recall: 0.8365 - val_accuracy: 0.8422 - val_loss: 0.2720 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - AUC: 0.9885 - Precision: 0.8299 - Recall: 0.8029 - accuracy: 0.8217 - loss: 0.3311 - val_AUC: 0.9914 - val_Precision: 0.8419 - val_Recall: 0.8263 - val_accuracy: 0.8314 - val_loss: 0.2597 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - AUC: 0.9891 - Precision: 0.8366 - Recall: 0.8133 - accuracy: 0.8244 - loss: 0.3253 - val_AUC: 0.9906 - val_Precision: 0.8358 - val_Recall: 0.8162 - val_accuracy: 0.8276 - val_loss: 0.2653 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - AUC: 0.9884 - Precision: 0.8273 - Recall: 0.8028 - accuracy: 0.8165 - loss: 0.3353 - val_AUC: 0.9912 - val_Precision: 0.8430 - val_Recall: 0.8263 - val_accuracy: 0.8365 - val_loss: 0.2595 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - AUC: 0.9890 - Precision: 0.8350 - Recall: 0.8075 - accuracy: 0.8239 - loss: 0.3241 - val_AUC: 0.9907 - val_Precision: 0.8431 - val_Recall: 0.8308 - val_accuracy: 0.8384 - val_loss: 0.2706 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - AUC: 0.9891 - Precision: 0.8292 - Recall: 0.8124 - accuracy: 0.8218 - loss: 0.3173 - val_AUC: 0.9915 - val_Precision: 0.8543 - val_Recall: 0.8391 - val_accuracy: 0.8467 - val_loss: 0.2630 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - AUC: 0.9899 - Precision: 0.8418 - Recall: 0.8190 - accuracy: 0.8346 - loss: 0.3075 - val_AUC: 0.9902 - val_Precision: 0.8341 - val_Recall: 0.8155 - val_accuracy: 0.8244 - val_loss: 0.2764 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - AUC: 0.9908 - Precision: 0.8514 - Recall: 0.8350 - accuracy: 0.8449 - loss: 0.3004 - val_AUC: 0.9908 - val_Precision: 0.8423 - val_Recall: 0.8327 - val_accuracy: 0.8372 - val_loss: 0.2657 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - AUC: 0.9901 - Precision: 0.8400 - Recall: 0.8208 - accuracy: 0.8331 - loss: 0.3005 - val_AUC: 0.9915 - val_Precision: 0.8511 - val_Recall: 0.8365 - val_accuracy: 0.8429 - val_loss: 0.2556 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - AUC: 0.9893 - Precision: 0.8295 - Recall: 0.8112 - accuracy: 0.8227 - loss: 0.3145 - val_AUC: 0.9897 - val_Precision: 0.8294 - val_Recall: 0.8225 - val_accuracy: 0.8263 - val_loss: 0.2877 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - AUC: 0.9904 - Precision: 0.8435 - Recall: 0.8247 - accuracy: 0.8360 - loss: 0.3012 - val_AUC: 0.9907 - val_Precision: 0.8463 - val_Recall: 0.8372 - val_accuracy: 0.8403 - val_loss: 0.2802 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - AUC: 0.9904 - Precision: 0.8455 - Recall: 0.8303 - accuracy: 0.8381 - loss: 0.3032 - val_AUC: 0.9908 - val_Precision: 0.8421 - val_Recall: 0.8346 - val_accuracy: 0.8372 - val_loss: 0.2685 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - AUC: 0.9903 - Precision: 0.8470 - Recall: 0.8305 - accuracy: 0.8375 - loss: 0.3032 - val_AUC: 0.9909 - val_Precision: 0.8323 - val_Recall: 0.8238 - val_accuracy: 0.8289 - val_loss: 0.2568 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m147/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - AUC: 0.9908 - Precision: 0.8443 - Recall: 0.8238 - accuracy: 0.8363 - loss: 0.2894\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - AUC: 0.9908 - Precision: 0.8443 - Recall: 0.8238 - accuracy: 0.8363 - loss: 0.2895 - val_AUC: 0.9914 - val_Precision: 0.8508 - val_Recall: 0.8416 - val_accuracy: 0.8448 - val_loss: 0.2559 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - AUC: 0.9906 - Precision: 0.8430 - Recall: 0.8245 - accuracy: 0.8335 - loss: 0.3023 - val_AUC: 0.9917 - val_Precision: 0.8380 - val_Recall: 0.8295 - val_accuracy: 0.8359 - val_loss: 0.2455 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - AUC: 0.9920 - Precision: 0.8585 - Recall: 0.8449 - accuracy: 0.8543 - loss: 0.2698 - val_AUC: 0.9917 - val_Precision: 0.8442 - val_Recall: 0.8378 - val_accuracy: 0.8384 - val_loss: 0.2455 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - AUC: 0.9919 - Precision: 0.8556 - Recall: 0.8445 - accuracy: 0.8489 - loss: 0.2691 - val_AUC: 0.9918 - val_Precision: 0.8472 - val_Recall: 0.8327 - val_accuracy: 0.8372 - val_loss: 0.2488 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - AUC: 0.9918 - Precision: 0.8523 - Recall: 0.8387 - accuracy: 0.8451 - loss: 0.2702 - val_AUC: 0.9916 - val_Precision: 0.8404 - val_Recall: 0.8340 - val_accuracy: 0.8340 - val_loss: 0.2483 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - AUC: 0.9910 - Precision: 0.8480 - Recall: 0.8324 - accuracy: 0.8409 - loss: 0.2861 - val_AUC: 0.9920 - val_Precision: 0.8424 - val_Recall: 0.8295 - val_accuracy: 0.8327 - val_loss: 0.2380 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - AUC: 0.9913 - Precision: 0.8517 - Recall: 0.8345 - accuracy: 0.8433 - loss: 0.2811 - val_AUC: 0.9915 - val_Precision: 0.8430 - val_Recall: 0.8333 - val_accuracy: 0.8365 - val_loss: 0.2437 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - AUC: 0.9920 - Precision: 0.8584 - Recall: 0.8446 - accuracy: 0.8504 - loss: 0.2690 - val_AUC: 0.9918 - val_Precision: 0.8441 - val_Recall: 0.8302 - val_accuracy: 0.8384 - val_loss: 0.2426 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - AUC: 0.9926 - Precision: 0.8632 - Recall: 0.8466 - accuracy: 0.8557 - loss: 0.2644 - val_AUC: 0.9923 - val_Precision: 0.8539 - val_Recall: 0.8441 - val_accuracy: 0.8486 - val_loss: 0.2412 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - AUC: 0.9917 - Precision: 0.8518 - Recall: 0.8346 - accuracy: 0.8434 - loss: 0.2732 - val_AUC: 0.9920 - val_Precision: 0.8487 - val_Recall: 0.8422 - val_accuracy: 0.8454 - val_loss: 0.2405 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m145/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - AUC: 0.9911 - Precision: 0.8481 - Recall: 0.8315 - accuracy: 0.8418 - loss: 0.2848\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - AUC: 0.9911 - Precision: 0.8483 - Recall: 0.8317 - accuracy: 0.8420 - loss: 0.2845 - val_AUC: 0.9918 - val_Precision: 0.8469 - val_Recall: 0.8378 - val_accuracy: 0.8416 - val_loss: 0.2487 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - AUC: 0.9918 - Precision: 0.8524 - Recall: 0.8381 - accuracy: 0.8447 - loss: 0.2663 - val_AUC: 0.9920 - val_Precision: 0.8462 - val_Recall: 0.8397 - val_accuracy: 0.8416 - val_loss: 0.2410 - learning_rate: 2.5000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - AUC: 0.9927 - Precision: 0.8641 - Recall: 0.8511 - accuracy: 0.8571 - loss: 0.2553 - val_AUC: 0.9918 - val_Precision: 0.8453 - val_Recall: 0.8378 - val_accuracy: 0.8416 - val_loss: 0.2383 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9920 - Precision: 0.8664 - Recall: 0.8493 - accuracy: 0.8613 - loss: 0.2705 - val_AUC: 0.9919 - val_Precision: 0.8491 - val_Recall: 0.8340 - val_accuracy: 0.8372 - val_loss: 0.2334 - learning_rate: 2.5000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - AUC: 0.9920 - Precision: 0.8534 - Recall: 0.8388 - accuracy: 0.8480 - loss: 0.2660 - val_AUC: 0.9920 - val_Precision: 0.8442 - val_Recall: 0.8378 - val_accuracy: 0.8410 - val_loss: 0.2393 - learning_rate: 2.5000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - AUC: 0.9919 - Precision: 0.8531 - Recall: 0.8407 - accuracy: 0.8471 - loss: 0.2647 - val_AUC: 0.9922 - val_Precision: 0.8470 - val_Recall: 0.8416 - val_accuracy: 0.8435 - val_loss: 0.2352 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - AUC: 0.9926 - Precision: 0.8626 - Recall: 0.8495 - accuracy: 0.8576 - loss: 0.2568 - val_AUC: 0.9922 - val_Precision: 0.8543 - val_Recall: 0.8467 - val_accuracy: 0.8505 - val_loss: 0.2381 - learning_rate: 2.5000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - AUC: 0.9926 - Precision: 0.8594 - Recall: 0.8503 - accuracy: 0.8561 - loss: 0.2588 - val_AUC: 0.9923 - val_Precision: 0.8466 - val_Recall: 0.8359 - val_accuracy: 0.8441 - val_loss: 0.2321 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - AUC: 0.9929 - Precision: 0.8650 - Recall: 0.8515 - accuracy: 0.8604 - loss: 0.2507 - val_AUC: 0.9923 - val_Precision: 0.8514 - val_Recall: 0.8422 - val_accuracy: 0.8486 - val_loss: 0.2372 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - AUC: 0.9921 - Precision: 0.8576 - Recall: 0.8476 - accuracy: 0.8545 - loss: 0.2631 - val_AUC: 0.9920 - val_Precision: 0.8463 - val_Recall: 0.8333 - val_accuracy: 0.8391 - val_loss: 0.2297 - learning_rate: 2.5000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - AUC: 0.9921 - Precision: 0.8546 - Recall: 0.8418 - accuracy: 0.8513 - loss: 0.2669 - val_AUC: 0.9921 - val_Precision: 0.8524 - val_Recall: 0.8410 - val_accuracy: 0.8422 - val_loss: 0.2364 - learning_rate: 2.5000e-04\n",
      "Restoring model weights from the end of the best epoch: 49.\n"
     ]
    }
   ],
   "source": [
    "# 6. Train with more epochs and class weights\n",
    "history = model_improved.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=50,  # Increase epochs, early stopping will prevent overfitting\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid),\n",
    "    class_weight=class_weight_dict,  # Apply class weights\n",
    "    callbacks=[early_stopping, lr_reduction],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b992b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "\n",
      "Improved Model Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       477\n",
      "           1       0.94      0.99      0.96       208\n",
      "           3       0.98      0.99      0.98       198\n",
      "           4       1.00      0.95      0.97       226\n",
      "           5       0.49      0.58      0.53       214\n",
      "           6       0.57      0.48      0.52       250\n",
      "\n",
      "    accuracy                           0.85      1573\n",
      "   macro avg       0.83      0.83      0.83      1573\n",
      "weighted avg       0.85      0.85      0.85      1573\n",
      "\n",
      "Confusion Matrix:\n",
      "[[473   0   4   0   0   0]\n",
      " [  1 206   0   1   0   0]\n",
      " [  1   0 196   0   1   0]\n",
      " [  1  10   0 215   0   0]\n",
      " [  0   0   0   0 124  90]\n",
      " [  0   4   0   0 126 120]]\n"
     ]
    }
   ],
   "source": [
    "# 7. Evaluate the improved model\n",
    "y_pred = model_improved.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(Y_test, axis=1)\n",
    "\n",
    "print(\"\\nImproved Model Evaluation:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4674559d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8. Visualize training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SMOTE: Synthetic Minority Over-sampling Technique\n",
    "# This will help with the class imbalance problem, particularly for class 6\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def apply_smote_balancing():\n",
    "    # Get the class indices from one-hot encoded labels\n",
    "    y_train_classes = np.argmax(Y_train, axis=1)\n",
    "    \n",
    "    # Apply SMOTE to generate synthetic samples for minority classes\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train_classes)\n",
    "    \n",
    "    # Convert back to one-hot encoding\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    y_resampled_cat = to_categorical(y_resampled, num_classes=7)\n",
    "    \n",
    "    print(f\"Original shape: {X_train.shape}, Resampled shape: {X_resampled.shape}\")\n",
    "    print(\"Class distribution before SMOTE:\", np.bincount(y_train_classes))\n",
    "    print(\"Class distribution after SMOTE:\", np.bincount(y_resampled))\n",
    "    \n",
    "    return X_resampled, y_resampled_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Focal Loss - To focus more on hard-to-classify examples like class 6\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        \n",
    "        epsilon = K.epsilon()\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "        \n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - \\\n",
    "               K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bab0e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Feature importance analysis and selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "def analyze_feature_importance(X, y, feature_names=None):\n",
    "    # Convert one-hot encoded labels to class indices if needed\n",
    "    if len(y.shape) > 1 and y.shape[1] > 1:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    \n",
    "    # Select k best features\n",
    "    selector = SelectKBest(f_classif, k='all')\n",
    "    selector.fit(X, y)\n",
    "    \n",
    "    # Get scores\n",
    "    scores = selector.scores_\n",
    "    \n",
    "    # If feature names are provided, display them with scores\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"Feature {i}\" for i in range(X.shape[1])]\n",
    "    \n",
    "    # Create dataframe of features and their scores\n",
    "    import pandas as pd\n",
    "    feature_scores = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Score': scores\n",
    "    })\n",
    "    \n",
    "    # Sort by score\n",
    "    feature_scores = feature_scores.sort_values('Score', ascending=False)\n",
    "    print(feature_scores)\n",
    "    \n",
    "    return feature_scores, selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce9a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Advanced model with attention mechanism\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, InputSpec, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\")\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        et = K.squeeze(K.tanh(K.dot(x, self.W) + self.b), axis=-1)\n",
    "        at = K.softmax(et)\n",
    "        at = K.expand_dims(at, axis=-1)\n",
    "        output = x * at\n",
    "        return K.sum(output, axis=1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def create_attention_model(input_shape):\n",
    "    inputs = Input(shape=(input_shape,))\n",
    "    \n",
    "    # First dense block\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Reshape for attention\n",
    "    reshaped = tf.keras.layers.Reshape((4, 32))(x)\n",
    "    \n",
    "    # Apply attention\n",
    "    attention_output = AttentionLayer()(reshaped)\n",
    "    \n",
    "    # Additional dense layers\n",
    "    x = Dense(64, activation='relu')(attention_output)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(7, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "        metrics=['accuracy', 'AUC', 'Precision', 'Recall']\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da989b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Hard example mining - focus training on the hardest examples\n",
    "def train_with_hard_example_mining(model, X_train, Y_train, X_valid, Y_valid, epochs=50):\n",
    "    batch_size = 32\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_batches = n_samples // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Calculate loss for each sample\n",
    "        sample_losses = []\n",
    "        for i in range(n_samples):\n",
    "            x_sample = X_train[i:i+1]\n",
    "            y_sample = Y_train[i:i+1]\n",
    "            loss = model.test_on_batch(x_sample, y_sample)[0]\n",
    "            sample_losses.append((i, loss))\n",
    "        \n",
    "        # Sort samples by loss (hardest examples first)\n",
    "        sample_losses.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Take the top 50% hardest examples\n",
    "        hard_indices = [x[0] for x in sample_losses[:n_samples//2]]\n",
    "        \n",
    "        # Train on hard examples\n",
    "        X_hard = X_train[hard_indices]\n",
    "        Y_hard = Y_train[hard_indices]\n",
    "        \n",
    "        model.fit(\n",
    "            X_hard, Y_hard,\n",
    "            epochs=1,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_acc = model.evaluate(X_valid, Y_valid, verbose=0)\n",
    "        print(f\"Validation loss: {val_loss:.4f}, accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac5ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Multi-stage training: First train on all data, then fine-tune on problem classes\n",
    "def multi_stage_training():\n",
    "    # First stage: Train on all data\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(7, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, Y_train,\n",
    "        epochs=25,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_valid, Y_valid),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Second stage: Focus on problem classes (5 and 6)\n",
    "    y_train_classes = np.argmax(Y_train, axis=1)\n",
    "    problem_indices = np.where((y_train_classes == 5) | (y_train_classes == 6))[0]\n",
    "    \n",
    "    # Also include some samples from other classes to prevent forgetting\n",
    "    other_indices = np.where((y_train_classes != 5) & (y_train_classes != 6))[0]\n",
    "    np.random.shuffle(other_indices)\n",
    "    selected_other = other_indices[:len(problem_indices)]\n",
    "    \n",
    "    fine_tune_indices = np.concatenate([problem_indices, selected_other])\n",
    "    X_fine_tune = X_train[fine_tune_indices]\n",
    "    Y_fine_tune = Y_train[fine_tune_indices]\n",
    "    \n",
    "    # Fine-tune with a very low learning rate\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_fine_tune, Y_fine_tune,\n",
    "        epochs=15,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_valid, Y_valid),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36008bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Custom Metrics for Class 6\n",
    "class ClassSixRecall(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='class_six_recall', **kwargs):\n",
    "        super(ClassSixRecall, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.argmax(y_true, axis=1)\n",
    "        y_pred = tf.argmax(y_pred, axis=1)\n",
    "        \n",
    "        # Find class 6 samples\n",
    "        mask = tf.equal(y_true, 6)\n",
    "        \n",
    "        # Calculate true positives for class 6\n",
    "        tp = tf.cast(tf.logical_and(mask, tf.equal(y_pred, 6)), tf.float32)\n",
    "        self.true_positives.assign_add(tf.reduce_sum(tp))\n",
    "        \n",
    "        # Calculate false negatives for class 6\n",
    "        fn = tf.cast(tf.logical_and(mask, tf.not_equal(y_pred, 6)), tf.float32)\n",
    "        self.false_negatives.assign_add(tf.reduce_sum(fn))\n",
    "    \n",
    "    def result(self):\n",
    "        return self.true_positives / (self.true_positives + self.false_negatives + K.epsilon())\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0)\n",
    "        self.false_negatives.assign(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e78d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (4716, 6), Resampled shape: (8454, 6)\n",
      "Class distribution before SMOTE: [1409  676    0  596  692  679  664]\n",
      "Class distribution after SMOTE: [1409 1409    0 1409 1409 1409 1409]\n",
      "Epoch 1/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 26ms/step - AUC: 0.7444 - Precision: 0.4088 - Recall: 0.0072 - accuracy: 0.3209 - loss: 0.0436 - val_AUC: 0.9647 - val_Precision: 1.0000 - val_Recall: 0.1991 - val_accuracy: 0.7322 - val_loss: 0.0183 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - AUC: 0.9458 - Precision: 0.8759 - Recall: 0.2436 - accuracy: 0.6574 - loss: 0.0209 - val_AUC: 0.9797 - val_Precision: 0.9928 - val_Recall: 0.4389 - val_accuracy: 0.8041 - val_loss: 0.0135 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - AUC: 0.9649 - Precision: 0.9050 - Recall: 0.4031 - accuracy: 0.7069 - loss: 0.0168 - val_AUC: 0.9844 - val_Precision: 0.9418 - val_Recall: 0.5770 - val_accuracy: 0.8104 - val_loss: 0.0116 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - AUC: 0.9728 - Precision: 0.9250 - Recall: 0.4826 - accuracy: 0.7510 - loss: 0.0146 - val_AUC: 0.9858 - val_Precision: 0.9735 - val_Recall: 0.5846 - val_accuracy: 0.8340 - val_loss: 0.0109 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - AUC: 0.9748 - Precision: 0.9241 - Recall: 0.4987 - accuracy: 0.7518 - loss: 0.0141 - val_AUC: 0.9875 - val_Precision: 0.9571 - val_Recall: 0.6380 - val_accuracy: 0.8346 - val_loss: 0.0102 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - AUC: 0.9779 - Precision: 0.9438 - Recall: 0.5324 - accuracy: 0.7626 - loss: 0.0131 - val_AUC: 0.9871 - val_Precision: 0.9602 - val_Recall: 0.6298 - val_accuracy: 0.8168 - val_loss: 0.0101 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - AUC: 0.9798 - Precision: 0.9522 - Recall: 0.5506 - accuracy: 0.7749 - loss: 0.0125 - val_AUC: 0.9879 - val_Precision: 0.9403 - val_Recall: 0.6310 - val_accuracy: 0.8340 - val_loss: 0.0097 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - AUC: 0.9810 - Precision: 0.9430 - Recall: 0.5547 - accuracy: 0.7761 - loss: 0.0121 - val_AUC: 0.9888 - val_Precision: 0.9888 - val_Recall: 0.6170 - val_accuracy: 0.8257 - val_loss: 0.0094 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - AUC: 0.9823 - Precision: 0.9561 - Recall: 0.5728 - accuracy: 0.7873 - loss: 0.0116 - val_AUC: 0.9898 - val_Precision: 0.9840 - val_Recall: 0.6247 - val_accuracy: 0.8295 - val_loss: 0.0090 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - AUC: 0.9827 - Precision: 0.9537 - Recall: 0.5798 - accuracy: 0.7908 - loss: 0.0114 - val_AUC: 0.9903 - val_Precision: 0.9930 - val_Recall: 0.6336 - val_accuracy: 0.8321 - val_loss: 0.0087 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - AUC: 0.9834 - Precision: 0.9590 - Recall: 0.5848 - accuracy: 0.7956 - loss: 0.0113 - val_AUC: 0.9902 - val_Precision: 0.9902 - val_Recall: 0.6406 - val_accuracy: 0.8257 - val_loss: 0.0087 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - AUC: 0.9836 - Precision: 0.9658 - Recall: 0.5797 - accuracy: 0.7896 - loss: 0.0111 - val_AUC: 0.9906 - val_Precision: 0.9792 - val_Recall: 0.6896 - val_accuracy: 0.8346 - val_loss: 0.0084 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - AUC: 0.9839 - Precision: 0.9575 - Recall: 0.5874 - accuracy: 0.7971 - loss: 0.0110 - val_AUC: 0.9897 - val_Precision: 0.9828 - val_Recall: 0.6552 - val_accuracy: 0.8270 - val_loss: 0.0088 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - AUC: 0.9855 - Precision: 0.9728 - Recall: 0.6059 - accuracy: 0.7962 - loss: 0.0104 - val_AUC: 0.9895 - val_Precision: 0.9686 - val_Recall: 0.6679 - val_accuracy: 0.8193 - val_loss: 0.0089 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - AUC: 0.9851 - Precision: 0.9681 - Recall: 0.6021 - accuracy: 0.7986 - loss: 0.0105 - val_AUC: 0.9905 - val_Precision: 0.9887 - val_Recall: 0.6654 - val_accuracy: 0.8327 - val_loss: 0.0083 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - AUC: 0.9855 - Precision: 0.9694 - Recall: 0.5952 - accuracy: 0.8032 - loss: 0.0103 - val_AUC: 0.9901 - val_Precision: 0.9769 - val_Recall: 0.6718 - val_accuracy: 0.8352 - val_loss: 0.0086 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - AUC: 0.9858 - Precision: 0.9727 - Recall: 0.6008 - accuracy: 0.8037 - loss: 0.0102 - val_AUC: 0.9910 - val_Precision: 0.9890 - val_Recall: 0.6858 - val_accuracy: 0.8391 - val_loss: 0.0080 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - AUC: 0.9870 - Precision: 0.9757 - Recall: 0.6206 - accuracy: 0.8116 - loss: 0.0097 - val_AUC: 0.9905 - val_Precision: 0.9833 - val_Recall: 0.6749 - val_accuracy: 0.8327 - val_loss: 0.0082 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - AUC: 0.9861 - Precision: 0.9715 - Recall: 0.6069 - accuracy: 0.8027 - loss: 0.0100 - val_AUC: 0.9913 - val_Precision: 0.9863 - val_Recall: 0.6851 - val_accuracy: 0.8378 - val_loss: 0.0078 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - AUC: 0.9866 - Precision: 0.9702 - Recall: 0.6125 - accuracy: 0.8089 - loss: 0.0099 - val_AUC: 0.9903 - val_Precision: 0.9773 - val_Recall: 0.6851 - val_accuracy: 0.8193 - val_loss: 0.0082 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - AUC: 0.9871 - Precision: 0.9785 - Recall: 0.6202 - accuracy: 0.8107 - loss: 0.0095 - val_AUC: 0.9916 - val_Precision: 0.9890 - val_Recall: 0.6870 - val_accuracy: 0.8378 - val_loss: 0.0077 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - AUC: 0.9880 - Precision: 0.9789 - Recall: 0.6384 - accuracy: 0.8164 - loss: 0.0093 - val_AUC: 0.9909 - val_Precision: 0.9817 - val_Recall: 0.6807 - val_accuracy: 0.8282 - val_loss: 0.0079 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - AUC: 0.9871 - Precision: 0.9795 - Recall: 0.6162 - accuracy: 0.8144 - loss: 0.0096 - val_AUC: 0.9906 - val_Precision: 0.9787 - val_Recall: 0.7029 - val_accuracy: 0.8314 - val_loss: 0.0079 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - AUC: 0.9881 - Precision: 0.9787 - Recall: 0.6309 - accuracy: 0.8192 - loss: 0.0092 - val_AUC: 0.9916 - val_Precision: 0.9892 - val_Recall: 0.6978 - val_accuracy: 0.8397 - val_loss: 0.0075 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - AUC: 0.9872 - Precision: 0.9759 - Recall: 0.6275 - accuracy: 0.8106 - loss: 0.0094 - val_AUC: 0.9912 - val_Precision: 0.9779 - val_Recall: 0.7023 - val_accuracy: 0.8384 - val_loss: 0.0077 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - AUC: 0.9876 - Precision: 0.9780 - Recall: 0.6291 - accuracy: 0.8072 - loss: 0.0093 - val_AUC: 0.9913 - val_Precision: 0.9796 - val_Recall: 0.7029 - val_accuracy: 0.8384 - val_loss: 0.0077 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - AUC: 0.9879 - Precision: 0.9817 - Recall: 0.6237 - accuracy: 0.8144 - loss: 0.0091 - val_AUC: 0.9909 - val_Precision: 0.9800 - val_Recall: 0.6864 - val_accuracy: 0.8333 - val_loss: 0.0077 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - AUC: 0.9885 - Precision: 0.9805 - Recall: 0.6351 - accuracy: 0.8197 - loss: 0.0090 - val_AUC: 0.9916 - val_Precision: 0.9856 - val_Recall: 0.6947 - val_accuracy: 0.8352 - val_loss: 0.0075 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - AUC: 0.9877 - Precision: 0.9786 - Recall: 0.6245 - accuracy: 0.8137 - loss: 0.0092 - val_AUC: 0.9920 - val_Precision: 0.9928 - val_Recall: 0.6972 - val_accuracy: 0.8429 - val_loss: 0.0071 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - AUC: 0.9877 - Precision: 0.9785 - Recall: 0.6249 - accuracy: 0.8153 - loss: 0.0092 - val_AUC: 0.9923 - val_Precision: 0.9875 - val_Recall: 0.7017 - val_accuracy: 0.8499 - val_loss: 0.0072 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 23ms/step - AUC: 0.9891 - Precision: 0.9808 - Recall: 0.6424 - accuracy: 0.8234 - loss: 0.0087 - val_AUC: 0.9902 - val_Precision: 0.9651 - val_Recall: 0.6858 - val_accuracy: 0.8257 - val_loss: 0.0083 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - AUC: 0.9882 - Precision: 0.9797 - Recall: 0.6330 - accuracy: 0.8189 - loss: 0.0090 - val_AUC: 0.9914 - val_Precision: 0.9748 - val_Recall: 0.7137 - val_accuracy: 0.8321 - val_loss: 0.0074 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - AUC: 0.9889 - Precision: 0.9826 - Recall: 0.6395 - accuracy: 0.8259 - loss: 0.0088 - val_AUC: 0.9899 - val_Precision: 0.9674 - val_Recall: 0.6794 - val_accuracy: 0.8333 - val_loss: 0.0086 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - AUC: 0.9883 - Precision: 0.9813 - Recall: 0.6350 - accuracy: 0.8158 - loss: 0.0089 - val_AUC: 0.9927 - val_Precision: 0.9893 - val_Recall: 0.7087 - val_accuracy: 0.8543 - val_loss: 0.0069 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - AUC: 0.9887 - Precision: 0.9830 - Recall: 0.6333 - accuracy: 0.8251 - loss: 0.0088 - val_AUC: 0.9917 - val_Precision: 0.9893 - val_Recall: 0.7048 - val_accuracy: 0.8448 - val_loss: 0.0072 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step - AUC: 0.9899 - Precision: 0.9823 - Recall: 0.6489 - accuracy: 0.8408 - loss: 0.0085 - val_AUC: 0.9920 - val_Precision: 0.9901 - val_Recall: 0.6978 - val_accuracy: 0.8492 - val_loss: 0.0071 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - AUC: 0.9891 - Precision: 0.9881 - Recall: 0.6403 - accuracy: 0.8268 - loss: 0.0086 - val_AUC: 0.9913 - val_Precision: 0.9794 - val_Recall: 0.6940 - val_accuracy: 0.8467 - val_loss: 0.0075 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - AUC: 0.9891 - Precision: 0.9835 - Recall: 0.6426 - accuracy: 0.8226 - loss: 0.0086 - val_AUC: 0.9911 - val_Precision: 0.9679 - val_Recall: 0.6908 - val_accuracy: 0.8448 - val_loss: 0.0082 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - AUC: 0.9883 - Precision: 0.9800 - Recall: 0.6397 - accuracy: 0.8212 - loss: 0.0089 - val_AUC: 0.9914 - val_Precision: 0.9768 - val_Recall: 0.6959 - val_accuracy: 0.8435 - val_loss: 0.0076 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - AUC: 0.9896 - Precision: 0.9870 - Recall: 0.6470 - accuracy: 0.8312 - loss: 0.0083 - val_AUC: 0.9920 - val_Precision: 0.9839 - val_Recall: 0.7004 - val_accuracy: 0.8403 - val_loss: 0.0070 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - AUC: 0.9904 - Precision: 0.9862 - Recall: 0.6573 - accuracy: 0.8400 - loss: 0.0081 - val_AUC: 0.9925 - val_Precision: 0.9848 - val_Recall: 0.6997 - val_accuracy: 0.8499 - val_loss: 0.0070 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.9897 - Precision: 0.9852 - Recall: 0.6495 - accuracy: 0.8391 - loss: 0.0083 - val_AUC: 0.9925 - val_Precision: 0.9848 - val_Recall: 0.7023 - val_accuracy: 0.8543 - val_loss: 0.0070 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - AUC: 0.9905 - Precision: 0.9888 - Recall: 0.6609 - accuracy: 0.8379 - loss: 0.0080 - val_AUC: 0.9923 - val_Precision: 0.9892 - val_Recall: 0.7017 - val_accuracy: 0.8441 - val_loss: 0.0070 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - AUC: 0.9903 - Precision: 0.9849 - Recall: 0.6477 - accuracy: 0.8426 - loss: 0.0082 - val_AUC: 0.9919 - val_Precision: 0.9875 - val_Recall: 0.7010 - val_accuracy: 0.8403 - val_loss: 0.0070 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Decision boundary analysis for classes 5 and 6\n",
    "def analyze_decision_boundaries():\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(Y_test, axis=1)\n",
    "    \n",
    "    # Find misclassifications between classes 5 and 6\n",
    "    misclassified = []\n",
    "    for i in range(len(y_test_classes)):\n",
    "        if (y_test_classes[i] == 6 and y_pred_classes[i] == 5) or \\\n",
    "           (y_test_classes[i] == 5 and y_pred_classes[i] == 6):\n",
    "            misclassified.append(i)\n",
    "    \n",
    "    # Get class 5 and 6 indices\n",
    "    class5_indices = np.where(y_test_classes == 5)[0]\n",
    "    class6_indices = np.where(y_test_classes == 6)[0]\n",
    "    \n",
    "    # Combine indices\n",
    "    analysis_indices = np.concatenate([class5_indices, class6_indices])\n",
    "    \n",
    "    # Get features and labels for analysis\n",
    "    X_analysis = X_test[analysis_indices]\n",
    "    y_analysis = y_test_classes[analysis_indices]\n",
    "    \n",
    "    # Apply t-SNE for dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_analysis)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, c in zip([5, 6], ['blue', 'red']):\n",
    "        idx = np.where(y_analysis == i)\n",
    "        plt.scatter(X_tsne[idx, 0], X_tsne[idx, 1], c=c, label=f'Class {i}')\n",
    "    \n",
    "    # Highlight misclassified points\n",
    "    mis_idx = [list(analysis_indices).index(i) for i in misclassified if i in analysis_indices]\n",
    "    plt.scatter(X_tsne[mis_idx, 0], X_tsne[mis_idx, 1], c='yellow', marker='x', s=100, label='Misclassified')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title('t-SNE visualization of classes 5 and 6')\n",
    "    plt.show()\n",
    "    \n",
    "    return X_analysis, y_analysis, misclassified\n",
    "\n",
    "# Example of how to use these methods:\n",
    "# 1. First, balance the training data with SMOTE\n",
    "X_resampled, Y_resampled = apply_smote_balancing()\n",
    "\n",
    "# 2. Train an advanced model with balanced data and focal loss\n",
    "input_shape = X_resampled.shape[1]\n",
    "advanced_model = create_attention_model(input_shape)\n",
    "\n",
    "# 3. Train with early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "\n",
    "history = advanced_model.fit(\n",
    "    X_resampled, Y_resampled,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid),\n",
    "    callbacks=[early_stopping, lr_reduction],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a56c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n",
      "\n",
      "Advanced Model Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       477\n",
      "           1       0.97      0.98      0.97       208\n",
      "           3       0.99      0.99      0.99       198\n",
      "           4       0.99      0.96      0.98       226\n",
      "           5       0.79      0.10      0.18       214\n",
      "           6       0.56      0.98      0.71       250\n",
      "\n",
      "    accuracy                           0.87      1573\n",
      "   macro avg       0.88      0.84      0.81      1573\n",
      "weighted avg       0.89      0.87      0.83      1573\n",
      "\n",
      "Confusion Matrix:\n",
      "[[477   0   0   0   0   0]\n",
      " [  2 203   0   2   0   1]\n",
      " [  1   0 197   0   0   0]\n",
      " [  0   7   1 218   0   0]\n",
      " [  0   0   1   0  22 191]\n",
      " [  0   0   0   0   6 244]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Evaluate the advanced model\n",
    "y_pred = advanced_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(Y_test, axis=1)\n",
    "\n",
    "print(\"\\nAdvanced Model Evaluation:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dbf9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OussamaTab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(6,)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12722001",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efc28d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.3497 - loss: 1.6338 - val_accuracy: 0.6667 - val_loss: 0.9094\n",
      "Epoch 2/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6789 - loss: 0.8568 - val_accuracy: 0.7894 - val_loss: 0.5695\n",
      "Epoch 3/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7718 - loss: 0.5865 - val_accuracy: 0.8034 - val_loss: 0.4614\n",
      "Epoch 4/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7804 - loss: 0.4890 - val_accuracy: 0.8276 - val_loss: 0.4100\n",
      "Epoch 5/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8055 - loss: 0.4402 - val_accuracy: 0.8276 - val_loss: 0.3783\n",
      "Epoch 6/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8141 - loss: 0.4133 - val_accuracy: 0.8359 - val_loss: 0.3547\n",
      "Epoch 7/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8109 - loss: 0.3901 - val_accuracy: 0.8238 - val_loss: 0.3438\n",
      "Epoch 8/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8148 - loss: 0.3682 - val_accuracy: 0.8251 - val_loss: 0.3442\n",
      "Epoch 9/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8276 - loss: 0.3428 - val_accuracy: 0.8397 - val_loss: 0.3284\n",
      "Epoch 10/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8258 - loss: 0.3385 - val_accuracy: 0.8359 - val_loss: 0.3155\n",
      "Epoch 11/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8313 - loss: 0.3355 - val_accuracy: 0.8289 - val_loss: 0.3186\n",
      "Epoch 12/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8325 - loss: 0.3189 - val_accuracy: 0.8422 - val_loss: 0.3074\n",
      "Epoch 13/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8317 - loss: 0.3154 - val_accuracy: 0.8372 - val_loss: 0.2969\n",
      "Epoch 14/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8385 - loss: 0.3112 - val_accuracy: 0.8384 - val_loss: 0.3093\n",
      "Epoch 15/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8355 - loss: 0.3001 - val_accuracy: 0.8352 - val_loss: 0.3058\n",
      "Epoch 16/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8403 - loss: 0.2967 - val_accuracy: 0.8308 - val_loss: 0.2908\n",
      "Epoch 17/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.8316 - loss: 0.2945 - val_accuracy: 0.8384 - val_loss: 0.2924\n",
      "Epoch 18/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8431 - loss: 0.2890 - val_accuracy: 0.8295 - val_loss: 0.2887\n",
      "Epoch 19/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8492 - loss: 0.2861 - val_accuracy: 0.8340 - val_loss: 0.2840\n",
      "Epoch 20/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8357 - loss: 0.2901 - val_accuracy: 0.8340 - val_loss: 0.2885\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       477\n",
      "           1       0.90      0.99      0.94       208\n",
      "           3       0.96      0.96      0.96       198\n",
      "           4       0.96      0.93      0.94       226\n",
      "           5       0.48      0.79      0.60       214\n",
      "           6       0.57      0.22      0.31       250\n",
      "\n",
      "    accuracy                           0.83      1573\n",
      "   macro avg       0.81      0.81      0.79      1573\n",
      "weighted avg       0.83      0.83      0.81      1573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid)\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict classes\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # if one-hot encoded\n",
    "y_true = Y_test.argmax(axis=1)          # or just use Y_test if sparse\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OussamaTab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv1D(64, 3, activation='relu', input_shape=(6, 1)),  # Use Conv2D for images\n",
    "    MaxPooling1D(2),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1809a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625dd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.4358 - loss: 1.5740 - val_accuracy: 0.6253 - val_loss: 0.8674\n",
      "Epoch 2/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.6758 - loss: 0.7849 - val_accuracy: 0.7087 - val_loss: 0.6419\n",
      "Epoch 3/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7315 - loss: 0.6271 - val_accuracy: 0.7621 - val_loss: 0.5387\n",
      "Epoch 4/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7661 - loss: 0.5253 - val_accuracy: 0.7767 - val_loss: 0.4825\n",
      "Epoch 5/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.7846 - loss: 0.4779 - val_accuracy: 0.8015 - val_loss: 0.4320\n",
      "Epoch 6/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.7994 - loss: 0.4334 - val_accuracy: 0.7977 - val_loss: 0.4064\n",
      "Epoch 7/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.8038 - loss: 0.4138 - val_accuracy: 0.7971 - val_loss: 0.3880\n",
      "Epoch 8/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.8158 - loss: 0.3913 - val_accuracy: 0.8168 - val_loss: 0.3794\n",
      "Epoch 9/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8231 - loss: 0.3716 - val_accuracy: 0.8047 - val_loss: 0.3767\n",
      "Epoch 10/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8219 - loss: 0.3695 - val_accuracy: 0.8206 - val_loss: 0.3511\n",
      "Epoch 11/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8379 - loss: 0.3321 - val_accuracy: 0.8181 - val_loss: 0.3367\n",
      "Epoch 12/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8328 - loss: 0.3375 - val_accuracy: 0.8327 - val_loss: 0.3377\n",
      "Epoch 13/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8350 - loss: 0.3343 - val_accuracy: 0.8327 - val_loss: 0.3230\n",
      "Epoch 14/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8333 - loss: 0.3211 - val_accuracy: 0.8244 - val_loss: 0.3194\n",
      "Epoch 15/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8323 - loss: 0.3160 - val_accuracy: 0.8244 - val_loss: 0.3316\n",
      "Epoch 16/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8363 - loss: 0.3162 - val_accuracy: 0.8257 - val_loss: 0.3067\n",
      "Epoch 17/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8278 - loss: 0.3089 - val_accuracy: 0.8346 - val_loss: 0.3105\n",
      "Epoch 18/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.8340 - loss: 0.2958 - val_accuracy: 0.8232 - val_loss: 0.3102\n",
      "Epoch 19/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8336 - loss: 0.3038 - val_accuracy: 0.8168 - val_loss: 0.3141\n",
      "Epoch 20/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.8322 - loss: 0.3022 - val_accuracy: 0.8314 - val_loss: 0.3054\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       477\n",
      "           1       0.92      0.95      0.93       208\n",
      "           3       0.98      0.94      0.96       198\n",
      "           4       0.86      0.96      0.91       226\n",
      "           5       0.49      0.93      0.64       214\n",
      "           6       0.58      0.04      0.08       250\n",
      "\n",
      "    accuracy                           0.82      1573\n",
      "   macro avg       0.80      0.80      0.75      1573\n",
      "weighted avg       0.82      0.82      0.77      1573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid)\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict classes\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # if one-hot encoded\n",
    "y_true = Y_test.argmax(axis=1)          # or just use Y_test if sparse\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750048c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, GRU\n",
    "# For time series data:\n",
    "# X_train.shape == (samples, timesteps, features)\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_valid = X_valid.reshape((X_valid.shape[0], X_valid.shape[1], 1))\n",
    "timesteps = X_train.shape[1]\n",
    "features = X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71d85e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.8342 - loss: 0.2951 - val_accuracy: 0.8270 - val_loss: 0.2941\n",
      "Epoch 2/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8259 - loss: 0.3012 - val_accuracy: 0.8365 - val_loss: 0.2861\n",
      "Epoch 3/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.8383 - loss: 0.2841 - val_accuracy: 0.8308 - val_loss: 0.2870\n",
      "Epoch 4/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8402 - loss: 0.2790 - val_accuracy: 0.8410 - val_loss: 0.3161\n",
      "Epoch 5/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8391 - loss: 0.2905 - val_accuracy: 0.8391 - val_loss: 0.2959\n",
      "Epoch 6/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8396 - loss: 0.2804 - val_accuracy: 0.8346 - val_loss: 0.2785\n",
      "Epoch 7/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8419 - loss: 0.2810 - val_accuracy: 0.8352 - val_loss: 0.2787\n",
      "Epoch 8/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.8415 - loss: 0.2720 - val_accuracy: 0.8372 - val_loss: 0.2754\n",
      "Epoch 9/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8343 - loss: 0.2714 - val_accuracy: 0.8511 - val_loss: 0.2836\n",
      "Epoch 10/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8485 - loss: 0.2645 - val_accuracy: 0.8441 - val_loss: 0.2636\n",
      "Epoch 11/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8402 - loss: 0.2727 - val_accuracy: 0.8511 - val_loss: 0.2626\n",
      "Epoch 12/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8484 - loss: 0.2661 - val_accuracy: 0.8333 - val_loss: 0.2695\n",
      "Epoch 13/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8470 - loss: 0.2657 - val_accuracy: 0.8448 - val_loss: 0.2768\n",
      "Epoch 14/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.8371 - loss: 0.2687 - val_accuracy: 0.8206 - val_loss: 0.2751\n",
      "Epoch 15/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8555 - loss: 0.2538 - val_accuracy: 0.8499 - val_loss: 0.2640\n",
      "Epoch 16/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8357 - loss: 0.2724 - val_accuracy: 0.8403 - val_loss: 0.2592\n",
      "Epoch 17/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8574 - loss: 0.2535 - val_accuracy: 0.8410 - val_loss: 0.3010\n",
      "Epoch 18/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.8392 - loss: 0.2691 - val_accuracy: 0.8289 - val_loss: 0.2744\n",
      "Epoch 19/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.8541 - loss: 0.2482 - val_accuracy: 0.8136 - val_loss: 0.2803\n",
      "Epoch 20/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.8446 - loss: 0.2565 - val_accuracy: 0.8473 - val_loss: 0.2786\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       477\n",
      "           1       0.98      0.94      0.96       208\n",
      "           3       0.99      0.94      0.97       198\n",
      "           4       0.96      0.97      0.96       226\n",
      "           5       0.48      0.98      0.64       214\n",
      "           6       0.63      0.09      0.15       250\n",
      "\n",
      "    accuracy                           0.83      1573\n",
      "   macro avg       0.84      0.82      0.78      1573\n",
      "weighted avg       0.86      0.83      0.80      1573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid)\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict classes\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # if one-hot encoded\n",
    "y_true = Y_test.argmax(axis=1)          # or just use Y_test if sparse\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1561430d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OussamaTab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(timesteps, features)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12464538",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d14cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.2984 - loss: 1.8142 - val_accuracy: 0.5636 - val_loss: 1.0581\n",
      "Epoch 2/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 27ms/step - accuracy: 0.5772 - loss: 0.9539 - val_accuracy: 0.6915 - val_loss: 0.6709\n",
      "Epoch 3/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.6862 - loss: 0.6832 - val_accuracy: 0.7424 - val_loss: 0.5403\n",
      "Epoch 4/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.7349 - loss: 0.5678 - val_accuracy: 0.7875 - val_loss: 0.4600\n",
      "Epoch 5/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.7549 - loss: 0.5098 - val_accuracy: 0.7608 - val_loss: 0.4422\n",
      "Epoch 6/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 83ms/step - accuracy: 0.7720 - loss: 0.4694 - val_accuracy: 0.7786 - val_loss: 0.4172\n",
      "Epoch 7/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 105ms/step - accuracy: 0.7757 - loss: 0.4278 - val_accuracy: 0.8066 - val_loss: 0.3726\n",
      "Epoch 8/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 89ms/step - accuracy: 0.7864 - loss: 0.4121 - val_accuracy: 0.7653 - val_loss: 0.4181\n",
      "Epoch 9/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 105ms/step - accuracy: 0.7912 - loss: 0.4055 - val_accuracy: 0.8162 - val_loss: 0.3682\n",
      "Epoch 10/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 106ms/step - accuracy: 0.7832 - loss: 0.3962 - val_accuracy: 0.7850 - val_loss: 0.3743\n",
      "Epoch 11/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 67ms/step - accuracy: 0.8043 - loss: 0.3794 - val_accuracy: 0.8219 - val_loss: 0.3416\n",
      "Epoch 12/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.8052 - loss: 0.3496 - val_accuracy: 0.8206 - val_loss: 0.3272\n",
      "Epoch 13/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - accuracy: 0.7963 - loss: 0.3580 - val_accuracy: 0.8270 - val_loss: 0.3281\n",
      "Epoch 14/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 76ms/step - accuracy: 0.8118 - loss: 0.3583 - val_accuracy: 0.8168 - val_loss: 0.3347\n",
      "Epoch 15/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 29ms/step - accuracy: 0.8242 - loss: 0.3333 - val_accuracy: 0.8232 - val_loss: 0.3136\n",
      "Epoch 16/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.8123 - loss: 0.3395 - val_accuracy: 0.8187 - val_loss: 0.3215\n",
      "Epoch 17/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8107 - loss: 0.3389 - val_accuracy: 0.8340 - val_loss: 0.3088\n",
      "Epoch 18/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8332 - loss: 0.3111 - val_accuracy: 0.8111 - val_loss: 0.3157\n",
      "Epoch 19/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8202 - loss: 0.3254 - val_accuracy: 0.8365 - val_loss: 0.2992\n",
      "Epoch 20/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8288 - loss: 0.3141 - val_accuracy: 0.8321 - val_loss: 0.2939\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       477\n",
      "           1       0.84      0.99      0.91       208\n",
      "           3       0.99      0.94      0.97       198\n",
      "           4       0.94      0.88      0.91       226\n",
      "           5       0.47      0.27      0.34       214\n",
      "           6       0.52      0.68      0.59       250\n",
      "\n",
      "    accuracy                           0.82      1573\n",
      "   macro avg       0.79      0.79      0.79      1573\n",
      "weighted avg       0.82      0.82      0.82      1573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid)\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict classes\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # if one-hot encoded\n",
    "y_true = Y_test.argmax(axis=1)          # or just use Y_test if sparse\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fef963c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OussamaTab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    GRU(64, input_shape=(timesteps, features)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2814dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356150f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.2719 - loss: 1.8000 - val_accuracy: 0.5324 - val_loss: 1.1895\n",
      "Epoch 2/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.5936 - loss: 1.0264 - val_accuracy: 0.7271 - val_loss: 0.6117\n",
      "Epoch 3/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.7173 - loss: 0.6426 - val_accuracy: 0.7977 - val_loss: 0.4562\n",
      "Epoch 4/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.7670 - loss: 0.5067 - val_accuracy: 0.8041 - val_loss: 0.3985\n",
      "Epoch 5/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - accuracy: 0.7762 - loss: 0.4345 - val_accuracy: 0.7983 - val_loss: 0.3733\n",
      "Epoch 6/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.8046 - loss: 0.3998 - val_accuracy: 0.8206 - val_loss: 0.3338\n",
      "Epoch 7/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.8051 - loss: 0.3815 - val_accuracy: 0.8219 - val_loss: 0.3343\n",
      "Epoch 8/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.8079 - loss: 0.3510 - val_accuracy: 0.8162 - val_loss: 0.3210\n",
      "Epoch 9/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - accuracy: 0.8055 - loss: 0.3481 - val_accuracy: 0.8168 - val_loss: 0.3122\n",
      "Epoch 10/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.8193 - loss: 0.3256 - val_accuracy: 0.8162 - val_loss: 0.3201\n",
      "Epoch 11/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.8177 - loss: 0.3228 - val_accuracy: 0.8308 - val_loss: 0.2978\n",
      "Epoch 12/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.8284 - loss: 0.3063 - val_accuracy: 0.8441 - val_loss: 0.2859\n",
      "Epoch 13/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.8373 - loss: 0.2935 - val_accuracy: 0.8238 - val_loss: 0.3029\n",
      "Epoch 14/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.8451 - loss: 0.2750 - val_accuracy: 0.8308 - val_loss: 0.2980\n",
      "Epoch 15/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.8377 - loss: 0.2851 - val_accuracy: 0.8066 - val_loss: 0.3540\n",
      "Epoch 16/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.8351 - loss: 0.2918 - val_accuracy: 0.8282 - val_loss: 0.2951\n",
      "Epoch 17/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.8412 - loss: 0.2706 - val_accuracy: 0.8244 - val_loss: 0.3170\n",
      "Epoch 18/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - accuracy: 0.8447 - loss: 0.2597 - val_accuracy: 0.8104 - val_loss: 0.3721\n",
      "Epoch 19/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 53ms/step - accuracy: 0.8540 - loss: 0.2532 - val_accuracy: 0.8155 - val_loss: 0.3578\n",
      "Epoch 20/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.8463 - loss: 0.2577 - val_accuracy: 0.7971 - val_loss: 0.4204\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       477\n",
      "           1       0.94      0.96      0.95       208\n",
      "           3       0.99      0.98      0.98       198\n",
      "           4       0.93      0.93      0.93       226\n",
      "           5       0.52      0.70      0.59       214\n",
      "           6       0.61      0.42      0.50       250\n",
      "\n",
      "    accuracy                           0.85      1573\n",
      "   macro avg       0.83      0.83      0.82      1573\n",
      "weighted avg       0.85      0.85      0.84      1573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid)\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict classes\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # if one-hot encoded\n",
    "y_true = Y_test.argmax(axis=1)          # or just use Y_test if sparse\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d5addd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OussamaTab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv1D(128, 3, activation='relu', padding='same', input_shape=(6, 1)),  # Short sequences, so kernel=3\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(64, 3, activation='relu', padding='same'),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587cae2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4121 - loss: 1.5281 - val_accuracy: 0.7023 - val_loss: 0.7898\n",
      "Epoch 2/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 44ms/step - accuracy: 0.7031 - loss: 0.7454 - val_accuracy: 0.7328 - val_loss: 0.5986\n",
      "Epoch 3/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 48ms/step - accuracy: 0.7522 - loss: 0.5576 - val_accuracy: 0.7697 - val_loss: 0.4901\n",
      "Epoch 4/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 82ms/step - accuracy: 0.7902 - loss: 0.4646 - val_accuracy: 0.8047 - val_loss: 0.4254\n",
      "Epoch 5/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 84ms/step - accuracy: 0.8079 - loss: 0.4339 - val_accuracy: 0.8174 - val_loss: 0.3965\n",
      "Epoch 6/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 69ms/step - accuracy: 0.8084 - loss: 0.4104 - val_accuracy: 0.8174 - val_loss: 0.3970\n",
      "Epoch 7/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 43ms/step - accuracy: 0.8109 - loss: 0.3904 - val_accuracy: 0.8034 - val_loss: 0.3754\n",
      "Epoch 8/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.8264 - loss: 0.3704 - val_accuracy: 0.8104 - val_loss: 0.3662\n",
      "Epoch 9/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.8252 - loss: 0.3629 - val_accuracy: 0.8130 - val_loss: 0.3515\n",
      "Epoch 10/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.8274 - loss: 0.3523 - val_accuracy: 0.8111 - val_loss: 0.3579\n",
      "Epoch 11/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 27ms/step - accuracy: 0.8282 - loss: 0.3460 - val_accuracy: 0.8206 - val_loss: 0.3694\n",
      "Epoch 12/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.8229 - loss: 0.3591 - val_accuracy: 0.8244 - val_loss: 0.3444\n",
      "Epoch 13/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 31ms/step - accuracy: 0.8296 - loss: 0.3336 - val_accuracy: 0.8047 - val_loss: 0.3348\n",
      "Epoch 14/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - accuracy: 0.8333 - loss: 0.3319 - val_accuracy: 0.8238 - val_loss: 0.3267\n",
      "Epoch 15/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 31ms/step - accuracy: 0.8402 - loss: 0.3107 - val_accuracy: 0.8219 - val_loss: 0.3254\n",
      "Epoch 16/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.8413 - loss: 0.3061 - val_accuracy: 0.8327 - val_loss: 0.3484\n",
      "Epoch 17/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.8477 - loss: 0.2972 - val_accuracy: 0.8238 - val_loss: 0.3076\n",
      "Epoch 18/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.8243 - loss: 0.3151 - val_accuracy: 0.8232 - val_loss: 0.3095\n",
      "Epoch 19/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.8438 - loss: 0.2893 - val_accuracy: 0.8251 - val_loss: 0.3048\n",
      "Epoch 20/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 30ms/step - accuracy: 0.8344 - loss: 0.3047 - val_accuracy: 0.8448 - val_loss: 0.3101\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       477\n",
      "           1       0.89      0.98      0.93       208\n",
      "           3       0.97      0.98      0.97       198\n",
      "           4       0.91      0.95      0.93       226\n",
      "           5       0.86      0.06      0.11       214\n",
      "           6       0.53      0.87      0.66       250\n",
      "\n",
      "    accuracy                           0.84      1573\n",
      "   macro avg       0.86      0.81      0.76      1573\n",
      "weighted avg       0.87      0.84      0.80      1573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid)\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict classes\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # if one-hot encoded\n",
    "y_true = Y_test.argmax(axis=1)          # or just use Y_test if sparse\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
