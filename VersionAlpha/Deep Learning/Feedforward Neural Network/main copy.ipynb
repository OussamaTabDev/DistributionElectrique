{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bffdb6a",
   "metadata": {},
   "source": [
    "# Deep learing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86043c9",
   "metadata": {},
   "source": [
    "## FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "779a60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout , GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3f247e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d3867222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>B</th>\n",
       "      <th>A</th>\n",
       "      <th>Ia</th>\n",
       "      <th>Ib</th>\n",
       "      <th>Ic</th>\n",
       "      <th>Va</th>\n",
       "      <th>Vb</th>\n",
       "      <th>Vc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-151.291812</td>\n",
       "      <td>-9.677452</td>\n",
       "      <td>85.800162</td>\n",
       "      <td>0.400750</td>\n",
       "      <td>-0.132935</td>\n",
       "      <td>-0.267815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-336.186183</td>\n",
       "      <td>-76.283262</td>\n",
       "      <td>18.328897</td>\n",
       "      <td>0.312732</td>\n",
       "      <td>-0.123633</td>\n",
       "      <td>-0.189099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-502.891583</td>\n",
       "      <td>-174.648023</td>\n",
       "      <td>-80.924663</td>\n",
       "      <td>0.265728</td>\n",
       "      <td>-0.114301</td>\n",
       "      <td>-0.151428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-593.941905</td>\n",
       "      <td>-217.703359</td>\n",
       "      <td>-124.891924</td>\n",
       "      <td>0.235511</td>\n",
       "      <td>-0.104940</td>\n",
       "      <td>-0.130570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-643.663617</td>\n",
       "      <td>-224.159427</td>\n",
       "      <td>-132.282815</td>\n",
       "      <td>0.209537</td>\n",
       "      <td>-0.095554</td>\n",
       "      <td>-0.113983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7856</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-66.237921</td>\n",
       "      <td>38.457041</td>\n",
       "      <td>24.912239</td>\n",
       "      <td>0.094421</td>\n",
       "      <td>-0.552019</td>\n",
       "      <td>0.457598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7857</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.849493</td>\n",
       "      <td>37.465454</td>\n",
       "      <td>25.515675</td>\n",
       "      <td>0.103778</td>\n",
       "      <td>-0.555186</td>\n",
       "      <td>0.451407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7858</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.446698</td>\n",
       "      <td>36.472055</td>\n",
       "      <td>26.106554</td>\n",
       "      <td>0.113107</td>\n",
       "      <td>-0.558211</td>\n",
       "      <td>0.445104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7859</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.029633</td>\n",
       "      <td>35.477088</td>\n",
       "      <td>26.684731</td>\n",
       "      <td>0.122404</td>\n",
       "      <td>-0.561094</td>\n",
       "      <td>0.438690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7860</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-64.598401</td>\n",
       "      <td>34.480799</td>\n",
       "      <td>27.250065</td>\n",
       "      <td>0.131669</td>\n",
       "      <td>-0.563835</td>\n",
       "      <td>0.432166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7861 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      G  C  B  A          Ia          Ib          Ic        Va        Vb  \\\n",
       "0     1  0  0  1 -151.291812   -9.677452   85.800162  0.400750 -0.132935   \n",
       "1     1  0  0  1 -336.186183  -76.283262   18.328897  0.312732 -0.123633   \n",
       "2     1  0  0  1 -502.891583 -174.648023  -80.924663  0.265728 -0.114301   \n",
       "3     1  0  0  1 -593.941905 -217.703359 -124.891924  0.235511 -0.104940   \n",
       "4     1  0  0  1 -643.663617 -224.159427 -132.282815  0.209537 -0.095554   \n",
       "...  .. .. .. ..         ...         ...         ...       ...       ...   \n",
       "7856  0  0  0  0  -66.237921   38.457041   24.912239  0.094421 -0.552019   \n",
       "7857  0  0  0  0  -65.849493   37.465454   25.515675  0.103778 -0.555186   \n",
       "7858  0  0  0  0  -65.446698   36.472055   26.106554  0.113107 -0.558211   \n",
       "7859  0  0  0  0  -65.029633   35.477088   26.684731  0.122404 -0.561094   \n",
       "7860  0  0  0  0  -64.598401   34.480799   27.250065  0.131669 -0.563835   \n",
       "\n",
       "            Vc  \n",
       "0    -0.267815  \n",
       "1    -0.189099  \n",
       "2    -0.151428  \n",
       "3    -0.130570  \n",
       "4    -0.113983  \n",
       "...        ...  \n",
       "7856  0.457598  \n",
       "7857  0.451407  \n",
       "7858  0.445104  \n",
       "7859  0.438690  \n",
       "7860  0.432166  \n",
       "\n",
       "[7861 rows x 10 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"G\" , \"C\" , \"B\" , \"A\" , \"Ia\" , \"Ib\" , \"Ic\" , \"Va\" , \"Vb\" , \"Vc\"] \n",
    "# Ia\" , \"Ib\" , \"Ic\" , \"Va\" , \"Vb\" , \"Vc is Input : features\n",
    "# G , C , B , A is OutPut : labels\n",
    "df = pd.read_csv(\"../../classData.csv\")\n",
    "cols[4:]\n",
    "# Step 1: Encode fault combinations into single-class labels\n",
    "fault_map = {\n",
    "    '0000': 0,  # No Fault\n",
    "    '1001': 1,  # LG\n",
    "    # '0011': 2,  # LL\n",
    "    '0110': 2 , # LL\n",
    "    '1011': 3,  # LLG\n",
    "    '0111': 4,  # LLL\n",
    "    '1111': 5   # LLLG\n",
    "}\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c5b5cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fault_type'] = df[['G', 'C', 'B', 'A']].astype(str).agg(''.join, axis=1)\n",
    "# Map to single class label\n",
    "df['fault_class'] = df['fault_type'].map(fault_map)\n",
    "# Drop rows with unknown fault combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a3a91397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep dropped rows (unknown fault types) in a separate DataFrame\n",
    "df_unknown = df[~df['fault_type'].isin(fault_map.keys())].copy()\n",
    "\n",
    "# Filter valid rows for training\n",
    "df = df[df['fault_type'].isin(fault_map.keys())].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4dafbd75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>B</th>\n",
       "      <th>A</th>\n",
       "      <th>Ia</th>\n",
       "      <th>Ib</th>\n",
       "      <th>Ic</th>\n",
       "      <th>Va</th>\n",
       "      <th>Vb</th>\n",
       "      <th>Vc</th>\n",
       "      <th>fault_type</th>\n",
       "      <th>fault_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-151.291812</td>\n",
       "      <td>-9.677452</td>\n",
       "      <td>85.800162</td>\n",
       "      <td>0.400750</td>\n",
       "      <td>-0.132935</td>\n",
       "      <td>-0.267815</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-336.186183</td>\n",
       "      <td>-76.283262</td>\n",
       "      <td>18.328897</td>\n",
       "      <td>0.312732</td>\n",
       "      <td>-0.123633</td>\n",
       "      <td>-0.189099</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-502.891583</td>\n",
       "      <td>-174.648023</td>\n",
       "      <td>-80.924663</td>\n",
       "      <td>0.265728</td>\n",
       "      <td>-0.114301</td>\n",
       "      <td>-0.151428</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-593.941905</td>\n",
       "      <td>-217.703359</td>\n",
       "      <td>-124.891924</td>\n",
       "      <td>0.235511</td>\n",
       "      <td>-0.104940</td>\n",
       "      <td>-0.130570</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-643.663617</td>\n",
       "      <td>-224.159427</td>\n",
       "      <td>-132.282815</td>\n",
       "      <td>0.209537</td>\n",
       "      <td>-0.095554</td>\n",
       "      <td>-0.113983</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7856</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-66.237921</td>\n",
       "      <td>38.457041</td>\n",
       "      <td>24.912239</td>\n",
       "      <td>0.094421</td>\n",
       "      <td>-0.552019</td>\n",
       "      <td>0.457598</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7857</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.849493</td>\n",
       "      <td>37.465454</td>\n",
       "      <td>25.515675</td>\n",
       "      <td>0.103778</td>\n",
       "      <td>-0.555186</td>\n",
       "      <td>0.451407</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7858</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.446698</td>\n",
       "      <td>36.472055</td>\n",
       "      <td>26.106554</td>\n",
       "      <td>0.113107</td>\n",
       "      <td>-0.558211</td>\n",
       "      <td>0.445104</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7859</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.029633</td>\n",
       "      <td>35.477088</td>\n",
       "      <td>26.684731</td>\n",
       "      <td>0.122404</td>\n",
       "      <td>-0.561094</td>\n",
       "      <td>0.438690</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7860</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-64.598401</td>\n",
       "      <td>34.480799</td>\n",
       "      <td>27.250065</td>\n",
       "      <td>0.131669</td>\n",
       "      <td>-0.563835</td>\n",
       "      <td>0.432166</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7861 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      G  C  B  A          Ia          Ib          Ic        Va        Vb  \\\n",
       "0     1  0  0  1 -151.291812   -9.677452   85.800162  0.400750 -0.132935   \n",
       "1     1  0  0  1 -336.186183  -76.283262   18.328897  0.312732 -0.123633   \n",
       "2     1  0  0  1 -502.891583 -174.648023  -80.924663  0.265728 -0.114301   \n",
       "3     1  0  0  1 -593.941905 -217.703359 -124.891924  0.235511 -0.104940   \n",
       "4     1  0  0  1 -643.663617 -224.159427 -132.282815  0.209537 -0.095554   \n",
       "...  .. .. .. ..         ...         ...         ...       ...       ...   \n",
       "7856  0  0  0  0  -66.237921   38.457041   24.912239  0.094421 -0.552019   \n",
       "7857  0  0  0  0  -65.849493   37.465454   25.515675  0.103778 -0.555186   \n",
       "7858  0  0  0  0  -65.446698   36.472055   26.106554  0.113107 -0.558211   \n",
       "7859  0  0  0  0  -65.029633   35.477088   26.684731  0.122404 -0.561094   \n",
       "7860  0  0  0  0  -64.598401   34.480799   27.250065  0.131669 -0.563835   \n",
       "\n",
       "            Vc fault_type  fault_class  \n",
       "0    -0.267815       1001            1  \n",
       "1    -0.189099       1001            1  \n",
       "2    -0.151428       1001            1  \n",
       "3    -0.130570       1001            1  \n",
       "4    -0.113983       1001            1  \n",
       "...        ...        ...          ...  \n",
       "7856  0.457598       0000            0  \n",
       "7857  0.451407       0000            0  \n",
       "7858  0.445104       0000            0  \n",
       "7859  0.438690       0000            0  \n",
       "7860  0.432166       0000            0  \n",
       "\n",
       "[7861 rows x 12 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3f85dc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>B</th>\n",
       "      <th>A</th>\n",
       "      <th>Ia</th>\n",
       "      <th>Ib</th>\n",
       "      <th>Ic</th>\n",
       "      <th>Va</th>\n",
       "      <th>Vb</th>\n",
       "      <th>Vc</th>\n",
       "      <th>fault_type</th>\n",
       "      <th>fault_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [G, C, B, A, Ia, Ib, Ic, Va, Vb, Vc, fault_type, fault_class]\n",
       "Index: []"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fa171c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OussamaTab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>B</th>\n",
       "      <th>A</th>\n",
       "      <th>Ia</th>\n",
       "      <th>Ib</th>\n",
       "      <th>Ic</th>\n",
       "      <th>Va</th>\n",
       "      <th>Vb</th>\n",
       "      <th>Vc</th>\n",
       "      <th>fault_type</th>\n",
       "      <th>fault_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5045</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>669.880516</td>\n",
       "      <td>163.852298</td>\n",
       "      <td>-833.735078</td>\n",
       "      <td>0.039339</td>\n",
       "      <td>-0.033173</td>\n",
       "      <td>-0.006166</td>\n",
       "      <td>1111</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>312.017622</td>\n",
       "      <td>62.640263</td>\n",
       "      <td>-46.344141</td>\n",
       "      <td>-0.374911</td>\n",
       "      <td>0.461247</td>\n",
       "      <td>-0.086336</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-131.444684</td>\n",
       "      <td>39.223623</td>\n",
       "      <td>-49.600749</td>\n",
       "      <td>-0.365095</td>\n",
       "      <td>0.593485</td>\n",
       "      <td>-0.228390</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3437</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>447.988797</td>\n",
       "      <td>-893.204103</td>\n",
       "      <td>447.370989</td>\n",
       "      <td>-0.022120</td>\n",
       "      <td>-0.020094</td>\n",
       "      <td>0.042214</td>\n",
       "      <td>0111</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7491</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-45.215120</td>\n",
       "      <td>5.449328</td>\n",
       "      <td>36.794741</td>\n",
       "      <td>0.389949</td>\n",
       "      <td>-0.575036</td>\n",
       "      <td>0.185087</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-525.424590</td>\n",
       "      <td>884.206453</td>\n",
       "      <td>36.739070</td>\n",
       "      <td>0.018069</td>\n",
       "      <td>-0.010008</td>\n",
       "      <td>-0.008061</td>\n",
       "      <td>1011</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22.950219</td>\n",
       "      <td>-790.547163</td>\n",
       "      <td>770.168143</td>\n",
       "      <td>-0.180863</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.180647</td>\n",
       "      <td>0110</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4469</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-381.393609</td>\n",
       "      <td>-516.572513</td>\n",
       "      <td>898.072615</td>\n",
       "      <td>-0.040965</td>\n",
       "      <td>0.019332</td>\n",
       "      <td>0.021633</td>\n",
       "      <td>1111</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5146</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-586.627772</td>\n",
       "      <td>865.289870</td>\n",
       "      <td>-278.664362</td>\n",
       "      <td>0.014980</td>\n",
       "      <td>0.026841</td>\n",
       "      <td>-0.041821</td>\n",
       "      <td>1111</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6271</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-60.216074</td>\n",
       "      <td>28.501162</td>\n",
       "      <td>28.374274</td>\n",
       "      <td>0.228731</td>\n",
       "      <td>-0.581430</td>\n",
       "      <td>0.352699</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1573 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      G  C  B  A          Ia          Ib          Ic        Va        Vb  \\\n",
       "5045  1  1  1  1  669.880516  163.852298 -833.735078  0.039339 -0.033173   \n",
       "159   1  0  0  1  312.017622   62.640263  -46.344141 -0.374911  0.461247   \n",
       "927   1  0  0  1 -131.444684   39.223623  -49.600749 -0.365095  0.593485   \n",
       "3437  0  1  1  1  447.988797 -893.204103  447.370989 -0.022120 -0.020094   \n",
       "7491  0  0  0  0  -45.215120    5.449328   36.794741  0.389949 -0.575036   \n",
       "...  .. .. .. ..         ...         ...         ...       ...       ...   \n",
       "1507  1  0  1  1 -525.424590  884.206453   36.739070  0.018069 -0.010008   \n",
       "2376  0  1  1  0   22.950219 -790.547163  770.168143 -0.180863  0.000216   \n",
       "4469  1  1  1  1 -381.393609 -516.572513  898.072615 -0.040965  0.019332   \n",
       "5146  1  1  1  1 -586.627772  865.289870 -278.664362  0.014980  0.026841   \n",
       "6271  0  0  0  0  -60.216074   28.501162   28.374274  0.228731 -0.581430   \n",
       "\n",
       "            Vc fault_type  fault_class  \n",
       "5045 -0.006166       1111            5  \n",
       "159  -0.086336       1001            1  \n",
       "927  -0.228390       1001            1  \n",
       "3437  0.042214       0111            4  \n",
       "7491  0.185087       0000            0  \n",
       "...        ...        ...          ...  \n",
       "1507 -0.008061       1011            3  \n",
       "2376  0.180647       0110            2  \n",
       "4469  0.021633       1111            5  \n",
       "5146 -0.041821       1111            5  \n",
       "6271  0.352699       0000            0  \n",
       "\n",
       "[1573 rows x 12 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train , valid , test = np.split(df.sample(frac = 1) , [int(0.6 * len(df)) , int(0.8 * len(df))])\n",
    "# train  60% |||||||||||||\n",
    "# valid  20%  1234567890\n",
    "# test 20%\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6b33ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataFrame):\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    x = dataFrame[['Ia', 'Ib', 'Ic', 'Va', 'Vb', 'Vc']].values\n",
    "    # Get the fault class values from the dataFrame parameter (not the global df)\n",
    "    y = dataFrame['fault_class'].values\n",
    "    \n",
    "    # Scale the input features\n",
    "    scaler = StandardScaler() \n",
    "    x = scaler.fit_transform(x)\n",
    "\n",
    "    # One-hot encode the target for categorical classification\n",
    "    y_cat = to_categorical(y, num_classes=7)\n",
    "    \n",
    "    return x, y_cat  # Return the one-hot encoded y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d92a9709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.17647826,  0.19275744, -0.02106013,  0.26858192, -1.73516021,\n",
       "         1.51364549],\n",
       "       [-0.09717342,  0.31883429, -0.2689104 , -2.11914115,  0.81968752,\n",
       "         1.16246214],\n",
       "       [ 0.60140597,  1.40737265, -2.40523249,  0.15522997, -0.04336931,\n",
       "        -0.10212948],\n",
       "       ...,\n",
       "       [ 0.08632018, -1.06671014,  1.13766292,  0.88975019,  0.1163328 ,\n",
       "        -0.95695057],\n",
       "       [-1.64980353,  1.86272567, -0.03615719,  0.01484714,  0.45680032,\n",
       "        -0.47911038],\n",
       "       [ 0.61390382, -1.89696479,  1.44465951, -0.08871894, -0.033779  ,\n",
       "         0.11800271]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-run these steps with the fixed function\n",
    "X_train, Y_train = scale_dataset(train)\n",
    "X_valid, Y_valid = scale_dataset(valid)\n",
    "X_test, Y_test = scale_dataset(test)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9443924e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.17647826,  0.19275744, -0.02106013,  0.26858192, -1.73516021,\n",
       "         1.51364549],\n",
       "       [-0.09717342,  0.31883429, -0.2689104 , -2.11914115,  0.81968752,\n",
       "         1.16246214],\n",
       "       [ 0.60140597,  1.40737265, -2.40523249,  0.15522997, -0.04336931,\n",
       "        -0.10212948],\n",
       "       ...,\n",
       "       [ 0.08632018, -1.06671014,  1.13766292,  0.88975019,  0.1163328 ,\n",
       "        -0.95695057],\n",
       "       [-1.64980353,  1.86272567, -0.03615719,  0.01484714,  0.45680032,\n",
       "        -0.47911038],\n",
       "       [ 0.61390382, -1.89696479,  1.44465951, -0.08871894, -0.033779  ,\n",
       "         0.11800271]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "46110a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0d1ef3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OussamaTab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Build the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(6,)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(7, activation='softmax')  # 7 classes for multiclass classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a00fcfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.3285 - loss: 1.7400 - val_accuracy: 0.6260 - val_loss: 0.9814\n",
      "Epoch 2/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6532 - loss: 0.9055 - val_accuracy: 0.7678 - val_loss: 0.5817\n",
      "Epoch 3/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7524 - loss: 0.5959 - val_accuracy: 0.7990 - val_loss: 0.4664\n",
      "Epoch 4/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7867 - loss: 0.4914 - val_accuracy: 0.8034 - val_loss: 0.4203\n",
      "Epoch 5/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8097 - loss: 0.4276 - val_accuracy: 0.8244 - val_loss: 0.3856\n",
      "Epoch 6/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8076 - loss: 0.4094 - val_accuracy: 0.8219 - val_loss: 0.3666\n",
      "Epoch 7/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8251 - loss: 0.3888 - val_accuracy: 0.8244 - val_loss: 0.3500\n",
      "Epoch 8/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8168 - loss: 0.3686 - val_accuracy: 0.8181 - val_loss: 0.3397\n",
      "Epoch 9/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8280 - loss: 0.3505 - val_accuracy: 0.8187 - val_loss: 0.3313\n",
      "Epoch 10/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8366 - loss: 0.3309 - val_accuracy: 0.8346 - val_loss: 0.3197\n",
      "Epoch 11/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8176 - loss: 0.3464 - val_accuracy: 0.8314 - val_loss: 0.3350\n",
      "Epoch 12/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8381 - loss: 0.3260 - val_accuracy: 0.8295 - val_loss: 0.3088\n",
      "Epoch 13/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8348 - loss: 0.3240 - val_accuracy: 0.8321 - val_loss: 0.3046\n",
      "Epoch 14/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8349 - loss: 0.3134 - val_accuracy: 0.8244 - val_loss: 0.3089\n",
      "Epoch 15/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8440 - loss: 0.3026 - val_accuracy: 0.8352 - val_loss: 0.3070\n",
      "Epoch 16/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8395 - loss: 0.3047 - val_accuracy: 0.8422 - val_loss: 0.2951\n",
      "Epoch 17/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8335 - loss: 0.3107 - val_accuracy: 0.8282 - val_loss: 0.2970\n",
      "Epoch 18/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8412 - loss: 0.3049 - val_accuracy: 0.8302 - val_loss: 0.3143\n",
      "Epoch 19/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8514 - loss: 0.2777 - val_accuracy: 0.8257 - val_loss: 0.2870\n",
      "Epoch 20/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8325 - loss: 0.2987 - val_accuracy: 0.8365 - val_loss: 0.3187\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Train the model\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, Y_train, \n",
    "    epochs=20, \n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid)  # Use separate validation data instead of validation_split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "be6318db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       477\n",
      "           1       0.91      0.96      0.93       225\n",
      "           2       0.97      0.98      0.97       208\n",
      "           3       0.99      0.95      0.97       223\n",
      "           4       1.00      0.08      0.15       222\n",
      "           5       0.50      0.93      0.65       218\n",
      "\n",
      "    accuracy                           0.84      1573\n",
      "   macro avg       0.89      0.81      0.78      1573\n",
      "weighted avg       0.90      0.84      0.81      1573\n",
      "\n",
      "[[477   0   0   0   0   0]\n",
      " [  9 215   0   1   0   0]\n",
      " [  2   0 203   0   0   3]\n",
      " [  0  12   0 211   0   0]\n",
      " [  1   0   3   0  18 200]\n",
      " [  0  10   4   2   0 202]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Make predictions on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert softmax probabilities to class predictions\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Convert one-hot encoded test labels back to class indices for comparison\n",
    "y_test_classes = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n",
    "print(confusion_matrix(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58405359",
   "metadata": {},
   "source": [
    "## better Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "96d6be4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OussamaTab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#  1. Create a deeper, more complex model\n",
    "model_improved = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(6,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(7, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6a3ff845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: np.float64(0.5679190751445087), 1: np.float64(1.1441048034934498), 2: np.float64(1.3034825870646767), 3: np.float64(1.1008403361344539), 4: np.float64(1.2110939907550078), 5: np.float64(1.1575846833578793)}\n"
     ]
    }
   ],
   "source": [
    "# 2. Use a different optimizer with a lower learning rate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# 3. Add class weights to handle class imbalance\n",
    "# Calculate class weights based on class frequencies\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Get class indices from the one-hot encoded training labels\n",
    "y_train_classes = np.argmax(Y_train, axis=1)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_classes),\n",
    "    y=y_train_classes\n",
    ")\n",
    "\n",
    "# Convert to dictionary for Keras\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(\"Class weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "27b48987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compile with additional metrics\n",
    "model_improved.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'AUC', 'Precision', 'Recall']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b3b0fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Use callbacks for early stopping and learning rate reduction\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "27d36c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - AUC: 0.6969 - Precision: 0.3993 - Recall: 0.0149 - accuracy: 0.2606 - loss: 1.7535 - val_AUC: 0.9675 - val_Precision: 0.8379 - val_Recall: 0.5687 - val_accuracy: 0.7316 - val_loss: 0.7181 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9474 - Precision: 0.7422 - Recall: 0.4924 - accuracy: 0.6539 - loss: 0.8420 - val_AUC: 0.9839 - val_Precision: 0.8923 - val_Recall: 0.6959 - val_accuracy: 0.8066 - val_loss: 0.4799 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9685 - Precision: 0.7667 - Recall: 0.6439 - accuracy: 0.7162 - loss: 0.6238 - val_AUC: 0.9841 - val_Precision: 0.8458 - val_Recall: 0.7640 - val_accuracy: 0.7971 - val_loss: 0.4182 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9762 - Precision: 0.8004 - Recall: 0.7166 - accuracy: 0.7627 - loss: 0.5524 - val_AUC: 0.9840 - val_Precision: 0.8149 - val_Recall: 0.7589 - val_accuracy: 0.7882 - val_loss: 0.4035 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9793 - Precision: 0.8154 - Recall: 0.7320 - accuracy: 0.7781 - loss: 0.5119 - val_AUC: 0.9881 - val_Precision: 0.8504 - val_Recall: 0.7812 - val_accuracy: 0.8232 - val_loss: 0.3518 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9826 - Precision: 0.8194 - Recall: 0.7466 - accuracy: 0.7888 - loss: 0.4661 - val_AUC: 0.9894 - val_Precision: 0.8553 - val_Recall: 0.8085 - val_accuracy: 0.8295 - val_loss: 0.3329 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9826 - Precision: 0.8146 - Recall: 0.7493 - accuracy: 0.7876 - loss: 0.4571 - val_AUC: 0.9887 - val_Precision: 0.8471 - val_Recall: 0.7824 - val_accuracy: 0.8225 - val_loss: 0.3266 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9839 - Precision: 0.8189 - Recall: 0.7643 - accuracy: 0.7961 - loss: 0.4372 - val_AUC: 0.9893 - val_Precision: 0.8550 - val_Recall: 0.7952 - val_accuracy: 0.8270 - val_loss: 0.3222 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9847 - Precision: 0.8252 - Recall: 0.7752 - accuracy: 0.8033 - loss: 0.4324 - val_AUC: 0.9896 - val_Precision: 0.8586 - val_Recall: 0.8111 - val_accuracy: 0.8384 - val_loss: 0.3128 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9870 - Precision: 0.8369 - Recall: 0.7925 - accuracy: 0.8194 - loss: 0.3951 - val_AUC: 0.9902 - val_Precision: 0.8538 - val_Recall: 0.8098 - val_accuracy: 0.8359 - val_loss: 0.2953 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - AUC: 0.9872 - Precision: 0.8375 - Recall: 0.7855 - accuracy: 0.8177 - loss: 0.3910 - val_AUC: 0.9908 - val_Precision: 0.8632 - val_Recall: 0.8232 - val_accuracy: 0.8441 - val_loss: 0.2896 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9870 - Precision: 0.8262 - Recall: 0.7872 - accuracy: 0.8086 - loss: 0.3946 - val_AUC: 0.9895 - val_Precision: 0.8426 - val_Recall: 0.8206 - val_accuracy: 0.8289 - val_loss: 0.2996 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9877 - Precision: 0.8332 - Recall: 0.7967 - accuracy: 0.8171 - loss: 0.3815 - val_AUC: 0.9898 - val_Precision: 0.8409 - val_Recall: 0.8206 - val_accuracy: 0.8295 - val_loss: 0.2975 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9886 - Precision: 0.8398 - Recall: 0.8045 - accuracy: 0.8221 - loss: 0.3648 - val_AUC: 0.9902 - val_Precision: 0.8440 - val_Recall: 0.8257 - val_accuracy: 0.8333 - val_loss: 0.2844 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9875 - Precision: 0.8279 - Recall: 0.7940 - accuracy: 0.8137 - loss: 0.3793 - val_AUC: 0.9887 - val_Precision: 0.8319 - val_Recall: 0.8060 - val_accuracy: 0.8193 - val_loss: 0.3048 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9875 - Precision: 0.8376 - Recall: 0.8032 - accuracy: 0.8229 - loss: 0.3730 - val_AUC: 0.9895 - val_Precision: 0.8373 - val_Recall: 0.8117 - val_accuracy: 0.8270 - val_loss: 0.2947 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9889 - Precision: 0.8413 - Recall: 0.8116 - accuracy: 0.8262 - loss: 0.3609 - val_AUC: 0.9900 - val_Precision: 0.8485 - val_Recall: 0.8302 - val_accuracy: 0.8422 - val_loss: 0.2907 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9889 - Precision: 0.8445 - Recall: 0.8164 - accuracy: 0.8290 - loss: 0.3629 - val_AUC: 0.9895 - val_Precision: 0.8333 - val_Recall: 0.8174 - val_accuracy: 0.8263 - val_loss: 0.2834 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9883 - Precision: 0.8328 - Recall: 0.8058 - accuracy: 0.8223 - loss: 0.3574 - val_AUC: 0.9901 - val_Precision: 0.8446 - val_Recall: 0.8092 - val_accuracy: 0.8276 - val_loss: 0.2781 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9882 - Precision: 0.8306 - Recall: 0.8005 - accuracy: 0.8192 - loss: 0.3607 - val_AUC: 0.9897 - val_Precision: 0.8403 - val_Recall: 0.8003 - val_accuracy: 0.8206 - val_loss: 0.2796 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9904 - Precision: 0.8499 - Recall: 0.8229 - accuracy: 0.8350 - loss: 0.3356 - val_AUC: 0.9903 - val_Precision: 0.8405 - val_Recall: 0.8314 - val_accuracy: 0.8352 - val_loss: 0.2724 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9899 - Precision: 0.8509 - Recall: 0.8202 - accuracy: 0.8364 - loss: 0.3347 - val_AUC: 0.9897 - val_Precision: 0.8405 - val_Recall: 0.8282 - val_accuracy: 0.8340 - val_loss: 0.2852 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9892 - Precision: 0.8417 - Recall: 0.8091 - accuracy: 0.8268 - loss: 0.3431 - val_AUC: 0.9906 - val_Precision: 0.8444 - val_Recall: 0.8352 - val_accuracy: 0.8384 - val_loss: 0.2678 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9897 - Precision: 0.8387 - Recall: 0.8107 - accuracy: 0.8301 - loss: 0.3353 - val_AUC: 0.9897 - val_Precision: 0.8409 - val_Recall: 0.8238 - val_accuracy: 0.8314 - val_loss: 0.2822 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9886 - Precision: 0.8321 - Recall: 0.8061 - accuracy: 0.8199 - loss: 0.3493 - val_AUC: 0.9896 - val_Precision: 0.8321 - val_Recall: 0.8168 - val_accuracy: 0.8263 - val_loss: 0.2833 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9899 - Precision: 0.8445 - Recall: 0.8208 - accuracy: 0.8355 - loss: 0.3379 - val_AUC: 0.9906 - val_Precision: 0.8437 - val_Recall: 0.8346 - val_accuracy: 0.8365 - val_loss: 0.2630 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - AUC: 0.9899 - Precision: 0.8441 - Recall: 0.8243 - accuracy: 0.8344 - loss: 0.3391 - val_AUC: 0.9907 - val_Precision: 0.8451 - val_Recall: 0.8327 - val_accuracy: 0.8391 - val_loss: 0.2597 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - AUC: 0.9891 - Precision: 0.8388 - Recall: 0.8122 - accuracy: 0.8252 - loss: 0.3435 - val_AUC: 0.9913 - val_Precision: 0.8494 - val_Recall: 0.8359 - val_accuracy: 0.8422 - val_loss: 0.2556 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9912 - Precision: 0.8501 - Recall: 0.8262 - accuracy: 0.8383 - loss: 0.3125 - val_AUC: 0.9894 - val_Precision: 0.8322 - val_Recall: 0.8263 - val_accuracy: 0.8302 - val_loss: 0.2823 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9889 - Precision: 0.8321 - Recall: 0.8102 - accuracy: 0.8219 - loss: 0.3498 - val_AUC: 0.9903 - val_Precision: 0.8409 - val_Recall: 0.8238 - val_accuracy: 0.8327 - val_loss: 0.2706 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9914 - Precision: 0.8571 - Recall: 0.8337 - accuracy: 0.8477 - loss: 0.3080 - val_AUC: 0.9904 - val_Precision: 0.8347 - val_Recall: 0.8193 - val_accuracy: 0.8276 - val_loss: 0.2673 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9900 - Precision: 0.8377 - Recall: 0.8168 - accuracy: 0.8287 - loss: 0.3306 - val_AUC: 0.9908 - val_Precision: 0.8441 - val_Recall: 0.8263 - val_accuracy: 0.8359 - val_loss: 0.2684 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m145/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9898 - Precision: 0.8367 - Recall: 0.8141 - accuracy: 0.8275 - loss: 0.3325\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9898 - Precision: 0.8368 - Recall: 0.8143 - accuracy: 0.8276 - loss: 0.3323 - val_AUC: 0.9901 - val_Precision: 0.8396 - val_Recall: 0.8193 - val_accuracy: 0.8270 - val_loss: 0.2653 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9904 - Precision: 0.8468 - Recall: 0.8222 - accuracy: 0.8344 - loss: 0.3151 - val_AUC: 0.9912 - val_Precision: 0.8503 - val_Recall: 0.8308 - val_accuracy: 0.8429 - val_loss: 0.2519 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - AUC: 0.9918 - Precision: 0.8548 - Recall: 0.8395 - accuracy: 0.8466 - loss: 0.2993 - val_AUC: 0.9904 - val_Precision: 0.8428 - val_Recall: 0.8289 - val_accuracy: 0.8378 - val_loss: 0.2640 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9911 - Precision: 0.8499 - Recall: 0.8282 - accuracy: 0.8404 - loss: 0.3053 - val_AUC: 0.9909 - val_Precision: 0.8379 - val_Recall: 0.8187 - val_accuracy: 0.8276 - val_loss: 0.2495 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9911 - Precision: 0.8492 - Recall: 0.8220 - accuracy: 0.8382 - loss: 0.3041 - val_AUC: 0.9897 - val_Precision: 0.8381 - val_Recall: 0.8232 - val_accuracy: 0.8282 - val_loss: 0.2728 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9916 - Precision: 0.8522 - Recall: 0.8331 - accuracy: 0.8434 - loss: 0.2932 - val_AUC: 0.9909 - val_Precision: 0.8395 - val_Recall: 0.8219 - val_accuracy: 0.8289 - val_loss: 0.2492 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9908 - Precision: 0.8393 - Recall: 0.8166 - accuracy: 0.8316 - loss: 0.3067 - val_AUC: 0.9910 - val_Precision: 0.8501 - val_Recall: 0.8333 - val_accuracy: 0.8378 - val_loss: 0.2533 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - AUC: 0.9923 - Precision: 0.8582 - Recall: 0.8343 - accuracy: 0.8474 - loss: 0.2883 - val_AUC: 0.9901 - val_Precision: 0.8358 - val_Recall: 0.8225 - val_accuracy: 0.8276 - val_loss: 0.2652 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - AUC: 0.9918 - Precision: 0.8542 - Recall: 0.8379 - accuracy: 0.8462 - loss: 0.2910 - val_AUC: 0.9902 - val_Precision: 0.8364 - val_Recall: 0.8130 - val_accuracy: 0.8232 - val_loss: 0.2614 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.9919 - Precision: 0.8550 - Recall: 0.8325 - accuracy: 0.8433 - loss: 0.2860 - val_AUC: 0.9902 - val_Precision: 0.8398 - val_Recall: 0.8206 - val_accuracy: 0.8251 - val_loss: 0.2576 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m134/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - AUC: 0.9912 - Precision: 0.8463 - Recall: 0.8279 - accuracy: 0.8394 - loss: 0.2978\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9913 - Precision: 0.8467 - Recall: 0.8286 - accuracy: 0.8398 - loss: 0.2975 - val_AUC: 0.9911 - val_Precision: 0.8420 - val_Recall: 0.8270 - val_accuracy: 0.8327 - val_loss: 0.2513 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9920 - Precision: 0.8552 - Recall: 0.8376 - accuracy: 0.8466 - loss: 0.2854 - val_AUC: 0.9902 - val_Precision: 0.8350 - val_Recall: 0.8174 - val_accuracy: 0.8225 - val_loss: 0.2582 - learning_rate: 2.5000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9920 - Precision: 0.8531 - Recall: 0.8352 - accuracy: 0.8437 - loss: 0.2840 - val_AUC: 0.9902 - val_Precision: 0.8398 - val_Recall: 0.8270 - val_accuracy: 0.8295 - val_loss: 0.2637 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - AUC: 0.9920 - Precision: 0.8546 - Recall: 0.8378 - accuracy: 0.8471 - loss: 0.2846 - val_AUC: 0.9910 - val_Precision: 0.8433 - val_Recall: 0.8282 - val_accuracy: 0.8327 - val_loss: 0.2496 - learning_rate: 2.5000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - AUC: 0.9921 - Precision: 0.8597 - Recall: 0.8425 - accuracy: 0.8528 - loss: 0.2925 - val_AUC: 0.9904 - val_Precision: 0.8397 - val_Recall: 0.8263 - val_accuracy: 0.8282 - val_loss: 0.2569 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - AUC: 0.9917 - Precision: 0.8540 - Recall: 0.8333 - accuracy: 0.8451 - loss: 0.2866\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - AUC: 0.9917 - Precision: 0.8540 - Recall: 0.8333 - accuracy: 0.8452 - loss: 0.2866 - val_AUC: 0.9905 - val_Precision: 0.8368 - val_Recall: 0.8219 - val_accuracy: 0.8276 - val_loss: 0.2548 - learning_rate: 2.5000e-04\n",
      "Epoch 48: early stopping\n",
      "Restoring model weights from the end of the best epoch: 38.\n"
     ]
    }
   ],
   "source": [
    "# 6. Train with more epochs and class weights\n",
    "history = model_improved.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=50,  # Increase epochs, early stopping will prevent overfitting\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid),\n",
    "    class_weight=class_weight_dict,  # Apply class weights\n",
    "    callbacks=[early_stopping, lr_reduction],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d6b992b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Improved Model Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       477\n",
      "           1       0.93      1.00      0.96       225\n",
      "           2       1.00      0.97      0.98       208\n",
      "           3       1.00      0.92      0.96       223\n",
      "           4       0.53      0.40      0.45       222\n",
      "           5       0.49      0.62      0.55       218\n",
      "\n",
      "    accuracy                           0.85      1573\n",
      "   macro avg       0.82      0.82      0.82      1573\n",
      "weighted avg       0.85      0.85      0.84      1573\n",
      "\n",
      "Confusion Matrix:\n",
      "[[477   0   0   0   0   0]\n",
      " [  0 224   0   1   0   0]\n",
      " [  4   0 201   0   3   0]\n",
      " [  0   9   0 205   0   9]\n",
      " [  1   0   0   0  88 133]\n",
      " [  0   8   0   0  74 136]]\n"
     ]
    }
   ],
   "source": [
    "# 7. Evaluate the improved model\n",
    "y_pred = model_improved.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(Y_test, axis=1)\n",
    "\n",
    "print(\"\\nImproved Model Evaluation:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4674559d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8. Visualize training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "909e7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SMOTE: Synthetic Minority Over-sampling Technique\n",
    "# This will help with the class imbalance problem, particularly for class 6\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def apply_smote_balancing():\n",
    "    # Get the class indices from one-hot encoded labels\n",
    "    y_train_classes = np.argmax(Y_train, axis=1)\n",
    "    \n",
    "    # Apply SMOTE to generate synthetic samples for minority classes\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train_classes)\n",
    "    \n",
    "    # Convert back to one-hot encoding\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    y_resampled_cat = to_categorical(y_resampled, num_classes=7)\n",
    "    \n",
    "    print(f\"Original shape: {X_train.shape}, Resampled shape: {X_resampled.shape}\")\n",
    "    print(\"Class distribution before SMOTE:\", np.bincount(y_train_classes))\n",
    "    print(\"Class distribution after SMOTE:\", np.bincount(y_resampled))\n",
    "    \n",
    "    return X_resampled, y_resampled_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3e0d860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Focal Loss - To focus more on hard-to-classify examples like class 6\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        \n",
    "        epsilon = K.epsilon()\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "        \n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - \\\n",
    "               K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0bab0e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Feature importance analysis and selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "def analyze_feature_importance(X, y, feature_names=None):\n",
    "    # Convert one-hot encoded labels to class indices if needed\n",
    "    if len(y.shape) > 1 and y.shape[1] > 1:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    \n",
    "    # Select k best features\n",
    "    selector = SelectKBest(f_classif, k='all')\n",
    "    selector.fit(X, y)\n",
    "    \n",
    "    # Get scores\n",
    "    scores = selector.scores_\n",
    "    \n",
    "    # If feature names are provided, display them with scores\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"Feature {i}\" for i in range(X.shape[1])]\n",
    "    \n",
    "    # Create dataframe of features and their scores\n",
    "    import pandas as pd\n",
    "    feature_scores = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Score': scores\n",
    "    })\n",
    "    \n",
    "    # Sort by score\n",
    "    feature_scores = feature_scores.sort_values('Score', ascending=False)\n",
    "    print(feature_scores)\n",
    "    \n",
    "    return feature_scores, selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e1ce9a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Advanced model with attention mechanism\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, InputSpec, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\")\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        et = K.squeeze(K.tanh(K.dot(x, self.W) + self.b), axis=-1)\n",
    "        at = K.softmax(et)\n",
    "        at = K.expand_dims(at, axis=-1)\n",
    "        output = x * at\n",
    "        return K.sum(output, axis=1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def create_attention_model(input_shape):\n",
    "    inputs = Input(shape=(input_shape,))\n",
    "    \n",
    "    # First dense block\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Reshape for attention\n",
    "    reshaped = tf.keras.layers.Reshape((4, 32))(x)\n",
    "    \n",
    "    # Apply attention\n",
    "    attention_output = AttentionLayer()(reshaped)\n",
    "    \n",
    "    # Additional dense layers\n",
    "    x = Dense(64, activation='relu')(attention_output)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(7, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "        metrics=['accuracy', 'AUC', 'Precision', 'Recall']\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "da989b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Hard example mining - focus training on the hardest examples\n",
    "def train_with_hard_example_mining(model, X_train, Y_train, X_valid, Y_valid, epochs=50):\n",
    "    batch_size = 32\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_batches = n_samples // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Calculate loss for each sample\n",
    "        sample_losses = []\n",
    "        for i in range(n_samples):\n",
    "            x_sample = X_train[i:i+1]\n",
    "            y_sample = Y_train[i:i+1]\n",
    "            loss = model.test_on_batch(x_sample, y_sample)[0]\n",
    "            sample_losses.append((i, loss))\n",
    "        \n",
    "        # Sort samples by loss (hardest examples first)\n",
    "        sample_losses.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Take the top 50% hardest examples\n",
    "        hard_indices = [x[0] for x in sample_losses[:n_samples//2]]\n",
    "        \n",
    "        # Train on hard examples\n",
    "        X_hard = X_train[hard_indices]\n",
    "        Y_hard = Y_train[hard_indices]\n",
    "        \n",
    "        model.fit(\n",
    "            X_hard, Y_hard,\n",
    "            epochs=1,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_acc = model.evaluate(X_valid, Y_valid, verbose=0)\n",
    "        print(f\"Validation loss: {val_loss:.4f}, accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7ac5ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Multi-stage training: First train on all data, then fine-tune on problem classes\n",
    "def multi_stage_training():\n",
    "    # First stage: Train on all data\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(7, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, Y_train,\n",
    "        epochs=25,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_valid, Y_valid),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Second stage: Focus on problem classes (5 and 6)\n",
    "    y_train_classes = np.argmax(Y_train, axis=1)\n",
    "    problem_indices = np.where((y_train_classes == 5) | (y_train_classes == 6))[0]\n",
    "    \n",
    "    # Also include some samples from other classes to prevent forgetting\n",
    "    other_indices = np.where((y_train_classes != 5) & (y_train_classes != 6))[0]\n",
    "    np.random.shuffle(other_indices)\n",
    "    selected_other = other_indices[:len(problem_indices)]\n",
    "    \n",
    "    fine_tune_indices = np.concatenate([problem_indices, selected_other])\n",
    "    X_fine_tune = X_train[fine_tune_indices]\n",
    "    Y_fine_tune = Y_train[fine_tune_indices]\n",
    "    \n",
    "    # Fine-tune with a very low learning rate\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_fine_tune, Y_fine_tune,\n",
    "        epochs=15,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_valid, Y_valid),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "36008bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Custom Metrics for Class 6\n",
    "class ClassSixRecall(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='class_six_recall', **kwargs):\n",
    "        super(ClassSixRecall, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.argmax(y_true, axis=1)\n",
    "        y_pred = tf.argmax(y_pred, axis=1)\n",
    "        \n",
    "        # Find class 6 samples\n",
    "        mask = tf.equal(y_true, 6)\n",
    "        \n",
    "        # Calculate true positives for class 6\n",
    "        tp = tf.cast(tf.logical_and(mask, tf.equal(y_pred, 6)), tf.float32)\n",
    "        self.true_positives.assign_add(tf.reduce_sum(tp))\n",
    "        \n",
    "        # Calculate false negatives for class 6\n",
    "        fn = tf.cast(tf.logical_and(mask, tf.not_equal(y_pred, 6)), tf.float32)\n",
    "        self.false_negatives.assign_add(tf.reduce_sum(fn))\n",
    "    \n",
    "    def result(self):\n",
    "        return self.true_positives / (self.true_positives + self.false_negatives + K.epsilon())\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0)\n",
    "        self.false_negatives.assign(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "34e78d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (4716, 6), Resampled shape: (8304, 6)\n",
      "Class distribution before SMOTE: [1384  687  603  714  649  679]\n",
      "Class distribution after SMOTE: [1384 1384 1384 1384 1384 1384]\n",
      "Epoch 1/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - AUC: 0.7508 - Precision: 0.4809 - Recall: 0.0083 - accuracy: 0.3296 - loss: 0.0440 - val_AUC: 0.9647 - val_Precision: 1.0000 - val_Recall: 0.2462 - val_accuracy: 0.7608 - val_loss: 0.0180 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.9447 - Precision: 0.8867 - Recall: 0.2384 - accuracy: 0.6523 - loss: 0.0211 - val_AUC: 0.9793 - val_Precision: 1.0000 - val_Recall: 0.4784 - val_accuracy: 0.8003 - val_loss: 0.0134 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - AUC: 0.9641 - Precision: 0.9049 - Recall: 0.4068 - accuracy: 0.7126 - loss: 0.0170 - val_AUC: 0.9829 - val_Precision: 0.9908 - val_Recall: 0.5471 - val_accuracy: 0.8225 - val_loss: 0.0120 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.9702 - Precision: 0.9242 - Recall: 0.4687 - accuracy: 0.7360 - loss: 0.0153 - val_AUC: 0.9849 - val_Precision: 0.9891 - val_Recall: 0.5757 - val_accuracy: 0.8136 - val_loss: 0.0111 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9748 - Precision: 0.9391 - Recall: 0.5086 - accuracy: 0.7484 - loss: 0.0140 - val_AUC: 0.9871 - val_Precision: 0.9918 - val_Recall: 0.6139 - val_accuracy: 0.8251 - val_loss: 0.0101 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9773 - Precision: 0.9413 - Recall: 0.5302 - accuracy: 0.7575 - loss: 0.0133 - val_AUC: 0.9870 - val_Precision: 0.9836 - val_Recall: 0.6113 - val_accuracy: 0.8200 - val_loss: 0.0100 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.9786 - Precision: 0.9505 - Recall: 0.5391 - accuracy: 0.7625 - loss: 0.0128 - val_AUC: 0.9883 - val_Precision: 0.9949 - val_Recall: 0.6202 - val_accuracy: 0.8327 - val_loss: 0.0094 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - AUC: 0.9816 - Precision: 0.9646 - Recall: 0.5622 - accuracy: 0.7836 - loss: 0.0119 - val_AUC: 0.9888 - val_Precision: 0.9939 - val_Recall: 0.6170 - val_accuracy: 0.8276 - val_loss: 0.0092 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - AUC: 0.9809 - Precision: 0.9594 - Recall: 0.5545 - accuracy: 0.7752 - loss: 0.0120 - val_AUC: 0.9884 - val_Precision: 0.9831 - val_Recall: 0.6291 - val_accuracy: 0.8314 - val_loss: 0.0094 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.9828 - Precision: 0.9590 - Recall: 0.5782 - accuracy: 0.7877 - loss: 0.0114 - val_AUC: 0.9897 - val_Precision: 0.9941 - val_Recall: 0.6431 - val_accuracy: 0.8333 - val_loss: 0.0088 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.9832 - Precision: 0.9616 - Recall: 0.5839 - accuracy: 0.7864 - loss: 0.0113 - val_AUC: 0.9891 - val_Precision: 0.9900 - val_Recall: 0.6272 - val_accuracy: 0.8238 - val_loss: 0.0089 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.9837 - Precision: 0.9611 - Recall: 0.5848 - accuracy: 0.7936 - loss: 0.0112 - val_AUC: 0.9901 - val_Precision: 0.9875 - val_Recall: 0.6559 - val_accuracy: 0.8327 - val_loss: 0.0085 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9834 - Precision: 0.9619 - Recall: 0.5778 - accuracy: 0.7911 - loss: 0.0112 - val_AUC: 0.9895 - val_Precision: 0.9858 - val_Recall: 0.6628 - val_accuracy: 0.8270 - val_loss: 0.0085 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.9845 - Precision: 0.9656 - Recall: 0.5919 - accuracy: 0.7940 - loss: 0.0108 - val_AUC: 0.9891 - val_Precision: 0.9894 - val_Recall: 0.6514 - val_accuracy: 0.8212 - val_loss: 0.0085 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.9849 - Precision: 0.9731 - Recall: 0.5924 - accuracy: 0.7935 - loss: 0.0106 - val_AUC: 0.9903 - val_Precision: 0.9961 - val_Recall: 0.6565 - val_accuracy: 0.8340 - val_loss: 0.0083 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.9863 - Precision: 0.9744 - Recall: 0.6050 - accuracy: 0.8063 - loss: 0.0101 - val_AUC: 0.9898 - val_Precision: 0.9818 - val_Recall: 0.6520 - val_accuracy: 0.8276 - val_loss: 0.0085 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.9858 - Precision: 0.9741 - Recall: 0.6072 - accuracy: 0.8041 - loss: 0.0102 - val_AUC: 0.9900 - val_Precision: 0.9934 - val_Recall: 0.6667 - val_accuracy: 0.8168 - val_loss: 0.0082 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - AUC: 0.9861 - Precision: 0.9681 - Recall: 0.6066 - accuracy: 0.8025 - loss: 0.0101 - val_AUC: 0.9899 - val_Precision: 0.9921 - val_Recall: 0.6431 - val_accuracy: 0.8372 - val_loss: 0.0083 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.9864 - Precision: 0.9764 - Recall: 0.6121 - accuracy: 0.8023 - loss: 0.0098 - val_AUC: 0.9898 - val_Precision: 0.9904 - val_Recall: 0.6578 - val_accuracy: 0.8212 - val_loss: 0.0082 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.9866 - Precision: 0.9730 - Recall: 0.6133 - accuracy: 0.8095 - loss: 0.0098 - val_AUC: 0.9901 - val_Precision: 0.9961 - val_Recall: 0.6463 - val_accuracy: 0.8314 - val_loss: 0.0081 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.9868 - Precision: 0.9774 - Recall: 0.6131 - accuracy: 0.8120 - loss: 0.0097 - val_AUC: 0.9896 - val_Precision: 0.9931 - val_Recall: 0.6412 - val_accuracy: 0.8270 - val_loss: 0.0082 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.9868 - Precision: 0.9805 - Recall: 0.6116 - accuracy: 0.8031 - loss: 0.0096 - val_AUC: 0.9896 - val_Precision: 0.9856 - val_Recall: 0.6533 - val_accuracy: 0.8270 - val_loss: 0.0083 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9877 - Precision: 0.9796 - Recall: 0.6260 - accuracy: 0.8206 - loss: 0.0094 - val_AUC: 0.9905 - val_Precision: 0.9886 - val_Recall: 0.6603 - val_accuracy: 0.8372 - val_loss: 0.0079 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - AUC: 0.9876 - Precision: 0.9797 - Recall: 0.6221 - accuracy: 0.8107 - loss: 0.0093 - val_AUC: 0.9904 - val_Precision: 0.9865 - val_Recall: 0.6489 - val_accuracy: 0.8448 - val_loss: 0.0080 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.9880 - Precision: 0.9818 - Recall: 0.6283 - accuracy: 0.8190 - loss: 0.0092 - val_AUC: 0.9888 - val_Precision: 0.9797 - val_Recall: 0.6438 - val_accuracy: 0.8289 - val_loss: 0.0084 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.9876 - Precision: 0.9780 - Recall: 0.6206 - accuracy: 0.8163 - loss: 0.0093 - val_AUC: 0.9895 - val_Precision: 0.9806 - val_Recall: 0.6444 - val_accuracy: 0.8270 - val_loss: 0.0085 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - AUC: 0.9886 - Precision: 0.9779 - Recall: 0.6378 - accuracy: 0.8217 - loss: 0.0090 - val_AUC: 0.9892 - val_Precision: 0.9885 - val_Recall: 0.6552 - val_accuracy: 0.8251 - val_loss: 0.0082 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.9882 - Precision: 0.9825 - Recall: 0.6366 - accuracy: 0.8174 - loss: 0.0090 - val_AUC: 0.9906 - val_Precision: 0.9860 - val_Recall: 0.6737 - val_accuracy: 0.8346 - val_loss: 0.0077 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9887 - Precision: 0.9803 - Recall: 0.6358 - accuracy: 0.8266 - loss: 0.0089 - val_AUC: 0.9900 - val_Precision: 0.9942 - val_Recall: 0.6578 - val_accuracy: 0.8321 - val_loss: 0.0079 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9885 - Precision: 0.9811 - Recall: 0.6357 - accuracy: 0.8225 - loss: 0.0089 - val_AUC: 0.9890 - val_Precision: 0.9866 - val_Recall: 0.6552 - val_accuracy: 0.8162 - val_loss: 0.0082 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9885 - Precision: 0.9809 - Recall: 0.6391 - accuracy: 0.8171 - loss: 0.0088 - val_AUC: 0.9904 - val_Precision: 0.9847 - val_Recall: 0.6565 - val_accuracy: 0.8378 - val_loss: 0.0079 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.9888 - Precision: 0.9857 - Recall: 0.6365 - accuracy: 0.8183 - loss: 0.0087 - val_AUC: 0.9889 - val_Precision: 0.9892 - val_Recall: 0.6387 - val_accuracy: 0.8244 - val_loss: 0.0083 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.9892 - Precision: 0.9839 - Recall: 0.6430 - accuracy: 0.8286 - loss: 0.0086 - val_AUC: 0.9913 - val_Precision: 0.9972 - val_Recall: 0.6692 - val_accuracy: 0.8480 - val_loss: 0.0075 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.9899 - Precision: 0.9868 - Recall: 0.6541 - accuracy: 0.8311 - loss: 0.0083 - val_AUC: 0.9904 - val_Precision: 0.9868 - val_Recall: 0.6648 - val_accuracy: 0.8302 - val_loss: 0.0077 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.9893 - Precision: 0.9836 - Recall: 0.6451 - accuracy: 0.8249 - loss: 0.0086 - val_AUC: 0.9885 - val_Precision: 0.9836 - val_Recall: 0.6495 - val_accuracy: 0.8206 - val_loss: 0.0084 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - AUC: 0.9893 - Precision: 0.9840 - Recall: 0.6491 - accuracy: 0.8218 - loss: 0.0086 - val_AUC: 0.9888 - val_Precision: 0.9797 - val_Recall: 0.6444 - val_accuracy: 0.8117 - val_loss: 0.0083 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - AUC: 0.9899 - Precision: 0.9853 - Recall: 0.6531 - accuracy: 0.8301 - loss: 0.0083 - val_AUC: 0.9887 - val_Precision: 0.9780 - val_Recall: 0.6495 - val_accuracy: 0.8104 - val_loss: 0.0083 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - AUC: 0.9889 - Precision: 0.9847 - Recall: 0.6394 - accuracy: 0.8198 - loss: 0.0087 - val_AUC: 0.9891 - val_Precision: 0.9845 - val_Recall: 0.6469 - val_accuracy: 0.8123 - val_loss: 0.0082 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - AUC: 0.9897 - Precision: 0.9837 - Recall: 0.6465 - accuracy: 0.8326 - loss: 0.0084 - val_AUC: 0.9891 - val_Precision: 0.9809 - val_Recall: 0.6520 - val_accuracy: 0.8206 - val_loss: 0.0082 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - AUC: 0.9897 - Precision: 0.9881 - Recall: 0.6500 - accuracy: 0.8295 - loss: 0.0082 - val_AUC: 0.9891 - val_Precision: 0.9875 - val_Recall: 0.6559 - val_accuracy: 0.8098 - val_loss: 0.0081 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - AUC: 0.9901 - Precision: 0.9898 - Recall: 0.6547 - accuracy: 0.8348 - loss: 0.0081 - val_AUC: 0.9897 - val_Precision: 0.9876 - val_Recall: 0.6597 - val_accuracy: 0.8200 - val_loss: 0.0081 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9908 - Precision: 0.9870 - Recall: 0.6662 - accuracy: 0.8348 - loss: 0.0078 - val_AUC: 0.9902 - val_Precision: 0.9840 - val_Recall: 0.6648 - val_accuracy: 0.8340 - val_loss: 0.0079 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9901 - Precision: 0.9886 - Recall: 0.6546 - accuracy: 0.8289 - loss: 0.0080 - val_AUC: 0.9896 - val_Precision: 0.9810 - val_Recall: 0.6571 - val_accuracy: 0.8117 - val_loss: 0.0081 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Decision boundary analysis for classes 5 and 6\n",
    "def analyze_decision_boundaries():\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(Y_test, axis=1)\n",
    "    \n",
    "    # Find misclassifications between classes 5 and 6\n",
    "    misclassified = []\n",
    "    for i in range(len(y_test_classes)):\n",
    "        if (y_test_classes[i] == 6 and y_pred_classes[i] == 5) or \\\n",
    "           (y_test_classes[i] == 5 and y_pred_classes[i] == 6):\n",
    "            misclassified.append(i)\n",
    "    \n",
    "    # Get class 5 and 6 indices\n",
    "    class5_indices = np.where(y_test_classes == 5)[0]\n",
    "    class6_indices = np.where(y_test_classes == 6)[0]\n",
    "    \n",
    "    # Combine indices\n",
    "    analysis_indices = np.concatenate([class5_indices, class6_indices])\n",
    "    \n",
    "    # Get features and labels for analysis\n",
    "    X_analysis = X_test[analysis_indices]\n",
    "    y_analysis = y_test_classes[analysis_indices]\n",
    "    \n",
    "    # Apply t-SNE for dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_analysis)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, c in zip([5, 6], ['blue', 'red']):\n",
    "        idx = np.where(y_analysis == i)\n",
    "        plt.scatter(X_tsne[idx, 0], X_tsne[idx, 1], c=c, label=f'Class {i}')\n",
    "    \n",
    "    # Highlight misclassified points\n",
    "    mis_idx = [list(analysis_indices).index(i) for i in misclassified if i in analysis_indices]\n",
    "    plt.scatter(X_tsne[mis_idx, 0], X_tsne[mis_idx, 1], c='yellow', marker='x', s=100, label='Misclassified')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title('t-SNE visualization of classes 5 and 6')\n",
    "    plt.show()\n",
    "    \n",
    "    return X_analysis, y_analysis, misclassified\n",
    "\n",
    "# Example of how to use these methods:\n",
    "# 1. First, balance the training data with SMOTE\n",
    "X_resampled, Y_resampled = apply_smote_balancing()\n",
    "\n",
    "# 2. Train an advanced model with balanced data and focal loss\n",
    "input_shape = X_resampled.shape[1]\n",
    "advanced_model = create_attention_model(input_shape)\n",
    "\n",
    "# 3. Train with early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "\n",
    "history = advanced_model.fit(\n",
    "    X_resampled, Y_resampled,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid),\n",
    "    callbacks=[early_stopping, lr_reduction],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bc9a56c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Advanced Model Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       477\n",
      "           1       0.97      0.99      0.98       225\n",
      "           2       1.00      0.99      0.99       208\n",
      "           3       0.99      0.96      0.97       223\n",
      "           4       0.58      0.23      0.33       222\n",
      "           5       0.51      0.83      0.63       218\n",
      "\n",
      "    accuracy                           0.86      1573\n",
      "   macro avg       0.84      0.83      0.82      1573\n",
      "weighted avg       0.87      0.86      0.85      1573\n",
      "\n",
      "Confusion Matrix:\n",
      "[[477   0   0   0   0   0]\n",
      " [  0 223   0   2   0   0]\n",
      " [  2   0 206   0   0   0]\n",
      " [  0   7   0 214   0   2]\n",
      " [  1   0   0   0  52 169]\n",
      " [  0   0   1   0  37 180]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Evaluate the advanced model\n",
    "y_pred = advanced_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(Y_test, axis=1)\n",
    "\n",
    "print(\"\\nAdvanced Model Evaluation:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e5dbf9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OussamaTab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(6,)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "12722001",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8efc28d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.4702 - loss: 1.3425 - val_accuracy: 0.7010 - val_loss: 0.7884\n",
      "Epoch 2/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6812 - loss: 0.7905 - val_accuracy: 0.7901 - val_loss: 0.5512\n",
      "Epoch 3/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7586 - loss: 0.5808 - val_accuracy: 0.7990 - val_loss: 0.4639\n",
      "Epoch 4/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7866 - loss: 0.4819 - val_accuracy: 0.7945 - val_loss: 0.4226\n",
      "Epoch 5/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7893 - loss: 0.4498 - val_accuracy: 0.8111 - val_loss: 0.3899\n",
      "Epoch 6/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8003 - loss: 0.4260 - val_accuracy: 0.8225 - val_loss: 0.3655\n",
      "Epoch 7/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8102 - loss: 0.4008 - val_accuracy: 0.8187 - val_loss: 0.3570\n",
      "Epoch 8/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8171 - loss: 0.3761 - val_accuracy: 0.8238 - val_loss: 0.3452\n",
      "Epoch 9/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8243 - loss: 0.3566 - val_accuracy: 0.8168 - val_loss: 0.3460\n",
      "Epoch 10/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8141 - loss: 0.3604 - val_accuracy: 0.8149 - val_loss: 0.3368\n",
      "Epoch 11/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8212 - loss: 0.3453 - val_accuracy: 0.8162 - val_loss: 0.3302\n",
      "Epoch 12/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8173 - loss: 0.3396 - val_accuracy: 0.8238 - val_loss: 0.3170\n",
      "Epoch 13/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8288 - loss: 0.3250 - val_accuracy: 0.8212 - val_loss: 0.3150\n",
      "Epoch 14/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8391 - loss: 0.3223 - val_accuracy: 0.8314 - val_loss: 0.3116\n",
      "Epoch 15/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8357 - loss: 0.3104 - val_accuracy: 0.8200 - val_loss: 0.3147\n",
      "Epoch 16/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8348 - loss: 0.3149 - val_accuracy: 0.8384 - val_loss: 0.3030\n",
      "Epoch 17/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8254 - loss: 0.3195 - val_accuracy: 0.8238 - val_loss: 0.2992\n",
      "Epoch 18/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.8292 - loss: 0.3067 - val_accuracy: 0.8397 - val_loss: 0.2965\n",
      "Epoch 19/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8446 - loss: 0.2967 - val_accuracy: 0.8289 - val_loss: 0.2906\n",
      "Epoch 20/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8447 - loss: 0.2878 - val_accuracy: 0.8403 - val_loss: 0.2939\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       477\n",
      "           1       0.94      0.97      0.95       225\n",
      "           2       1.00      0.98      0.99       208\n",
      "           3       0.94      0.96      0.95       223\n",
      "           4       0.59      0.21      0.31       222\n",
      "           5       0.49      0.77      0.60       218\n",
      "\n",
      "    accuracy                           0.84      1573\n",
      "   macro avg       0.82      0.81      0.80      1573\n",
      "weighted avg       0.85      0.84      0.83      1573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid)\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict classes\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # if one-hot encoded\n",
    "y_true = Y_test.argmax(axis=1)          # or just use Y_test if sparse\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b182fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OussamaTab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv1D(64, 3, activation='relu', input_shape=(6, 1)),  # Use Conv2D for images\n",
    "    MaxPooling1D(2),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9e1809a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9625dd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.3848 - loss: 1.5395 - val_accuracy: 0.6501 - val_loss: 0.8128\n",
      "Epoch 2/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6729 - loss: 0.7700 - val_accuracy: 0.7029 - val_loss: 0.6288\n",
      "Epoch 3/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.7276 - loss: 0.6068 - val_accuracy: 0.7583 - val_loss: 0.5357\n",
      "Epoch 4/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7620 - loss: 0.5201 - val_accuracy: 0.7697 - val_loss: 0.4872\n",
      "Epoch 5/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7591 - loss: 0.4895 - val_accuracy: 0.7761 - val_loss: 0.4558\n",
      "Epoch 6/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.7866 - loss: 0.4346 - val_accuracy: 0.7945 - val_loss: 0.4389\n",
      "Epoch 7/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8011 - loss: 0.4212 - val_accuracy: 0.7882 - val_loss: 0.4160\n",
      "Epoch 8/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7987 - loss: 0.3927 - val_accuracy: 0.7888 - val_loss: 0.4263\n",
      "Epoch 9/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8133 - loss: 0.3822 - val_accuracy: 0.7964 - val_loss: 0.4021\n",
      "Epoch 10/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8095 - loss: 0.3692 - val_accuracy: 0.8136 - val_loss: 0.3810\n",
      "Epoch 11/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8308 - loss: 0.3545 - val_accuracy: 0.8060 - val_loss: 0.3593\n",
      "Epoch 12/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8274 - loss: 0.3547 - val_accuracy: 0.7996 - val_loss: 0.3630\n",
      "Epoch 13/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.8317 - loss: 0.3446 - val_accuracy: 0.8244 - val_loss: 0.3664\n",
      "Epoch 14/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8241 - loss: 0.3343 - val_accuracy: 0.8270 - val_loss: 0.3410\n",
      "Epoch 15/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8304 - loss: 0.3289 - val_accuracy: 0.8187 - val_loss: 0.3402\n",
      "Epoch 16/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8263 - loss: 0.3257 - val_accuracy: 0.8225 - val_loss: 0.3258\n",
      "Epoch 17/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8184 - loss: 0.3214 - val_accuracy: 0.8174 - val_loss: 0.3308\n",
      "Epoch 18/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8282 - loss: 0.3207 - val_accuracy: 0.8187 - val_loss: 0.3184\n",
      "Epoch 19/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8274 - loss: 0.2990 - val_accuracy: 0.8276 - val_loss: 0.3203\n",
      "Epoch 20/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8430 - loss: 0.2968 - val_accuracy: 0.8282 - val_loss: 0.3214\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       477\n",
      "           1       0.94      0.94      0.94       225\n",
      "           2       0.94      0.98      0.96       208\n",
      "           3       0.91      0.96      0.93       223\n",
      "           4       0.54      0.93      0.68       222\n",
      "           5       0.71      0.06      0.10       218\n",
      "\n",
      "    accuracy                           0.84      1573\n",
      "   macro avg       0.83      0.81      0.77      1573\n",
      "weighted avg       0.85      0.84      0.80      1573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid)\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict classes\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # if one-hot encoded\n",
    "y_true = Y_test.argmax(axis=1)          # or just use Y_test if sparse\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "750048c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, GRU\n",
    "# For time series data:\n",
    "# X_train.shape == (samples, timesteps, features)\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_valid = X_valid.reshape((X_valid.shape[0], X_valid.shape[1], 1))\n",
    "timesteps = X_train.shape[1]\n",
    "features = X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4add0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e71d85e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.8322 - loss: 0.3022 - val_accuracy: 0.8282 - val_loss: 0.3070\n",
      "Epoch 2/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8396 - loss: 0.3000 - val_accuracy: 0.8232 - val_loss: 0.3175\n",
      "Epoch 3/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8315 - loss: 0.3085 - val_accuracy: 0.8314 - val_loss: 0.3099\n",
      "Epoch 4/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8402 - loss: 0.3019 - val_accuracy: 0.8289 - val_loss: 0.3137\n",
      "Epoch 5/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8418 - loss: 0.2851 - val_accuracy: 0.8308 - val_loss: 0.3064\n",
      "Epoch 6/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8228 - loss: 0.2952 - val_accuracy: 0.8295 - val_loss: 0.2975\n",
      "Epoch 7/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8377 - loss: 0.2866 - val_accuracy: 0.8314 - val_loss: 0.3032\n",
      "Epoch 8/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8418 - loss: 0.2774 - val_accuracy: 0.8193 - val_loss: 0.2983\n",
      "Epoch 9/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8431 - loss: 0.2701 - val_accuracy: 0.8282 - val_loss: 0.3052\n",
      "Epoch 10/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8471 - loss: 0.2768 - val_accuracy: 0.8232 - val_loss: 0.2871\n",
      "Epoch 11/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8465 - loss: 0.2718 - val_accuracy: 0.8327 - val_loss: 0.3068\n",
      "Epoch 12/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8460 - loss: 0.2837 - val_accuracy: 0.8346 - val_loss: 0.3240\n",
      "Epoch 13/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8386 - loss: 0.2912 - val_accuracy: 0.8359 - val_loss: 0.3171\n",
      "Epoch 14/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8462 - loss: 0.2723 - val_accuracy: 0.8232 - val_loss: 0.3157\n",
      "Epoch 15/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8466 - loss: 0.2730 - val_accuracy: 0.8397 - val_loss: 0.2948\n",
      "Epoch 16/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8383 - loss: 0.2722 - val_accuracy: 0.8346 - val_loss: 0.2802\n",
      "Epoch 17/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8478 - loss: 0.2695 - val_accuracy: 0.8187 - val_loss: 0.2984\n",
      "Epoch 18/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8347 - loss: 0.2702 - val_accuracy: 0.8270 - val_loss: 0.2862\n",
      "Epoch 19/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8435 - loss: 0.2653 - val_accuracy: 0.8410 - val_loss: 0.2797\n",
      "Epoch 20/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8585 - loss: 0.2534 - val_accuracy: 0.8321 - val_loss: 0.2938\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       477\n",
      "           1       0.89      0.98      0.93       225\n",
      "           2       0.96      0.99      0.97       208\n",
      "           3       1.00      0.94      0.97       223\n",
      "           4       1.00      0.08      0.15       222\n",
      "           5       0.50      0.91      0.65       218\n",
      "\n",
      "    accuracy                           0.84      1573\n",
      "   macro avg       0.89      0.82      0.78      1573\n",
      "weighted avg       0.90      0.84      0.81      1573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid)\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict classes\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # if one-hot encoded\n",
    "y_true = Y_test.argmax(axis=1)          # or just use Y_test if sparse\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1561430d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OussamaTab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(timesteps, features)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "12464538",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b0d14cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.3048 - loss: 1.8368 - val_accuracy: 0.5649 - val_loss: 1.1127\n",
      "Epoch 2/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5765 - loss: 0.9865 - val_accuracy: 0.6947 - val_loss: 0.6440\n",
      "Epoch 3/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6853 - loss: 0.6764 - val_accuracy: 0.7487 - val_loss: 0.5195\n",
      "Epoch 4/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7372 - loss: 0.5616 - val_accuracy: 0.7780 - val_loss: 0.4492\n",
      "Epoch 5/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7499 - loss: 0.4997 - val_accuracy: 0.7958 - val_loss: 0.4219\n",
      "Epoch 6/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7736 - loss: 0.4520 - val_accuracy: 0.7926 - val_loss: 0.4153\n",
      "Epoch 7/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7926 - loss: 0.4215 - val_accuracy: 0.7990 - val_loss: 0.3952\n",
      "Epoch 8/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7930 - loss: 0.4030 - val_accuracy: 0.8162 - val_loss: 0.3543\n",
      "Epoch 9/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8167 - loss: 0.3710 - val_accuracy: 0.8257 - val_loss: 0.3380\n",
      "Epoch 10/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8004 - loss: 0.3795 - val_accuracy: 0.8066 - val_loss: 0.3568\n",
      "Epoch 11/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8022 - loss: 0.3615 - val_accuracy: 0.7691 - val_loss: 0.4433\n",
      "Epoch 12/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8103 - loss: 0.3627 - val_accuracy: 0.8263 - val_loss: 0.3255\n",
      "Epoch 13/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8124 - loss: 0.3415 - val_accuracy: 0.8117 - val_loss: 0.3435\n",
      "Epoch 14/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8197 - loss: 0.3428 - val_accuracy: 0.7850 - val_loss: 0.4010\n",
      "Epoch 15/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8265 - loss: 0.3288 - val_accuracy: 0.8200 - val_loss: 0.3159\n",
      "Epoch 16/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8204 - loss: 0.3254 - val_accuracy: 0.8162 - val_loss: 0.3130\n",
      "Epoch 17/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8202 - loss: 0.3263 - val_accuracy: 0.8238 - val_loss: 0.3161\n",
      "Epoch 18/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8302 - loss: 0.3162 - val_accuracy: 0.8104 - val_loss: 0.3477\n",
      "Epoch 19/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8340 - loss: 0.3263 - val_accuracy: 0.8130 - val_loss: 0.3538\n",
      "Epoch 20/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8337 - loss: 0.2947 - val_accuracy: 0.8193 - val_loss: 0.3226\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       477\n",
      "           1       0.93      0.98      0.95       225\n",
      "           2       0.91      0.98      0.94       208\n",
      "           3       0.71      0.96      0.81       223\n",
      "           4       0.60      0.13      0.21       222\n",
      "           5       0.46      0.59      0.52       218\n",
      "\n",
      "    accuracy                           0.81      1573\n",
      "   macro avg       0.76      0.77      0.74      1573\n",
      "weighted avg       0.80      0.81      0.78      1573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid)\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict classes\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # if one-hot encoded\n",
    "y_true = Y_test.argmax(axis=1)          # or just use Y_test if sparse\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9fef963c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OussamaTab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    GRU(64, input_shape=(timesteps, features)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2814dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "356150f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.2833 - loss: 1.8023 - val_accuracy: 0.6380 - val_loss: 0.9319\n",
      "Epoch 2/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6458 - loss: 0.8202 - val_accuracy: 0.7551 - val_loss: 0.5484\n",
      "Epoch 3/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7440 - loss: 0.5450 - val_accuracy: 0.7863 - val_loss: 0.4555\n",
      "Epoch 4/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7730 - loss: 0.4775 - val_accuracy: 0.7856 - val_loss: 0.4251\n",
      "Epoch 5/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.7839 - loss: 0.4177 - val_accuracy: 0.8003 - val_loss: 0.3698\n",
      "Epoch 6/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.7874 - loss: 0.4043 - val_accuracy: 0.8193 - val_loss: 0.3481\n",
      "Epoch 7/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7934 - loss: 0.3917 - val_accuracy: 0.8149 - val_loss: 0.3295\n",
      "Epoch 8/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8177 - loss: 0.3419 - val_accuracy: 0.8155 - val_loss: 0.3300\n",
      "Epoch 9/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8063 - loss: 0.3500 - val_accuracy: 0.8295 - val_loss: 0.3152\n",
      "Epoch 10/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8138 - loss: 0.3276 - val_accuracy: 0.8206 - val_loss: 0.3161\n",
      "Epoch 11/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8358 - loss: 0.3102 - val_accuracy: 0.8123 - val_loss: 0.3080\n",
      "Epoch 12/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.8270 - loss: 0.3084 - val_accuracy: 0.8251 - val_loss: 0.3129\n",
      "Epoch 13/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.8216 - loss: 0.3217 - val_accuracy: 0.8365 - val_loss: 0.2878\n",
      "Epoch 14/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8158 - loss: 0.3174 - val_accuracy: 0.8187 - val_loss: 0.3007\n",
      "Epoch 15/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8302 - loss: 0.3001 - val_accuracy: 0.8302 - val_loss: 0.2914\n",
      "Epoch 16/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.8309 - loss: 0.2930 - val_accuracy: 0.8130 - val_loss: 0.3087\n",
      "Epoch 17/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.8310 - loss: 0.2829 - val_accuracy: 0.8206 - val_loss: 0.2974\n",
      "Epoch 18/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8252 - loss: 0.2808 - val_accuracy: 0.8232 - val_loss: 0.2916\n",
      "Epoch 19/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8409 - loss: 0.2687 - val_accuracy: 0.8429 - val_loss: 0.2577\n",
      "Epoch 20/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8339 - loss: 0.2624 - val_accuracy: 0.8168 - val_loss: 0.3020\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       477\n",
      "           1       0.95      0.94      0.95       225\n",
      "           2       0.98      0.97      0.98       208\n",
      "           3       0.97      0.85      0.91       223\n",
      "           4       0.55      0.27      0.36       222\n",
      "           5       0.47      0.78      0.59       218\n",
      "\n",
      "    accuracy                           0.83      1573\n",
      "   macro avg       0.82      0.80      0.80      1573\n",
      "weighted avg       0.85      0.83      0.83      1573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid)\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict classes\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # if one-hot encoded\n",
    "y_true = Y_test.argmax(axis=1)          # or just use Y_test if sparse\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "78d5addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv1D(128, 3, activation='relu', padding='same', input_shape=(6, 1)),  # Short sequences, so kernel=3\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(64, 3, activation='relu', padding='same'),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f29b758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "587cae2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.3837 - loss: 1.5865 - val_accuracy: 0.6781 - val_loss: 0.8254\n",
      "Epoch 2/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7032 - loss: 0.7560 - val_accuracy: 0.7627 - val_loss: 0.5964\n",
      "Epoch 3/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7649 - loss: 0.5721 - val_accuracy: 0.7850 - val_loss: 0.5076\n",
      "Epoch 4/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7782 - loss: 0.4995 - val_accuracy: 0.7780 - val_loss: 0.4476\n",
      "Epoch 5/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7796 - loss: 0.4613 - val_accuracy: 0.8028 - val_loss: 0.4341\n",
      "Epoch 6/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7966 - loss: 0.4179 - val_accuracy: 0.8073 - val_loss: 0.4003\n",
      "Epoch 7/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8131 - loss: 0.4001 - val_accuracy: 0.8149 - val_loss: 0.4044\n",
      "Epoch 8/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8089 - loss: 0.4004 - val_accuracy: 0.8130 - val_loss: 0.4110\n",
      "Epoch 9/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8178 - loss: 0.3780 - val_accuracy: 0.8117 - val_loss: 0.3634\n",
      "Epoch 10/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8177 - loss: 0.3714 - val_accuracy: 0.8041 - val_loss: 0.3782\n",
      "Epoch 11/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8194 - loss: 0.3557 - val_accuracy: 0.8155 - val_loss: 0.3519\n",
      "Epoch 12/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.8206 - loss: 0.3503 - val_accuracy: 0.8193 - val_loss: 0.3545\n",
      "Epoch 13/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8189 - loss: 0.3478 - val_accuracy: 0.8092 - val_loss: 0.3686\n",
      "Epoch 14/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8246 - loss: 0.3356 - val_accuracy: 0.8104 - val_loss: 0.3483\n",
      "Epoch 15/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8183 - loss: 0.3370 - val_accuracy: 0.8270 - val_loss: 0.3503\n",
      "Epoch 16/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8266 - loss: 0.3354 - val_accuracy: 0.8257 - val_loss: 0.3282\n",
      "Epoch 17/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8343 - loss: 0.3181 - val_accuracy: 0.7952 - val_loss: 0.3579\n",
      "Epoch 18/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8278 - loss: 0.3306 - val_accuracy: 0.8181 - val_loss: 0.3321\n",
      "Epoch 19/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8369 - loss: 0.3013 - val_accuracy: 0.8238 - val_loss: 0.3250\n",
      "Epoch 20/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8231 - loss: 0.3089 - val_accuracy: 0.8130 - val_loss: 0.3196\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       477\n",
      "           1       0.90      0.96      0.92       225\n",
      "           2       0.92      0.99      0.95       208\n",
      "           3       0.95      0.94      0.94       223\n",
      "           4       0.56      0.44      0.49       222\n",
      "           5       0.50      0.52      0.51       218\n",
      "\n",
      "    accuracy                           0.84      1573\n",
      "   macro avg       0.80      0.81      0.80      1573\n",
      "weighted avg       0.83      0.84      0.83      1573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid)\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict classes\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # if one-hot encoded\n",
    "y_true = Y_test.argmax(axis=1)          # or just use Y_test if sparse\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
