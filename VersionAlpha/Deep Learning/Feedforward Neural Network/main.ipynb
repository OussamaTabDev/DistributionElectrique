{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bffdb6a",
   "metadata": {},
   "source": [
    "# Deep learing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86043c9",
   "metadata": {},
   "source": [
    "## FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "779a60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3f247e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d3867222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>B</th>\n",
       "      <th>A</th>\n",
       "      <th>Ia</th>\n",
       "      <th>Ib</th>\n",
       "      <th>Ic</th>\n",
       "      <th>Va</th>\n",
       "      <th>Vb</th>\n",
       "      <th>Vc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-151.291812</td>\n",
       "      <td>-9.677452</td>\n",
       "      <td>85.800162</td>\n",
       "      <td>0.400750</td>\n",
       "      <td>-0.132935</td>\n",
       "      <td>-0.267815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-336.186183</td>\n",
       "      <td>-76.283262</td>\n",
       "      <td>18.328897</td>\n",
       "      <td>0.312732</td>\n",
       "      <td>-0.123633</td>\n",
       "      <td>-0.189099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-502.891583</td>\n",
       "      <td>-174.648023</td>\n",
       "      <td>-80.924663</td>\n",
       "      <td>0.265728</td>\n",
       "      <td>-0.114301</td>\n",
       "      <td>-0.151428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-593.941905</td>\n",
       "      <td>-217.703359</td>\n",
       "      <td>-124.891924</td>\n",
       "      <td>0.235511</td>\n",
       "      <td>-0.104940</td>\n",
       "      <td>-0.130570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-643.663617</td>\n",
       "      <td>-224.159427</td>\n",
       "      <td>-132.282815</td>\n",
       "      <td>0.209537</td>\n",
       "      <td>-0.095554</td>\n",
       "      <td>-0.113983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7856</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-66.237921</td>\n",
       "      <td>38.457041</td>\n",
       "      <td>24.912239</td>\n",
       "      <td>0.094421</td>\n",
       "      <td>-0.552019</td>\n",
       "      <td>0.457598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7857</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.849493</td>\n",
       "      <td>37.465454</td>\n",
       "      <td>25.515675</td>\n",
       "      <td>0.103778</td>\n",
       "      <td>-0.555186</td>\n",
       "      <td>0.451407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7858</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.446698</td>\n",
       "      <td>36.472055</td>\n",
       "      <td>26.106554</td>\n",
       "      <td>0.113107</td>\n",
       "      <td>-0.558211</td>\n",
       "      <td>0.445104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7859</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.029633</td>\n",
       "      <td>35.477088</td>\n",
       "      <td>26.684731</td>\n",
       "      <td>0.122404</td>\n",
       "      <td>-0.561094</td>\n",
       "      <td>0.438690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7860</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-64.598401</td>\n",
       "      <td>34.480799</td>\n",
       "      <td>27.250065</td>\n",
       "      <td>0.131669</td>\n",
       "      <td>-0.563835</td>\n",
       "      <td>0.432166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7861 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      G  C  B  A          Ia          Ib          Ic        Va        Vb  \\\n",
       "0     1  0  0  1 -151.291812   -9.677452   85.800162  0.400750 -0.132935   \n",
       "1     1  0  0  1 -336.186183  -76.283262   18.328897  0.312732 -0.123633   \n",
       "2     1  0  0  1 -502.891583 -174.648023  -80.924663  0.265728 -0.114301   \n",
       "3     1  0  0  1 -593.941905 -217.703359 -124.891924  0.235511 -0.104940   \n",
       "4     1  0  0  1 -643.663617 -224.159427 -132.282815  0.209537 -0.095554   \n",
       "...  .. .. .. ..         ...         ...         ...       ...       ...   \n",
       "7856  0  0  0  0  -66.237921   38.457041   24.912239  0.094421 -0.552019   \n",
       "7857  0  0  0  0  -65.849493   37.465454   25.515675  0.103778 -0.555186   \n",
       "7858  0  0  0  0  -65.446698   36.472055   26.106554  0.113107 -0.558211   \n",
       "7859  0  0  0  0  -65.029633   35.477088   26.684731  0.122404 -0.561094   \n",
       "7860  0  0  0  0  -64.598401   34.480799   27.250065  0.131669 -0.563835   \n",
       "\n",
       "            Vc  \n",
       "0    -0.267815  \n",
       "1    -0.189099  \n",
       "2    -0.151428  \n",
       "3    -0.130570  \n",
       "4    -0.113983  \n",
       "...        ...  \n",
       "7856  0.457598  \n",
       "7857  0.451407  \n",
       "7858  0.445104  \n",
       "7859  0.438690  \n",
       "7860  0.432166  \n",
       "\n",
       "[7861 rows x 10 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"G\" , \"C\" , \"B\" , \"A\" , \"Ia\" , \"Ib\" , \"Ic\" , \"Va\" , \"Vb\" , \"Vc\"] \n",
    "# Ia\" , \"Ib\" , \"Ic\" , \"Va\" , \"Vb\" , \"Vc is Input : features\n",
    "# G , C , B , A is OutPut : labels\n",
    "df = pd.read_csv(\"../../classData.csv\")\n",
    "cols[4:]\n",
    "# Step 1: Encode fault combinations into single-class labels\n",
    "fault_map = {\n",
    "    '0000': 0,  # No Fault\n",
    "    '1001': 1,  # LG\n",
    "    '0011': 2,  # LL\n",
    "    '0110': 3 , # Unkown : 3\n",
    "    '1011': 4,  # LLG\n",
    "    '0111': 5,  # LLL\n",
    "    '1111': 6   # LLLG\n",
    "}\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c5b5cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fault_type'] = df[['G', 'C', 'B', 'A']].astype(str).agg(''.join, axis=1)\n",
    "# Map to single class label\n",
    "df['fault_class'] = df['fault_type'].map(fault_map)\n",
    "# Drop rows with unknown fault combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a3a91397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep dropped rows (unknown fault types) in a separate DataFrame\n",
    "df_unknown = df[~df['fault_type'].isin(fault_map.keys())].copy()\n",
    "\n",
    "# Filter valid rows for training\n",
    "df = df[df['fault_type'].isin(fault_map.keys())].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4dafbd75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>B</th>\n",
       "      <th>A</th>\n",
       "      <th>Ia</th>\n",
       "      <th>Ib</th>\n",
       "      <th>Ic</th>\n",
       "      <th>Va</th>\n",
       "      <th>Vb</th>\n",
       "      <th>Vc</th>\n",
       "      <th>fault_type</th>\n",
       "      <th>fault_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-151.291812</td>\n",
       "      <td>-9.677452</td>\n",
       "      <td>85.800162</td>\n",
       "      <td>0.400750</td>\n",
       "      <td>-0.132935</td>\n",
       "      <td>-0.267815</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-336.186183</td>\n",
       "      <td>-76.283262</td>\n",
       "      <td>18.328897</td>\n",
       "      <td>0.312732</td>\n",
       "      <td>-0.123633</td>\n",
       "      <td>-0.189099</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-502.891583</td>\n",
       "      <td>-174.648023</td>\n",
       "      <td>-80.924663</td>\n",
       "      <td>0.265728</td>\n",
       "      <td>-0.114301</td>\n",
       "      <td>-0.151428</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-593.941905</td>\n",
       "      <td>-217.703359</td>\n",
       "      <td>-124.891924</td>\n",
       "      <td>0.235511</td>\n",
       "      <td>-0.104940</td>\n",
       "      <td>-0.130570</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-643.663617</td>\n",
       "      <td>-224.159427</td>\n",
       "      <td>-132.282815</td>\n",
       "      <td>0.209537</td>\n",
       "      <td>-0.095554</td>\n",
       "      <td>-0.113983</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7856</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-66.237921</td>\n",
       "      <td>38.457041</td>\n",
       "      <td>24.912239</td>\n",
       "      <td>0.094421</td>\n",
       "      <td>-0.552019</td>\n",
       "      <td>0.457598</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7857</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.849493</td>\n",
       "      <td>37.465454</td>\n",
       "      <td>25.515675</td>\n",
       "      <td>0.103778</td>\n",
       "      <td>-0.555186</td>\n",
       "      <td>0.451407</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7858</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.446698</td>\n",
       "      <td>36.472055</td>\n",
       "      <td>26.106554</td>\n",
       "      <td>0.113107</td>\n",
       "      <td>-0.558211</td>\n",
       "      <td>0.445104</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7859</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-65.029633</td>\n",
       "      <td>35.477088</td>\n",
       "      <td>26.684731</td>\n",
       "      <td>0.122404</td>\n",
       "      <td>-0.561094</td>\n",
       "      <td>0.438690</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7860</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-64.598401</td>\n",
       "      <td>34.480799</td>\n",
       "      <td>27.250065</td>\n",
       "      <td>0.131669</td>\n",
       "      <td>-0.563835</td>\n",
       "      <td>0.432166</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7861 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      G  C  B  A          Ia          Ib          Ic        Va        Vb  \\\n",
       "0     1  0  0  1 -151.291812   -9.677452   85.800162  0.400750 -0.132935   \n",
       "1     1  0  0  1 -336.186183  -76.283262   18.328897  0.312732 -0.123633   \n",
       "2     1  0  0  1 -502.891583 -174.648023  -80.924663  0.265728 -0.114301   \n",
       "3     1  0  0  1 -593.941905 -217.703359 -124.891924  0.235511 -0.104940   \n",
       "4     1  0  0  1 -643.663617 -224.159427 -132.282815  0.209537 -0.095554   \n",
       "...  .. .. .. ..         ...         ...         ...       ...       ...   \n",
       "7856  0  0  0  0  -66.237921   38.457041   24.912239  0.094421 -0.552019   \n",
       "7857  0  0  0  0  -65.849493   37.465454   25.515675  0.103778 -0.555186   \n",
       "7858  0  0  0  0  -65.446698   36.472055   26.106554  0.113107 -0.558211   \n",
       "7859  0  0  0  0  -65.029633   35.477088   26.684731  0.122404 -0.561094   \n",
       "7860  0  0  0  0  -64.598401   34.480799   27.250065  0.131669 -0.563835   \n",
       "\n",
       "            Vc fault_type  fault_class  \n",
       "0    -0.267815       1001            1  \n",
       "1    -0.189099       1001            1  \n",
       "2    -0.151428       1001            1  \n",
       "3    -0.130570       1001            1  \n",
       "4    -0.113983       1001            1  \n",
       "...        ...        ...          ...  \n",
       "7856  0.457598       0000            0  \n",
       "7857  0.451407       0000            0  \n",
       "7858  0.445104       0000            0  \n",
       "7859  0.438690       0000            0  \n",
       "7860  0.432166       0000            0  \n",
       "\n",
       "[7861 rows x 12 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3f85dc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>B</th>\n",
       "      <th>A</th>\n",
       "      <th>Ia</th>\n",
       "      <th>Ib</th>\n",
       "      <th>Ic</th>\n",
       "      <th>Va</th>\n",
       "      <th>Vb</th>\n",
       "      <th>Vc</th>\n",
       "      <th>fault_type</th>\n",
       "      <th>fault_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [G, C, B, A, Ia, Ib, Ic, Va, Vb, Vc, fault_type, fault_class]\n",
       "Index: []"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fa171c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OussamaTab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>C</th>\n",
       "      <th>B</th>\n",
       "      <th>A</th>\n",
       "      <th>Ia</th>\n",
       "      <th>Ib</th>\n",
       "      <th>Ic</th>\n",
       "      <th>Va</th>\n",
       "      <th>Vb</th>\n",
       "      <th>Vc</th>\n",
       "      <th>fault_type</th>\n",
       "      <th>fault_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4773</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-806.520131</td>\n",
       "      <td>715.441711</td>\n",
       "      <td>91.076276</td>\n",
       "      <td>-0.002646</td>\n",
       "      <td>0.037963</td>\n",
       "      <td>-0.035317</td>\n",
       "      <td>1111</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.006022</td>\n",
       "      <td>-11.128249</td>\n",
       "      <td>-45.048248</td>\n",
       "      <td>0.259558</td>\n",
       "      <td>0.351698</td>\n",
       "      <td>-0.611256</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4852</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-602.536400</td>\n",
       "      <td>-258.611634</td>\n",
       "      <td>861.145790</td>\n",
       "      <td>-0.040870</td>\n",
       "      <td>0.030177</td>\n",
       "      <td>0.010693</td>\n",
       "      <td>1111</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6848</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.805739</td>\n",
       "      <td>17.035809</td>\n",
       "      <td>-73.001984</td>\n",
       "      <td>-0.044279</td>\n",
       "      <td>0.559146</td>\n",
       "      <td>-0.514867</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3220</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-18.961677</td>\n",
       "      <td>-577.329623</td>\n",
       "      <td>598.568000</td>\n",
       "      <td>-0.441752</td>\n",
       "      <td>-0.025738</td>\n",
       "      <td>0.467490</td>\n",
       "      <td>0110</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4095</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-874.490799</td>\n",
       "      <td>543.253282</td>\n",
       "      <td>333.261107</td>\n",
       "      <td>-0.014331</td>\n",
       "      <td>0.041708</td>\n",
       "      <td>-0.027377</td>\n",
       "      <td>0111</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5856</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-67.844517</td>\n",
       "      <td>44.836291</td>\n",
       "      <td>19.538624</td>\n",
       "      <td>0.091856</td>\n",
       "      <td>-0.548246</td>\n",
       "      <td>0.456391</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5713</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.631244</td>\n",
       "      <td>80.837999</td>\n",
       "      <td>-92.766372</td>\n",
       "      <td>-0.547007</td>\n",
       "      <td>0.538919</td>\n",
       "      <td>0.008088</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5285</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-203.198382</td>\n",
       "      <td>-643.307361</td>\n",
       "      <td>846.503479</td>\n",
       "      <td>-0.041061</td>\n",
       "      <td>0.011472</td>\n",
       "      <td>0.029589</td>\n",
       "      <td>1111</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7476</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-56.166839</td>\n",
       "      <td>19.738104</td>\n",
       "      <td>33.453398</td>\n",
       "      <td>0.273078</td>\n",
       "      <td>-0.587118</td>\n",
       "      <td>0.314039</td>\n",
       "      <td>0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1573 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      G  C  B  A          Ia          Ib          Ic        Va        Vb  \\\n",
       "4773  1  1  1  1 -806.520131  715.441711   91.076276 -0.002646  0.037963   \n",
       "6815  0  0  0  0   53.006022  -11.128249  -45.048248  0.259558  0.351698   \n",
       "4852  1  1  1  1 -602.536400 -258.611634  861.145790 -0.040870  0.030177   \n",
       "6848  0  0  0  0   52.805739   17.035809  -73.001984 -0.044279  0.559146   \n",
       "3220  0  1  1  0  -18.961677 -577.329623  598.568000 -0.441752 -0.025738   \n",
       "...  .. .. .. ..         ...         ...         ...       ...       ...   \n",
       "4095  0  1  1  1 -874.490799  543.253282  333.261107 -0.014331  0.041708   \n",
       "5856  0  0  0  0  -67.844517   44.836291   19.538624  0.091856 -0.548246   \n",
       "5713  0  0  0  0    8.631244   80.837999  -92.766372 -0.547007  0.538919   \n",
       "5285  1  1  1  1 -203.198382 -643.307361  846.503479 -0.041061  0.011472   \n",
       "7476  0  0  0  0  -56.166839   19.738104   33.453398  0.273078 -0.587118   \n",
       "\n",
       "            Vc fault_type  fault_class  \n",
       "4773 -0.035317       1111            6  \n",
       "6815 -0.611256       0000            0  \n",
       "4852  0.010693       1111            6  \n",
       "6848 -0.514867       0000            0  \n",
       "3220  0.467490       0110            3  \n",
       "...        ...        ...          ...  \n",
       "4095 -0.027377       0111            5  \n",
       "5856  0.456391       0000            0  \n",
       "5713  0.008088       0000            0  \n",
       "5285  0.029589       1111            6  \n",
       "7476  0.314039       0000            0  \n",
       "\n",
       "[1573 rows x 12 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train , valid , test = np.split(df.sample(frac = 1) , [int(0.6 * len(df)) , int(0.8 * len(df))])\n",
    "# train  60% |||||||||||||\n",
    "# valid  20%  1234567890\n",
    "# test 20%\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6b33ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataFrame):\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    x = dataFrame[['Ia', 'Ib', 'Ic', 'Va', 'Vb', 'Vc']].values\n",
    "    # Get the fault class values from the dataFrame parameter (not the global df)\n",
    "    y = dataFrame['fault_class'].values\n",
    "    \n",
    "    # Scale the input features\n",
    "    scaler = StandardScaler() \n",
    "    x = scaler.fit_transform(x)\n",
    "\n",
    "    # One-hot encode the target for categorical classification\n",
    "    y_cat = to_categorical(y, num_classes=7)\n",
    "    \n",
    "    return x, y_cat  # Return the one-hot encoded y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d92a9709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.46155898e+00, -2.85323280e-01,  2.16627416e+00,\n",
       "        -9.82652723e-02,  9.26303950e-02, -2.10655450e-03],\n",
       "       [-1.65718540e-02,  3.39294097e-02, -2.64842834e-02,\n",
       "         2.04553783e+00, -1.17750116e+00, -7.18866054e-01],\n",
       "       [-1.56052226e-01,  1.99321075e-01, -4.81443408e-02,\n",
       "         3.63832065e-02, -1.63690019e+00,  1.62895461e+00],\n",
       "       ...,\n",
       "       [-1.26119515e-01, -1.67114760e-01,  3.62670466e-01,\n",
       "        -1.68506509e+00, -1.43378727e-01,  1.72332578e+00],\n",
       "       [-6.47038809e-02,  1.85143040e+00, -2.10487575e+00,\n",
       "         8.82556803e-01,  3.33796569e-03, -8.29693988e-01],\n",
       "       [-1.19580799e+00, -3.43064479e-02, -4.64265747e-02,\n",
       "         7.54048076e-01,  1.95485209e-01, -9.04589555e-01]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-run these steps with the fixed function\n",
    "X_train, Y_train = scale_dataset(train)\n",
    "X_valid, Y_valid = scale_dataset(valid)\n",
    "X_test, Y_test = scale_dataset(test)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9443924e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.46155898e+00, -2.85323280e-01,  2.16627416e+00,\n",
       "        -9.82652723e-02,  9.26303950e-02, -2.10655450e-03],\n",
       "       [-1.65718540e-02,  3.39294097e-02, -2.64842834e-02,\n",
       "         2.04553783e+00, -1.17750116e+00, -7.18866054e-01],\n",
       "       [-1.56052226e-01,  1.99321075e-01, -4.81443408e-02,\n",
       "         3.63832065e-02, -1.63690019e+00,  1.62895461e+00],\n",
       "       ...,\n",
       "       [-1.26119515e-01, -1.67114760e-01,  3.62670466e-01,\n",
       "        -1.68506509e+00, -1.43378727e-01,  1.72332578e+00],\n",
       "       [-6.47038809e-02,  1.85143040e+00, -2.10487575e+00,\n",
       "         8.82556803e-01,  3.33796569e-03, -8.29693988e-01],\n",
       "       [-1.19580799e+00, -3.43064479e-02, -4.64265747e-02,\n",
       "         7.54048076e-01,  1.95485209e-01, -9.04589555e-01]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "46110a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0d1ef3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OussamaTab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Build the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(6,)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(7, activation='softmax')  # 7 classes for multiclass classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a00fcfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.3004 - loss: 1.7725 - val_accuracy: 0.5388 - val_loss: 1.0935\n",
      "Epoch 2/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6244 - loss: 0.9634 - val_accuracy: 0.7303 - val_loss: 0.6670\n",
      "Epoch 3/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7530 - loss: 0.6269 - val_accuracy: 0.7672 - val_loss: 0.5271\n",
      "Epoch 4/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7911 - loss: 0.5052 - val_accuracy: 0.8066 - val_loss: 0.4602\n",
      "Epoch 5/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8073 - loss: 0.4526 - val_accuracy: 0.7990 - val_loss: 0.4240\n",
      "Epoch 6/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8074 - loss: 0.4197 - val_accuracy: 0.8174 - val_loss: 0.3998\n",
      "Epoch 7/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8133 - loss: 0.3888 - val_accuracy: 0.8174 - val_loss: 0.3831\n",
      "Epoch 8/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8256 - loss: 0.3746 - val_accuracy: 0.8149 - val_loss: 0.3810\n",
      "Epoch 9/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8251 - loss: 0.3598 - val_accuracy: 0.8162 - val_loss: 0.3628\n",
      "Epoch 10/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8428 - loss: 0.3411 - val_accuracy: 0.8149 - val_loss: 0.3539\n",
      "Epoch 11/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8313 - loss: 0.3325 - val_accuracy: 0.8200 - val_loss: 0.3501\n",
      "Epoch 12/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8211 - loss: 0.3351 - val_accuracy: 0.8142 - val_loss: 0.3413\n",
      "Epoch 13/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8257 - loss: 0.3327 - val_accuracy: 0.8181 - val_loss: 0.3407\n",
      "Epoch 14/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8377 - loss: 0.3085 - val_accuracy: 0.8289 - val_loss: 0.3304\n",
      "Epoch 15/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8416 - loss: 0.3117 - val_accuracy: 0.8200 - val_loss: 0.3363\n",
      "Epoch 16/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8355 - loss: 0.3097 - val_accuracy: 0.8257 - val_loss: 0.3196\n",
      "Epoch 17/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8347 - loss: 0.2965 - val_accuracy: 0.8238 - val_loss: 0.3247\n",
      "Epoch 18/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8371 - loss: 0.3047 - val_accuracy: 0.8187 - val_loss: 0.3306\n",
      "Epoch 19/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8488 - loss: 0.2783 - val_accuracy: 0.8206 - val_loss: 0.3125\n",
      "Epoch 20/20\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8447 - loss: 0.2820 - val_accuracy: 0.8181 - val_loss: 0.3221\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Train the model\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, Y_train, \n",
    "    epochs=20, \n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid)  # Use separate validation data instead of validation_split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "be6318db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       470\n",
      "           1       0.98      0.82      0.89       218\n",
      "           3       0.94      0.98      0.96       196\n",
      "           4       0.83      0.98      0.90       232\n",
      "           5       0.50      0.79      0.61       226\n",
      "           6       0.50      0.15      0.23       231\n",
      "\n",
      "    accuracy                           0.82      1573\n",
      "   macro avg       0.79      0.79      0.76      1573\n",
      "weighted avg       0.81      0.82      0.79      1573\n",
      "\n",
      "[[470   0   0   0   0   0]\n",
      " [  9 178   0  31   0   0]\n",
      " [  4   0 192   0   0   0]\n",
      " [  0   4   0 228   0   0]\n",
      " [  1   0   9   2 179  35]\n",
      " [  1   0   3  13 179  35]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Make predictions on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert softmax probabilities to class predictions\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Convert one-hot encoded test labels back to class indices for comparison\n",
    "y_test_classes = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n",
    "print(confusion_matrix(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58405359",
   "metadata": {},
   "source": [
    "## better Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "96d6be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1. Create a deeper, more complex model\n",
    "model_improved = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(6,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(7, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6a3ff845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: np.float64(0.5446985446985447), 1: np.float64(1.1592920353982301), 2: np.float64(1.2697899838449112), 3: np.float64(1.1873111782477341), 4: np.float64(1.220496894409938), 5: np.float64(1.173134328358209)}\n"
     ]
    }
   ],
   "source": [
    "# 2. Use a different optimizer with a lower learning rate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# 3. Add class weights to handle class imbalance\n",
    "# Calculate class weights based on class frequencies\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Get class indices from the one-hot encoded training labels\n",
    "y_train_classes = np.argmax(Y_train, axis=1)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_classes),\n",
    "    y=y_train_classes\n",
    ")\n",
    "\n",
    "# Convert to dictionary for Keras\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(\"Class weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "27b48987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compile with additional metrics\n",
    "model_improved.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'AUC', 'Precision', 'Recall']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b3b0fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Use callbacks for early stopping and learning rate reduction\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "27d36c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - AUC: 0.7122 - Precision: 0.4582 - Recall: 0.0236 - accuracy: 0.3159 - loss: 1.6188 - val_AUC: 0.9606 - val_Precision: 0.8463 - val_Recall: 0.4625 - val_accuracy: 0.7080 - val_loss: 0.7711 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - AUC: 0.9489 - Precision: 0.7729 - Recall: 0.5011 - accuracy: 0.6640 - loss: 0.8169 - val_AUC: 0.9766 - val_Precision: 0.8313 - val_Recall: 0.7239 - val_accuracy: 0.7754 - val_loss: 0.5331 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - AUC: 0.9696 - Precision: 0.7956 - Recall: 0.6580 - accuracy: 0.7362 - loss: 0.6038 - val_AUC: 0.9844 - val_Precision: 0.8467 - val_Recall: 0.7659 - val_accuracy: 0.8047 - val_loss: 0.4265 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - AUC: 0.9770 - Precision: 0.8075 - Recall: 0.7035 - accuracy: 0.7548 - loss: 0.5116 - val_AUC: 0.9850 - val_Precision: 0.8266 - val_Recall: 0.7793 - val_accuracy: 0.8015 - val_loss: 0.3978 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - AUC: 0.9809 - Precision: 0.8152 - Recall: 0.7405 - accuracy: 0.7814 - loss: 0.4657 - val_AUC: 0.9851 - val_Precision: 0.8261 - val_Recall: 0.7888 - val_accuracy: 0.8079 - val_loss: 0.3916 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - AUC: 0.9824 - Precision: 0.8196 - Recall: 0.7644 - accuracy: 0.7956 - loss: 0.4403 - val_AUC: 0.9866 - val_Precision: 0.8379 - val_Recall: 0.7856 - val_accuracy: 0.8136 - val_loss: 0.3618 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - AUC: 0.9845 - Precision: 0.8264 - Recall: 0.7654 - accuracy: 0.7956 - loss: 0.4140 - val_AUC: 0.9868 - val_Precision: 0.8336 - val_Recall: 0.7996 - val_accuracy: 0.8162 - val_loss: 0.3600 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - AUC: 0.9855 - Precision: 0.8280 - Recall: 0.7750 - accuracy: 0.8043 - loss: 0.3903 - val_AUC: 0.9877 - val_Precision: 0.8351 - val_Recall: 0.8117 - val_accuracy: 0.8251 - val_loss: 0.3440 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9848 - Precision: 0.8215 - Recall: 0.7734 - accuracy: 0.7997 - loss: 0.4024 - val_AUC: 0.9870 - val_Precision: 0.8323 - val_Recall: 0.7990 - val_accuracy: 0.8136 - val_loss: 0.3432 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - AUC: 0.9882 - Precision: 0.8425 - Recall: 0.8062 - accuracy: 0.8294 - loss: 0.3505 - val_AUC: 0.9890 - val_Precision: 0.8430 - val_Recall: 0.8060 - val_accuracy: 0.8257 - val_loss: 0.3133 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - AUC: 0.9860 - Precision: 0.8206 - Recall: 0.7885 - accuracy: 0.8079 - loss: 0.3812 - val_AUC: 0.9885 - val_Precision: 0.8383 - val_Recall: 0.8111 - val_accuracy: 0.8251 - val_loss: 0.3253 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - AUC: 0.9885 - Precision: 0.8466 - Recall: 0.8061 - accuracy: 0.8259 - loss: 0.3514 - val_AUC: 0.9884 - val_Precision: 0.8325 - val_Recall: 0.8060 - val_accuracy: 0.8225 - val_loss: 0.3196 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - AUC: 0.9880 - Precision: 0.8367 - Recall: 0.8047 - accuracy: 0.8175 - loss: 0.3531 - val_AUC: 0.9894 - val_Precision: 0.8360 - val_Recall: 0.8238 - val_accuracy: 0.8282 - val_loss: 0.3166 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - AUC: 0.9881 - Precision: 0.8384 - Recall: 0.8001 - accuracy: 0.8188 - loss: 0.3446 - val_AUC: 0.9894 - val_Precision: 0.8420 - val_Recall: 0.8238 - val_accuracy: 0.8308 - val_loss: 0.3051 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - AUC: 0.9895 - Precision: 0.8433 - Recall: 0.8107 - accuracy: 0.8292 - loss: 0.3256 - val_AUC: 0.9883 - val_Precision: 0.8259 - val_Recall: 0.8028 - val_accuracy: 0.8168 - val_loss: 0.3227 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - AUC: 0.9888 - Precision: 0.8421 - Recall: 0.8034 - accuracy: 0.8208 - loss: 0.3392 - val_AUC: 0.9902 - val_Precision: 0.8457 - val_Recall: 0.8263 - val_accuracy: 0.8346 - val_loss: 0.2972 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - AUC: 0.9896 - Precision: 0.8485 - Recall: 0.8143 - accuracy: 0.8309 - loss: 0.3232 - val_AUC: 0.9893 - val_Precision: 0.8331 - val_Recall: 0.8162 - val_accuracy: 0.8257 - val_loss: 0.3080 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - AUC: 0.9882 - Precision: 0.8300 - Recall: 0.8009 - accuracy: 0.8160 - loss: 0.3381 - val_AUC: 0.9894 - val_Precision: 0.8464 - val_Recall: 0.8028 - val_accuracy: 0.8289 - val_loss: 0.3046 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9897 - Precision: 0.8389 - Recall: 0.8108 - accuracy: 0.8255 - loss: 0.3154 - val_AUC: 0.9898 - val_Precision: 0.8352 - val_Recall: 0.8219 - val_accuracy: 0.8282 - val_loss: 0.2992 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - AUC: 0.9894 - Precision: 0.8461 - Recall: 0.8111 - accuracy: 0.8259 - loss: 0.3275 - val_AUC: 0.9893 - val_Precision: 0.8380 - val_Recall: 0.8130 - val_accuracy: 0.8251 - val_loss: 0.3060 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9885 - Precision: 0.8359 - Recall: 0.8095 - accuracy: 0.8213 - loss: 0.3326 - val_AUC: 0.9900 - val_Precision: 0.8406 - val_Recall: 0.8251 - val_accuracy: 0.8314 - val_loss: 0.2934 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - AUC: 0.9906 - Precision: 0.8552 - Recall: 0.8266 - accuracy: 0.8400 - loss: 0.3046 - val_AUC: 0.9898 - val_Precision: 0.8396 - val_Recall: 0.8155 - val_accuracy: 0.8276 - val_loss: 0.2917 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - AUC: 0.9902 - Precision: 0.8519 - Recall: 0.8236 - accuracy: 0.8365 - loss: 0.3116 - val_AUC: 0.9901 - val_Precision: 0.8386 - val_Recall: 0.8200 - val_accuracy: 0.8302 - val_loss: 0.2803 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - AUC: 0.9909 - Precision: 0.8459 - Recall: 0.8202 - accuracy: 0.8342 - loss: 0.2938 - val_AUC: 0.9903 - val_Precision: 0.8387 - val_Recall: 0.8238 - val_accuracy: 0.8302 - val_loss: 0.2885 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9908 - Precision: 0.8524 - Recall: 0.8282 - accuracy: 0.8416 - loss: 0.2939 - val_AUC: 0.9887 - val_Precision: 0.8319 - val_Recall: 0.8028 - val_accuracy: 0.8206 - val_loss: 0.3137 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - AUC: 0.9906 - Precision: 0.8490 - Recall: 0.8246 - accuracy: 0.8386 - loss: 0.2943 - val_AUC: 0.9890 - val_Precision: 0.8330 - val_Recall: 0.8123 - val_accuracy: 0.8270 - val_loss: 0.3093 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9906 - Precision: 0.8552 - Recall: 0.8234 - accuracy: 0.8392 - loss: 0.3006 - val_AUC: 0.9893 - val_Precision: 0.8405 - val_Recall: 0.8111 - val_accuracy: 0.8289 - val_loss: 0.3076 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - AUC: 0.9909 - Precision: 0.8539 - Recall: 0.8278 - accuracy: 0.8399 - loss: 0.2969 - val_AUC: 0.9902 - val_Precision: 0.8420 - val_Recall: 0.8206 - val_accuracy: 0.8302 - val_loss: 0.2795 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - AUC: 0.9915 - Precision: 0.8591 - Recall: 0.8360 - accuracy: 0.8485 - loss: 0.2878 - val_AUC: 0.9882 - val_Precision: 0.8329 - val_Recall: 0.7831 - val_accuracy: 0.8257 - val_loss: 0.3020 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9903 - Precision: 0.8409 - Recall: 0.8166 - accuracy: 0.8280 - loss: 0.2973 - val_AUC: 0.9896 - val_Precision: 0.8394 - val_Recall: 0.8079 - val_accuracy: 0.8327 - val_loss: 0.2967 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - AUC: 0.9911 - Precision: 0.8493 - Recall: 0.8277 - accuracy: 0.8398 - loss: 0.2842 - val_AUC: 0.9903 - val_Precision: 0.8447 - val_Recall: 0.8168 - val_accuracy: 0.8352 - val_loss: 0.2845 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - AUC: 0.9905 - Precision: 0.8434 - Recall: 0.8168 - accuracy: 0.8302 - loss: 0.2926 - val_AUC: 0.9909 - val_Precision: 0.8470 - val_Recall: 0.8238 - val_accuracy: 0.8378 - val_loss: 0.2692 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - AUC: 0.9905 - Precision: 0.8461 - Recall: 0.8242 - accuracy: 0.8364 - loss: 0.2990 - val_AUC: 0.9894 - val_Precision: 0.8441 - val_Recall: 0.8060 - val_accuracy: 0.8340 - val_loss: 0.2938 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9907 - Precision: 0.8513 - Recall: 0.8250 - accuracy: 0.8386 - loss: 0.2942 - val_AUC: 0.9896 - val_Precision: 0.8344 - val_Recall: 0.8238 - val_accuracy: 0.8289 - val_loss: 0.2970 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9914 - Precision: 0.8516 - Recall: 0.8378 - accuracy: 0.8449 - loss: 0.2813 - val_AUC: 0.9906 - val_Precision: 0.8431 - val_Recall: 0.8206 - val_accuracy: 0.8302 - val_loss: 0.2751 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9919 - Precision: 0.8544 - Recall: 0.8365 - accuracy: 0.8465 - loss: 0.2715 - val_AUC: 0.9883 - val_Precision: 0.8428 - val_Recall: 0.8187 - val_accuracy: 0.8289 - val_loss: 0.2988 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m141/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - AUC: 0.9906 - Precision: 0.8432 - Recall: 0.8192 - accuracy: 0.8344 - loss: 0.2798\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9907 - Precision: 0.8434 - Recall: 0.8196 - accuracy: 0.8345 - loss: 0.2800 - val_AUC: 0.9906 - val_Precision: 0.8389 - val_Recall: 0.8251 - val_accuracy: 0.8321 - val_loss: 0.2719 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9912 - Precision: 0.8484 - Recall: 0.8265 - accuracy: 0.8404 - loss: 0.2811 - val_AUC: 0.9897 - val_Precision: 0.8342 - val_Recall: 0.8193 - val_accuracy: 0.8302 - val_loss: 0.2862 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9918 - Precision: 0.8618 - Recall: 0.8413 - accuracy: 0.8512 - loss: 0.2682 - val_AUC: 0.9895 - val_Precision: 0.8401 - val_Recall: 0.8053 - val_accuracy: 0.8333 - val_loss: 0.2865 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - AUC: 0.9919 - Precision: 0.8563 - Recall: 0.8323 - accuracy: 0.8488 - loss: 0.2598 - val_AUC: 0.9901 - val_Precision: 0.8396 - val_Recall: 0.8257 - val_accuracy: 0.8340 - val_loss: 0.2879 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9914 - Precision: 0.8500 - Recall: 0.8306 - accuracy: 0.8434 - loss: 0.2731 - val_AUC: 0.9901 - val_Precision: 0.8406 - val_Recall: 0.8219 - val_accuracy: 0.8346 - val_loss: 0.2837 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m144/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - AUC: 0.9921 - Precision: 0.8587 - Recall: 0.8361 - accuracy: 0.8497 - loss: 0.2637\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.9921 - Precision: 0.8585 - Recall: 0.8361 - accuracy: 0.8496 - loss: 0.2638 - val_AUC: 0.9906 - val_Precision: 0.8434 - val_Recall: 0.8289 - val_accuracy: 0.8346 - val_loss: 0.2770 - learning_rate: 5.0000e-04\n",
      "Epoch 42: early stopping\n",
      "Restoring model weights from the end of the best epoch: 32.\n"
     ]
    }
   ],
   "source": [
    "# 6. Train with more epochs and class weights\n",
    "history = model_improved.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=50,  # Increase epochs, early stopping will prevent overfitting\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, Y_valid),\n",
    "    class_weight=class_weight_dict,  # Apply class weights\n",
    "    callbacks=[early_stopping, lr_reduction],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d6b992b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\n",
      "Improved Model Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       470\n",
      "           1       0.98      0.91      0.95       218\n",
      "           3       0.96      1.00      0.98       196\n",
      "           4       0.96      0.97      0.96       232\n",
      "           5       0.50      0.89      0.64       226\n",
      "           6       0.25      0.05      0.09       231\n",
      "\n",
      "    accuracy                           0.83      1573\n",
      "   macro avg       0.77      0.80      0.77      1573\n",
      "weighted avg       0.80      0.83      0.80      1573\n",
      "\n",
      "Confusion Matrix:\n",
      "[[469   1   0   0   0   0]\n",
      " [  6 199   0   0   0  13]\n",
      " [  0   0 196   0   0   0]\n",
      " [  0   2   0 224   0   6]\n",
      " [  0   0   7   0 202  17]\n",
      " [  0   1   2  10 206  12]]\n"
     ]
    }
   ],
   "source": [
    "# 7. Evaluate the improved model\n",
    "y_pred = model_improved.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(Y_test, axis=1)\n",
    "\n",
    "print(\"\\nImproved Model Evaluation:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4674559d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8. Visualize training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ea62a08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OussamaTab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best: 0.7449 with {'neurons1': 64, 'neurons2': 32, 'dropout': 0.2, 'lr': 0.01}\n",
      "New best: 0.7710 with {'neurons1': 64, 'neurons2': 32, 'dropout': 0.2, 'lr': 0.001}\n",
      "New best: 0.7729 with {'neurons1': 64, 'neurons2': 32, 'dropout': 0.3, 'lr': 0.01}\n",
      "New best: 0.7786 with {'neurons1': 64, 'neurons2': 64, 'dropout': 0.3, 'lr': 0.001}\n",
      "New best: 0.7850 with {'neurons1': 64, 'neurons2': 128, 'dropout': 0.2, 'lr': 0.001}\n",
      "New best: 0.7920 with {'neurons1': 128, 'neurons2': 128, 'dropout': 0.2, 'lr': 0.001}\n",
      "New best: 0.7926 with {'neurons1': 256, 'neurons2': 128, 'dropout': 0.2, 'lr': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# 1. Feature Engineering - Add derived features\n",
    "def add_engineered_features(x):\n",
    "    \"\"\"Add engineered features to help the model learn patterns better\"\"\"\n",
    "    # Original features: Ia, Ib, Ic, Va, Vb, Vc\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    import numpy as np\n",
    "    x_new = x.copy()\n",
    "    \n",
    "    # Extract individual components for easier manipulation\n",
    "    ia = x[:, 0]\n",
    "    ib = x[:, 1]\n",
    "    ic = x[:, 2]\n",
    "    va = x[:, 3]\n",
    "    vb = x[:, 4]\n",
    "    vc = x[:, 5]\n",
    "    \n",
    "    # Calculate phase differences (useful for identifying fault types)\n",
    "    phase_diff_ab = np.abs(ia - ib)\n",
    "    phase_diff_bc = np.abs(ib - ic)\n",
    "    phase_diff_ca = np.abs(ic - ia)\n",
    "    \n",
    "    # Calculate voltage-current ratios (impedance-like features)\n",
    "    # Add small constant to avoid division by zero\n",
    "    z_a = np.abs(va) / (np.abs(ia) + 1e-10)\n",
    "    z_b = np.abs(vb) / (np.abs(ib) + 1e-10)\n",
    "    z_c = np.abs(vc) / (np.abs(ic) + 1e-10)\n",
    "    \n",
    "    # Calculate power-related features\n",
    "    p_a = va * ia\n",
    "    p_b = vb * ib\n",
    "    p_c = vc * ic\n",
    "    \n",
    "    # Stack new features to original features\n",
    "    new_features = np.column_stack([\n",
    "        phase_diff_ab.reshape(-1, 1),\n",
    "        phase_diff_bc.reshape(-1, 1),\n",
    "        phase_diff_ca.reshape(-1, 1),\n",
    "        z_a.reshape(-1, 1),\n",
    "        z_b.reshape(-1, 1),\n",
    "        z_c.reshape(-1, 1),\n",
    "        p_a.reshape(-1, 1),\n",
    "        p_b.reshape(-1, 1),\n",
    "        p_c.reshape(-1, 1)\n",
    "    ])\n",
    "    \n",
    "    # Combine with original features\n",
    "    x_engineered = np.hstack((x_new, new_features))\n",
    "    \n",
    "    return x_engineered\n",
    "\n",
    "# Apply feature engineering to the datasets\n",
    "X_train_eng = add_engineered_features(X_train)\n",
    "X_valid_eng = add_engineered_features(X_valid)\n",
    "X_test_eng = add_engineered_features(X_test)\n",
    "\n",
    "# 2. Create an Ensemble of Models\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, concatenate\n",
    "\n",
    "def create_ensemble():\n",
    "    # Model 1: Deep network\n",
    "    input_layer = Input(shape=(X_train_eng.shape[1],))\n",
    "    \n",
    "    # Branch 1: Deep network\n",
    "    x1 = Dense(128, activation='relu')(input_layer)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    x1 = Dense(64, activation='relu')(x1)\n",
    "    x1 = Dense(32, activation='relu')(x1)\n",
    "    \n",
    "    # Branch 2: Wider network\n",
    "    x2 = Dense(256, activation='relu')(input_layer)\n",
    "    x2 = Dropout(0.4)(x2)\n",
    "    x2 = Dense(128, activation='relu')(x2)\n",
    "    \n",
    "    # Branch 3: Shallow network\n",
    "    x3 = Dense(64, activation='relu')(input_layer)\n",
    "    x3 = Dense(64, activation='relu')(x3)\n",
    "    \n",
    "    # Combine branches\n",
    "    combined = concatenate([x1, x2, x3])\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(7, activation='softmax')(combined)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and train ensemble\n",
    "ensemble_model = create_ensemble()\n",
    "\n",
    "# 3. Example of k-fold cross-validation to get more robust results\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_with_kfold(X, y, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        print(f\"Training fold {fold+1}/{n_splits}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_fold_train, X_fold_val = X[train_idx], X[val_idx]\n",
    "        y_fold_train, y_fold_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Create and compile model\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(X.shape[1],)),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(7, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_fold_train, y_fold_train,\n",
    "            epochs=30,\n",
    "            batch_size=32,\n",
    "            validation_data=(X_fold_val, y_fold_val),\n",
    "            class_weight=class_weight_dict,\n",
    "            callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, val_acc = model.evaluate(X_fold_val, y_fold_val, verbose=0)\n",
    "        fold_results.append((val_loss, val_acc))\n",
    "        print(f\"Fold {fold+1} validation accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    # Calculate average performance\n",
    "    avg_loss = np.mean([res[0] for res in fold_results])\n",
    "    avg_acc = np.mean([res[1] for res in fold_results])\n",
    "    print(f\"Average validation loss: {avg_loss:.4f}\")\n",
    "    print(f\"Average validation accuracy: {avg_acc:.4f}\")\n",
    "    \n",
    "    return fold_results\n",
    "\n",
    "# 4. Hyperparameter tuning\n",
    "def train_with_hyperparams(neurons1, neurons2, dropout_rate, learning_rate):\n",
    "    \"\"\"Train model with specific hyperparameters and return validation accuracy\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(neurons1, activation='relu', input_shape=(X_train_eng.shape[1],)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(neurons2, activation='relu'),\n",
    "        Dense(7, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_eng, Y_train,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_valid_eng, Y_valid),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    return max(history.history['val_accuracy'])\n",
    "\n",
    "# Grid of hyperparameters to try\n",
    "param_grid = {\n",
    "    'neurons1': [64, 128, 256],\n",
    "    'neurons2': [32, 64, 128],\n",
    "    'dropout_rate': [0.2, 0.3, 0.4],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001]\n",
    "}\n",
    "\n",
    "# Example of how to use - uncomment to run\n",
    "best_acc = 0\n",
    "best_params = {}\n",
    "for n1 in param_grid['neurons1']:\n",
    "    for n2 in param_grid['neurons2']:\n",
    "        for dr in param_grid['dropout_rate']:\n",
    "            for lr in param_grid['learning_rate']:\n",
    "                acc = train_with_hyperparams(n1, n2, dr, lr)\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_params = {'neurons1': n1, 'neurons2': n2, 'dropout': dr, 'lr': lr}\n",
    "                    print(f\"New best: {best_acc:.4f} with {best_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
